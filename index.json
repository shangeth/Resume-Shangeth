[{"authors":["admin"],"categories":null,"content":"I am a 4rd year Undergrad Dual degree student at BITS Pilani Goa Campus. Actively Looking for research opportunities in Deep Learning to start from May, 2020.\n","date":1558759380,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1558759380,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a 4rd year Undergrad Dual degree student at BITS Pilani Goa Campus. Actively Looking for research opportunities in Deep Learning to start from May, 2020.","tags":null,"title":"Shangeth Rajaa","type":"authors"},{"authors":null,"categories":null,"content":" Content Prerequisites Study Materials and References Help Me improve the Course Quality Loading… ","date":1567036800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1567036800,"objectID":"26b574e37d08aa6c19fd68b51660eb1a","permalink":"/google-ml-academy/deeplearning/","publishdate":"2019-08-29T00:00:00Z","relpermalink":"/google-ml-academy/deeplearning/","section":"google-ml-academy","summary":" Content Prerequisites Study Materials and References Help Me improve the Course Quality Loading… ","tags":null,"title":"Course Overview","type":"docs"},{"authors":null,"categories":null,"content":" \n Open in GitHub\nGoogle ML Academy 2019 Instructor: Shangeth Rajaa \nBefore starting with Neural Networks, we will look into 2 important machine learning models to understand regression and classification tasks - Linear Regression (Regression) - Logistic Regression (Classification)\nLinear Regression Most of the Deep learning Courses do not start with linear regression(LinReg), but LinReg gave me a better understanding of machine learning, so i will start with that, hoping that will make the understanding of Neural networks easier.\nYou can think of LinReg model as a curve fitting or function approximation model. Given a dataset $(X, y)$, the task is to find a relation $f$ between $X$ and $y$ such that $y = f(X)$. We are interested in this mapping $f: X \\rightarrow y$, as for any given $X$ in the future we can find $y = f(X)$.\nFor example, given a dataset about housing prices vs area of the house(Toy Dataset)\n   House Price($) y Area(1000 sqft) X     1034 2.4   1145 2.7   1252 3.04   2231 4.67   3423 5.3   \u0026hellip; \u0026hellip;    If we can find any the mapping between $X$ and $y$, $y = f(X)$, then its easy to predict the values of $y$ for any given $X$.\n# example dataset for linear regression %matplotlib inline import numpy as np import matplotlib.pyplot as plt x = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 plt.figure(figsize=(12,7)) plt.scatter(x, y, label='Data $(x,y)$') plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  So the prediction will be a line in case of 2-D data like above.\nThe predicted model will be line $ y = m x + c $ or in ML world popularly $ y = w x + b $.\n$y = f(X) = wX+b$ will be the Linear regression model and the objective is to find the best $w$ and $b$ that will give the nearest values for each $(X,y=wX+b)$ pair in the dataset.\n# example dataset for linear regression import numpy as np import matplotlib.pyplot as plt x = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 plt.figure(figsize=(12,7)) plt.scatter(x, y, label='Data $(x,y)$') plt.plot(x, 2 * x + 1, label='Predicted Line $y = f(X)$', color='r',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  What about Multi Dimensional Data? In real world, the dataset is going to be multi dimensional, ie: the price of a House will not just depend on the area of the house, it may also depend on multiple factors like no of rooms, locality, distance from airport, etc.\n   House Price($) y Area(1000 sqft) x1 No of Rooms x2 Distance form airport x3     1034 2.4 2 5.4   1145 2.7 3 3.1   1252 3.04 3 4.21   2231 4.67 2 2.3   3423 5.3 1 12   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;    In this case the model will be $y = f(X) = w_1x_1 + w_2x_2+w_3x_3+\u0026hellip;+w_nx_n + b$\nThis can also be represented in matrix form as $y = f(X) = X.W + b$\n$$X = \\begin{bmatrix} x_1 \u0026amp; x_2 \u0026amp; x_3 \u0026amp; \\dots \u0026amp; x_n \\end{bmatrix}$$\n$$W = \\begin{bmatrix}x_1\\\\x_2\\\\ \\vdots\\\\x_n\\end{bmatrix}$$\n$$b = [b]$$\nHow good is the Model? Before learning how to find $W$ and $b$ of a model. Let us assume we have a calculated values for optimal $W$ and $b$, how do we know how good is this model $\\hat{y} = f(X) = X.W + b$.\nNotice it is not $y$, its $\\hat{y}$. Because W and b are not the exact values, they are calculated approximate values to get $f(X)$ close to $y$. So we represent the prediction as $\\hat{y} = X.W + b$ and the true target as $y$.\nOur goal is to make $\\hat{y}$ as close as possible to $y$.\nSo we use a metric function call Mean Squared Error (MSE)\n$MSE(y, \\hat{y}) = \\mathcal{L}(y , \\hat{y}) = \\dfrac{1}{n} \\sum_{i=1}^{i=n}{(y_i - \\hat{y}_i)^2}$\nWhy MSE? Why not just subtraction? - model with 5 is better than -5 in sbtraction which actually is not both models are equally bad, but the predictions are in opposite direction. - MSE takes care of -5 and 5 ; $(-5)^2$ = $5^2$ = 25.\nimport numpy as np def MSE(y, y_hat): num_ex = len(y) mse_loss = np.sum((y - y_hat)**2)/num_ex return mse_loss  y = np.array([1.02, 2.3, 6.7, 3]) y_hat1 = np.array([10.43, 23.4, 12, 11]) y_hat2 = np.array([1, 1.9, 7, 3.1]) MSE(y,y), MSE(y, y_hat1), MSE(y, y_hat2)  (0.0, 156.46202499999998, 0.06509999999999996)  # example dataset for linear regression import numpy as np import matplotlib.pyplot as plt x = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 w = -1 b = 0 y_hat = w * x + b mse_loss = MSE(y, y_hat) plt.figure(figsize=(12,7)) plt.scatter(x, y, label='Data $(x,y)$') plt.plot(x, y_hat, label='Predicted Line $y = f(X)$', color='r',linewidth=4.0) plt.text(0,-40,'MSE = {:.3f}'.format(mse_loss), fontsize=20) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  # example dataset for linear regression import numpy as np import matplotlib.pyplot as plt x = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 w = 1 b = 0.5 y_hat = w * x + b mse_loss = MSE(y, y_hat) plt.figure(figsize=(12,7)) plt.scatter(x, y, label='Data $(x,y)$') plt.plot(x, y_hat, label='Predicted Line $y = f(X)$', color='r',linewidth=4.0) plt.text(0,-40,'MSE = {:.3f}'.format(mse_loss), fontsize=20) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  You can evidently see that a better model have less MSE loss. So the object of finding $W$ and $b$ will be to reduce the MSE Loss.\nIts is not generally possible to get a MSE of 0 in real world data, as the real world data is always noisy and with outliers.\nHow to find the Best Model? (W, b)  randomly initialize W, b in loop for n steps/epochs{  find $\\hat{y} = X.W + b$ find $MSE = \\mathcal{L}(y, \\hat{y})$ find $ \\frac{\\partial \\mathcal{L}}{\\partial w} $ and $ \\frac{\\partial \\mathcal{L}}{\\partial b} $ Update W and b with $w := w - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w}$ and $b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$ }   This is called Gradient Descent.\nLet\u0026rsquo;s try it first and see why it works.\n$\\mathcal{L}= \\dfrac{1}{n} \\sum_{i=1}^{i=n}{(y_i - wx^i-b)^2} $\n$ \\dfrac{\\partial \\mathcal{L}}{\\partial w} = \\dfrac{1}{n} \\sum_{i=1}^{i=n}{ 2 (y_i - wx^i-b) (-x^i)} $\n$= \\dfrac{2}{n} \\sum_{i=1}^{i=n}{ (wx^i+b - y_i) (x^i)}$\n$ \\dfrac{\\partial \\mathcal{L}}{\\partial b} = \\dfrac{1}{n} \\sum_{i=1}^{i=n}{ 2 (y_i - wx^i-b) (-1)}$\n$= \\dfrac{2}{n} \\sum_{i=1}^{i=n}{ (wx^i+b - y_i)}$\nTherefore\n$w := w - \\alpha \\dfrac{2}{n} (wX+b - y) (X)$\n$b := b - \\alpha \\dfrac{2}{n} \\sum_{i=1}^{i=n}{ (wX+b - y_i)}$\ndef model_forward(x, w, b): y_hat = w * x + b return y_hat def gradient_descent(w, b, X, y, a): w = w - a * 2 / X.shape[0] * np.dot(X.T, model_forward(X, w, b)- y) b = b - a * 2 / X.shape[0] * np.sum(model_forward(X, w, b)- y) return w, b  w, b = np.random.random(1), np.random.random(1) X = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 y_hat = model_forward(X, w, b) MSE(y, y_hat) # of randomly initialized model  831.0459153266066  losses = [] alpha = 0.00001 for i in range(1000): y_hat = model_forward(X, w, b) mse = MSE(y_hat, y) losses.append(mse) w, b = gradient_descent(w, b, X, y, alpha) if i%500 == 0: y_hat = model_forward(X, w, b) mse = MSE(y_hat, y) plt.figure(figsize=(12,7)) plt.scatter(X, y, label='Data $(X, y)$') plt.plot(X, y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,-30,'Epoch = {}'.format(i+1), fontsize=20) plt.text(0,-40,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show() plt.plot(losses) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show()  You can see how the loss decreases and the model prediction becomes better with the number of epochs.\nTry to run the code with different alpha, different no of epochs and different initialization of w and b.\nLinear Regression in TensorFlow import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np def MSE(y, y_hat): num_ex = len(y) mse_loss = np.sum((y - y_hat)**2)/num_ex return mse_loss X = np.arange(-25, 25, 0.5).astype('float32') y = (2 * X + 1 + np.random.randn(100)*5).astype('float32') model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])]) model.compile(optimizer='adam', loss='mean_squared_error') tf_history = model.fit(X, y, epochs=1000, verbose=False) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X, y, label='Data $(X, y)$') plt.plot(X, y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,-40,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Lets look into each line - model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n Sequential is like a container or wrapper which holds all the operations to be performed on your input.   model.compile(optimizer='adam', loss='mean_squared_error')\nCompile configures the training process. It defines the metrics, optimizers,..etc. You will understand more about this as we go on with more examples.  tf_history = model.fit(x, y, epochs=1000, verbose=False)\nmodel.fit actually trains the model for given number of epochs. Note: i've used Verbose=False(as the output will be long), set it to true to check the loss and metrics for each epoch.   Just 3-4 line of Tensorflow code can train a ml model, this is the advantage of using packages like Tensorflow/PyTorch. These packages are best optimized for speed and performance.\nmodel.summary()  Model: \u0026quot;sequential\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 1) 2 ================================================================= Total params: 2 Trainable params: 2 Non-trainable params: 0 _________________________________________________________________  tf.keras.Sequential? # if you are not sure of any library, its better to look into the docs.  ","date":1567033200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567033200,"objectID":"069cfca4a1416e1ec9d06942b98f0666","permalink":"/google-ml-academy/deeplearning/1.1/","publishdate":"2019-08-29T00:00:00+01:00","relpermalink":"/google-ml-academy/deeplearning/1.1/","section":"google-ml-academy","summary":"Open in GitHub\nGoogle ML Academy 2019 Instructor: Shangeth Rajaa \nBefore starting with Neural Networks, we will look into 2 important machine learning models to understand regression and classification tasks - Linear Regression (Regression) - Logistic Regression (Classification)\nLinear Regression Most of the Deep learning Courses do not start with linear regression(LinReg), but LinReg gave me a better understanding of machine learning, so i will start with that, hoping that will make the understanding of Neural networks easier.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" \n Open in GitHub\nGoogle ML Academy 2019 Instructor: Shangeth Rajaa \nTry Completing this Notebook before going through this page.   Open in GitHub Its a very simple assignment, you can finish it in less than 10 minutes. If you are stuck somewhere refer this page for solutions.\nTask-1 : Linear Regression on Non-Linear Data  Get X and y from dataset() function Train a Linear Regression model for this dataset. Visualize the model prediction  Dataset Call dataset() function to get X, y\nimport numpy as np import matplotlib.pyplot as plt def dataset(show=True): X = np.arange(-25, 25, 0.1) # Try changing y to a different function y = X**3 + 20 + np.random.randn(500)*1000 if show: plt.scatter(X, y) plt.show() return X, y X, y = dataset()  Scaling Dataset The maximum value of y in the dataset goes upto 15000 and the minimum values is less than -15000. The range of y is very large which makes the convergence/loss reduction slower. So will we scale the data, scaling the data will help the model converge faster. If all the features and target are in same range, there will be symmetry in the curve of Loss vs weights/bias, which makes the convergence faster.\nWe will do a very simple type of scaling, we will divide all the values of the data with the maximum values for X and y respectively.\nX, y = dataset() print(max(X), max(y), min(X), min(y)) X = X/max(X) y = y/max(y) print(max(X), max(y), min(X), min(y))  24.90000000000071 16694.307606867886 -25.0 -16126.103960535462 1.0 1.0 -1.0040160642569995 -0.9659642280642613  This is not a great scaling method, but good to start. We will see many more scaling/normalizing methods later.\nTry training the model with and without scaling and see the difference yourself.\nLinear Regression in TensorFlow import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np X, y = dataset(show=False) X_scaled = X/max(X) y_scaled = y/max(y) model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])]) # you can also define optimizers in this way, so you can change parameters like lr. optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_scaled, y_scaled, epochs=500, verbose=True) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_scaled) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_scaled, y_scaled, label='Data $(X, y)$') plt.plot(X_scaled, y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  WARNING: Logging before flag parsing goes to stderr. W0829 03:45:55.755687 139671975163776 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor Epoch 1/500 500/500 [==============================] - 0s 951us/sample - loss: 0.2951 Epoch 2/500 500/500 [==============================] - 0s 41us/sample - loss: 0.2856 . . Epoch 499/500 500/500 [==============================] - 0s 41us/sample - loss: 0.0264 Epoch 500/500 500/500 [==============================] - 0s 39us/sample - loss: 0.0263  Looks the model Prediction for this dataset is not very great, but that is expected as the model is a straight line, it cannot predict non linear regression data. Is there a way to train a regression model for this task?\nPolynomial Regression So when the dataset is not linear, linear regression cannot learn the dataset and make good predictions.\nSo we need a polynomial model which consideres the polynomial terms as well. So we need terms like $x^2$, $x^3$, \u0026hellip;, $x^n$ for the model to learn a polynomial of $n^{th}$ degree.\n$\\hat{y} = w_0 + w_1x + w_2x^2 + \u0026hellip; + w_nx^n$\nOne down side of this model is that, We will have to decide the value of n. But this is better than a linear regression model. We can get an idea of the value of n by visualizing a dataset, but for multi variable dataset, we will have to try different values of n and check which is better.\nPolynomial Features you can calculate the polynomial features for each feature by programming it or you can try sklearn.preprocessing.PolynomialFeatures which allows us to make polynomial terms of our data.\nWe will try degree 2, 3 and 4\nX, y = dataset(show=False) X_scaled = X/max(X) y_scaled = y/max(y)  from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=2) X_2 = poly.fit_transform(X_scaled.reshape(-1,1)) print(X_2.shape) print(X_2[0])  (500, 3) [ 1. -1.00401606 1.00804826]  from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=3) X_3 = poly.fit_transform(X_scaled.reshape(-1,1)) print(X_3.shape) print(X_3[0])  (500, 4) [ 1. -1.00401606 1.00804826 -1.01209664]  from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=4) X_4 = poly.fit_transform(X_scaled.reshape(-1,1)) print(X_4.shape) print(X_4[0])  (500, 5) [ 1. -1.00401606 1.00804826 -1.01209664 1.01616129]  The PolynomialFeatures returns $[1, x, x^2, x^3,\u0026hellip;]$.\nTask - 2  Train a model with polynomial terms in the dataset. Visualize the prediction of the model  The code remains the same except, the no of input features will be 3, 4, 5 respectively.\nTensorflow Model with 2nd Degree import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[3])]) optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_2, y_scaled, epochs=500, verbose=True) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_2) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_2[:, 1], y_scaled, label='Data $(X, y)$') plt.plot(X_2[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Epoch 1/500 500/500 [==============================] - 0s 332us/sample - loss: 0.5278 Epoch 2/500 500/500 [==============================] - 0s 38us/sample - loss: 0.4773 . . Epoch 499/500 500/500 [==============================] - 0s 44us/sample - loss: 0.0242 Epoch 500/500 500/500 [==============================] - 0s 43us/sample - loss: 0.0242  Why is the polynomial regression with 2-d features look like a straight line?\nWell because the model thinks that a straight line(look like, we can\u0026rsquo;t be sure if its a straight like, it can a parabola as well) better fits the dataset than a parabola. If you train the model for less epochs you can notice the model tries to fit the data with a parabola(2-d) but it improves as it moves to a line.\nTrain the same model for may be 50 epochs.\nimport tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[3])]) optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_2, y_scaled, epochs=50, verbose=True) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_2) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_2[:, 1], y_scaled, label='Data $(X, y)$') plt.plot(X_2[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Epoch 1/50 500/500 [==============================] - 0s 370us/sample - loss: 0.8566 Epoch 2/50 500/500 [==============================] - 0s 38us/sample - loss: 0.7970 . . Epoch 49/50 500/500 [==============================] - 0s 38us/sample - loss: 0.1027 Epoch 50/50 500/500 [==============================] - 0s 35us/sample - loss: 0.1000  You can clearly see that the model tries to fit the data with a parabola which doen\u0026rsquo;t seem to fit well, so it changes the parameters to fit a line.\nTensorflow Model with 3rd Degree import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[4])]) optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_3, y_scaled, epochs=500, verbose=True) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_3) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_3[:, 1], y_scaled, label='Data $(X, y)$') plt.plot(X_3[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Epoch 1/500 500/500 [==============================] - 0s 456us/sample - loss: 0.4445 Epoch 2/500 500/500 [==============================] - 0s 40us/sample - loss: 0.3993 . . Epoch 499/500 500/500 [==============================] - 0s 38us/sample - loss: 0.0040 Epoch 500/500 500/500 [==============================] - 0s 37us/sample - loss: 0.0039  3-D features perfectly fit the data with a 3rd degree polynomial as expected.\nTensorflow Model with 4th Degree import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[5])]) optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_4, y_scaled, epochs=500, verbose=True) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_4) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_4[:, 1], y_scaled, label='Data $(X, y)$') plt.plot(X_4[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Epoch 1/500 500/500 [==============================] - 0s 240us/sample - loss: 0.5839 Epoch 2/500 500/500 [==============================] - 0s 37us/sample - loss: 0.5453 . . Epoch 499/500 500/500 [==============================] - 0s 37us/sample - loss: 0.0040 Epoch 500/500 500/500 [==============================] - 0s 39us/sample - loss: 0.0040  4th degree poly-regression also did a good job in fitting the data as it also have the 3rd degree terms.\nimport tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[5])]) optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_4, y_scaled, epochs=50, verbose=True) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_4) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_4[:, 1], y_scaled, label='Data $(X, y)$') plt.plot(X_4[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Epoch 1/50 500/500 [==============================] - 0s 181us/sample - loss: 0.6724 Epoch 2/50 500/500 [==============================] - 0s 35us/sample - loss: 0.6104 . . Epoch 48/50 500/500 [==============================] - 0s 34us/sample - loss: 0.0661 Epoch 49/50 500/500 [==============================] - 0s 35us/sample - loss: 0.0655 Epoch 50/50 500/500 [==============================] - 0s 38us/sample - loss: 0.0648  If you run the 4th degree poly-regression for fewer epochs, you can notice, the model tries to fit a 4th(or higher than 3rd) degree polynomial but as the loss is high, the model changes it parameters to set the 4th degree terms to almost 0 and thus giving a 3rd degree polynomial as you train for more epochs.\nThis is polynomial regression. Yes, its easy. But one issue, as this was a toy dataset we know its a 3rd degree data, so we tried 2,3,4. But when the data is multi dimensional we cannot visualize the dataset, so its difficult to decide the degree. This is why you will see Neural Networks are awesome. They are End-End, they do not need several feature extraction from our side, they can extract necessary features of their own.\nMake a Higher degree (4th/5th degree) data and try polynomial regression on it. Also try different functions like exponents, trignometric..etc.\n","date":1567033200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567033200,"objectID":"dbae80859d0685cb16b6d9812848ba6a","permalink":"/google-ml-academy/deeplearning/1.2/","publishdate":"2019-08-29T00:00:00+01:00","relpermalink":"/google-ml-academy/deeplearning/1.2/","section":"google-ml-academy","summary":"Open in GitHub\nGoogle ML Academy 2019 Instructor: Shangeth Rajaa \nTry Completing this Notebook before going through this page.   Open in GitHub Its a very simple assignment, you can finish it in less than 10 minutes. If you are stuck somewhere refer this page for solutions.\nTask-1 : Linear Regression on Non-Linear Data  Get X and y from dataset() function Train a Linear Regression model for this dataset.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" \nGoogle ML Academy 2019 Instructor: Shangeth Rajaa \nLogistic Regression Logistic Regression is one of the most commonly used classification model. Unlike Linear Regression which predicts a real unbounded value $\\hat{y} = f(X) = WX+b$, Logistic Regression predicts the probability of a data belonging to a particular class.\n\nFor example for a given data (X,y) where X is a recieved email and y is 0 if email is spam and 1 if email is not spam. Logistic regression predicts the probability of the email X to be not spam. $\\hat{y} = f(X) = P(y=1|X)$\n\nSo if $\\hat{y} = P(y=1|X) \u0026gt; 0.5$ then the probability of the email to be spam ih high, so $X$ is a spam email.\nOther examples of classification can be\n Classification of image as cat, dog, parrot $ y = {0, 1, 2}$. Here y can be 0 or 1 or 2 depending on the probability of model prediction. (Multi class Classification) Classification of Cancer report as Malignant/Benign $y = {0, 1}$. (Binary Classification)  Why not use Linear Regression Model for classification? $\\hat{y}_{linreg} = WX+b$ where $\\hat{y} \\in \\mathbf{R} $, so the prediction can take value from $-\\infty$ to $\\infty$.\n$\\hat{y}_{classification} \\in { 0, 1, 2, \u0026hellip;, n }$, Classification prediction takes discrete values depening on number of class.\nSo we need a model which limits the prediction in the range ${0,1}$ for binary classification and ${0,1, 2, \u0026hellip;, n}$ for multi-class classification.\nClassification Data Let\u0026rsquo;s use sklearn.datasets.make_blobs to make a random classification dataset in 2D space so we can visualize it.\nWe are generating less data for visualization, for training we will use more data.\nfrom sklearn.datasets import make_blobs # 10 examples, (X)2 feature, (y)2 classes X, y = make_blobs(n_samples=10, n_features=2, centers=2, random_state=0) X.shape, y.shape  ((10, 2), (10,))  Let\u0026rsquo;s seperate the 0 and 1 class to visualize it.\nimport numpy as np class_0 = np.where(y == 0) class_1 = np.where(y == 1) X_0 = X[class_0] X_1 = X[class_1] X_0.shape, X_1.shape  ((5, 2), (5, 2))  import matplotlib.pyplot as plt plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  One possible linear classifier for this dataset can be\nimport matplotlib.pyplot as plt x = np.arange(-1, 5, 0.5) y = 3 - 0.5 * x plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.plot(x, y, label='Classifier', color='black') plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Let us consider the line $y = 3-x/2$ is the best classifier which seperates the data.\nLet $C(x,y) = y + x/2-3 $.\nBy basic school geometry, we know points in opposite side of a line $C(x,y)$ will give opposite values on $C(x,y)$.\nie: $C(x_1,y_1).C(x_2,y_2) \u0026lt; 0$, then $(x_1, y_1)$ and $(x_2, y_2)$ lies in the opposite side of $C(x,y)$.\nWhy are we even talking about this property? Well, this can tell something about how good a classifier is.\n Let us take 2 classifiers  one which classifies all points correctly one which misclassifies few points  calculate the $C(x,y)$ for every point and check how its different for both classifiers.  import matplotlib.pyplot as plt x = np.arange(-1, 5, 0.5) y = 3 - 0.5 * x plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.plot(x, y, label='Classifier', color='black') for i in range(len(X_0)): plt.text(X_0[i, 0]+0.1, X_0[i, 1], '$C = {:.3f}$'.format(X_0[i, 0]/2 + X_0[i, 1]- 3), fontsize=20) for i in range(len(X_1)): plt.text(X_1[i, 0]+0.1, X_1[i, 1], '$C = {:.3f}$'.format(X_1[i, 0]/2 + X_1[i, 1]- 3), fontsize=20) plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Now let\u0026rsquo;s try with a bad classifier\nimport matplotlib.pyplot as plt x = np.arange(-1, 5, 0.5) y = 3 - x plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.plot(x, y, label='Classifier', color='black') for i in range(len(X_0)): plt.text(X_0[i, 0]+0.1, X_0[i, 1], '$C = {:.3f}$'.format(X_0[i, 0] + X_0[i, 1]- 3), fontsize=20) for i in range(len(X_1)): plt.text(X_1[i, 0]+0.1, X_1[i, 1], '$C = {:.3f}$'.format(X_1[i, 0] + X_1[i, 1]- 3), fontsize=20) plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  You can see, the same class points have same sign when the classifier is good. So this metric C is actually a good way to find how good a classifier is.\nWe would like to know the probability of a point to be class 1. $P(y=1|X)$\nWe can do this by converting $C(x,y)$ into a range $[0,1]$ using a function called Sigmoid/Logistic.\nSigmoid/Logistic Function $g(x) = \\dfrac{1}{1+e^{-x}}$ Sigmoid can convert a number in Real range to $[0, 1]$ which is what we need to convert the score $C(x,y)$ to probability $P(y=1|X)$.\nLet\u0026rsquo;s code sigmoid in Numpy.\nimport numpy as np def sigmoid(x): return 1/(1+np.exp(-x))  a = np.array([-200,980, 0.1, -23, 1e-3]) sigmoid(a)  array([1.38389653e-87, 1.00000000e+00, 5.24979187e-01, 1.02618796e-10, 5.00250000e-01])  Sigmoid converted all the number from range 980 to -200 into a range of [0,1].\nProbabilities with Sigmoid import matplotlib.pyplot as plt x = np.arange(-1, 5, 0.5) y = 3 - 0.5 * x plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.plot(x, y, label='Classifier', color='black') for i in range(len(X_0)): plt.text(X_0[i, 0]+0.1, X_0[i, 1], '$\\hat{{y}} = {:.3f}$'.format(sigmoid(X_0[i, 0]/2 + X_0[i, 1]- 3)), fontsize=20) for i in range(len(X_1)): plt.text(X_1[i, 0]+0.1, X_1[i, 1], '$\\hat{{y}} = {:.3f}$'.format(sigmoid(X_1[i, 0]/2 + X_1[i, 1]- 3)), fontsize=20) plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  So sigmoid gives the probability of the point to be class \u0026lsquo;1\u0026rsquo; ie: $P(y=1|X)$.\nNote: this can also be $\\hat{y} = P(y=0|X)$, it depends on how you define 1 and 0 class. you can define it either way, but usually we use $\\hat{y} = P(y=1|X)$.\nWhen $\\hat{y} \u0026gt; 0.5$ we classify it as class \u0026ldquo;1\u0026rdquo; and when $\\hat{y} \u0026lt;= 0.5$ we classify it as class \u0026ldquo;0\u0026rdquo;.\nHow to compare the models? We still didn\u0026rsquo;t learn about how to find the best model. But let\u0026rsquo;s say we have 2 models, how do we compare which one is the best?\nBy comparing the $\\hat{y}$(prediction of the model) and $y$(true label) of each data.\nimport matplotlib.pyplot as plt x = np.arange(-1, 5, 0.5) y = 3 - 0.5 * x plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.plot(x, y, label='Classifier', color='black') for i in range(len(X_0)): plt.text(X_0[i, 0]+0.1, X_0[i, 1]-0.2, '$\\hat{{y}} = {:.3f}$\\n$y=1$'.format(sigmoid(X_0[i, 0]/2 + X_0[i, 1]- 3)), fontsize=20) for i in range(len(X_1)): plt.text(X_1[i, 0]+0.1, X_1[i, 1]-0.2, '$\\hat{{y}} = {:.3f}$\\n$y=0$'.format(sigmoid(X_1[i, 0]/2 + X_1[i, 1]- 3)), fontsize=20) plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Maximum Likelihood $\\hat{y} = g(W.X+b)$ gives the probability of $X$ belonging to class ${1}$. We want every point to have maximum predicted probability of that point to have its true label.ie: - if true label $y=1$, then we want to maximize $\\hat{y}_1 = \\hat{y} = g(W.X+b)$. - if true label $y=0$, then we want to maximize $\\hat{y}_0 = 1 - \\hat{y} = 1 - g(W.X+b)$.\nAs a point can have only 2 option either ${0, 1}$. so $\\hat{y}_0 + \\hat{y}_1 = 1$.\nLikelihood Product of predicted probabilities of every point to have its true label.\n$L = \\prod_{i=1}^{m}{P(\\hat{y}^i|X^i)}$\nwhere $\\hat{y}^i$ means $i^{th}$ data prediction and $X^i$ means $i^{th}$ data.\nSo the objective of any classification model in Machine Learning is to maximize this Likelihood $L$ thus its called Maximum Likelihood.\nLet\u0026rsquo;s take an example to calculate the Maximum Likelihood.\nimport numpy as np y_hat = np.array([0.94, 0.0271, 0.43, 0.56, 0.012]) MaxL = 1 for i in range(len(y_hat)): MaxL *= y_hat[i] MaxL  7.360967039999999e-05  The maximum Likelihood of just 5 numbers goes to an order of $10^{-5}$, in real dataset we will have thousands, sometimes millions of data which will give a MaximumLikelihood beyond the range of computation.\nSo we use the property of logarithm $log(a.b) = log(a) + log(b)$ to make this multiplication to addition, so it remains in the range of computation.\n$Log(MaxL) = log(\\prod_{i=1}^{m}{P(\\hat{y}^i|X^i))}$\n$LogLikelihood = \\sum_{i=1}^{m}{log(P(\\hat{y}^i|X^i))}$\nLet\u0026rsquo;s try this Log likelihood with Numpy\nimport numpy as np y_hat = np.array([0.94, 0.0271, 0.43, 0.56, 0.012]) LogL = 1 for i in range(len(y_hat)): LogL += np.log(y_hat[i]) LogL  -8.516734149556177  This number is can be used in computation easily and you can observe for any dataset Log likelihood will be in a good range of computation and it will be a negative number as log of any number less than one is negative.\nSo we introduce a negative sign to make it positive. Why? We would like to make this problem to find minimum loss, optimization is relatively easier for convex functions than concave.\n$NegLogLikelihood = -\\sum_{i=1}^{m}{log(P(\\hat{y}^i|X^i))}$\nThis is called as **Negative Log Likelihood Loss **or also as Cross Entropy Loss.\nNow there are 2 cases - $y = 0$, then we want $P(\\hat{y}^i|X^i) = \\hat{y}_0 = 1 - \\hat{y} = 1 - g(W.X+b)$ - $y = 1$, then $P(\\hat{y}^i|X^i) = \\hat{y}_1 = \\hat{y} = g(W.X+b)$\nSo we generalize this 2 cases with\n$ NLL(y, \\hat{y}) = -\\dfrac{1}{m} \\sum_{i=1}^{m}{y^i log(\\hat{y}^i) + (1-y^i) log(1 - \\hat{y}^i)}$\nWe divide the loss with m, to get the average, so the number of example may not affect the loss.\nLet\u0026rsquo;s code this loss with Numpy.\ndef CrossEntropy(y_hat, y): return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean()  y = np.array([0, 1, 1, 0, 0, 1, 1, 1 ]) y_hat_1 = np.array([0.11, 0.76, 0.56, 0.21, 0.04, 0.7, 0.64, 0.95]) y_hat_2 = np.array([0.71, 0.36, 0.16, 0.61, 0.34, 0.5, 0.14, 0.8]) CrossEntropy(y_hat_1, y),CrossEntropy(y_hat_2, y)  (0.26269860327583516, 1.041454329918626)  See how a bad prediction gives more CrossEntropy loss than a better prediction.\nFinding the best Model : Gradient Descent We are going to use the same optimization algorithm which we used for Linear Regression. In almost every deep learning problem, we will use gradient descent or a variation or better version of Gradient Descent. Adam and SGD are better versions of Gradient descent which also uses something called momentum. We will learn more about it later.\n randomly initialize W, b in loop for n steps/epochs{  find $\\hat{y} = g(X.W + b)$ find $ \\mathcal{L}(y, \\hat{y}) = NLL(y, \\hat{y})$ find $ \\frac{\\partial \\mathcal{L}}{\\partial w} $ and $ \\frac{\\partial \\mathcal{L}}{\\partial b} $ Update W and b with $w := w - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w}$ and $b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$ }   Every thing remains same as Linear Regression except now the Loss function $\\mathcal{L}(y, \\hat{y})$ is different, so $ \\frac{\\partial \\mathcal{L}}{\\partial w} $ and $ \\frac{\\partial \\mathcal{L}}{\\partial b} $ may be different. But it can easily be alculated with chain rule.\nIf you didn\u0026rsquo;t understand how we calculated $ \\frac{\\partial \\mathcal{L}}{\\partial w} $ and $ \\frac{\\partial \\mathcal{L}}{\\partial b} $ in Linear Regression , I strongly recommend you to learn multi variable calculus. Its very easy and interesting.\nOne good thing about frameworks like Tensorflow, PyTorch is that they have soemthing called Automatic Gradient. So you don\u0026rsquo;t need to perform these gradient calculation by hand and code it, you give the loss function and parameters, the framework will calculate the gradients of Loss wrt every parameter and update the parameter.\nSo after chain rule,\n$\\dfrac{\\partial \\mathcal{L}}{\\partial w_i} = \\dfrac{1}{m} \\sum_{j=1}^{m}{x_i^j(\\hat{y}^j - y^j)}$\n$\\dfrac{\\partial \\mathcal{L}}{\\partial b} = \\dfrac{1}{m} \\sum_{j=1}^{m}{(\\hat{y}^j - y^j)}$\nWe will update the parameters with\n$w_i := w_i - \\alpha \\dfrac{1}{m} \\sum_{j=1}^{m}{x_i^j(\\hat{y}^j - y^j)}$\n$b := b - \\alpha \\dfrac{1}{m} \\sum_{j=1}^{m}{((\\hat{y}^j - y^j)}$\nwhere $\\alpha$ is called learning rate, if learning rate is very high the model will learn faster, but may not converge well. if the learning rate is less, the model may take more time but will converge well ie: will get to less loss.\nNow Let\u0026rsquo;s code Logistic Regression model in Numpy and train it. Then use Tensorflow\u0026rsquo;s Keras API to train Logistic Regression Model.\nimport numpy as np def gradient_descent(w, b, X, y, a): w = w - a / X.shape[0] * np.dot(X.T, Log_Reg_model(X, w, b)- y) b = b - a / X.shape[0] * np.sum(Log_Reg_model(X, w, b)- y) return w, b  So far we have seen the math which is used in Logistic Regression model. But when you code, you will have to take care of the dimensions as well. we have seen $\\hat{y} = g(X.W+b)$ here $X.W$ need to be in correct dimension for matrix multiplication.\ndef sigmoid(x): return 1/(1+np.exp(-x)) def Log_Reg_model(x, w, b): y_hat = sigmoid(np.matmul(x, w) + b) return y_hat  # here i am initializing w as (2,1) to match X(1000,2) # you can also initialize w as (1,2) and use np.matmul(X, w.T) + b w, b = np.random.random((2, 1)), np.random.random((1, 1)) from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=1000, n_features=2, centers=2, random_state=9) y = y.reshape(-1,1) print(X.shape, y.shape, w.shape, b.shape) print(sigmoid(np.matmul(X, w) + b).shape) # to check the dimension  (1000, 2) (1000, 1) (2, 1) (1, 1) (1000, 1)  # shape of prediction and label should match Log_Reg_model(X, w, b).shape, y.shape  ((1000, 1), (1000, 1))  # Test the Cross entropy loss def CrossEntropy(y_hat, y): return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean() y_hat = Log_Reg_model(X, w, b) CrossEntropy(y_hat, y)  0.5361011603546345  Lets code a function to visualize\nimport matplotlib.pyplot as plt def visualize_classification(X, y, w, b, e=None, loss=None): class_0 = np.where(y == 0) class_1 = np.where(y == 1) X_0 = X[class_0] X_1 = X[class_1] # change this according to the plot scale x0 = np.arange(-15, 5, 0.01) y0 = ((-b - x0 * w[1])/w[0]).reshape(-1) plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.plot(x0, y0, label='Classifier', color='black') if e is not None: plt.text(-4,3,'Epoch = {}'.format(e), fontsize=20) if loss is not None: plt.text(-4,2,'CE = {:.3f}'.format(loss), fontsize=20) plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show() visualize_classification(X, y.reshape(-1), w, b, e=1, loss=1.1)  losses = [] alpha = 0.001 w, b = np.random.random((2, 1)), np.random.random(1) for i in range(10000): y_hat = Log_Reg_model(X, w, b) ce = CrossEntropy(y_hat, y) losses.append(ce) w, b = gradient_descent(w, b, X, y, alpha) if i%2000 == 0: y_hat = Log_Reg_model(X, w, b) ce = CrossEntropy(y_hat, y) visualize_classification(X, y.reshape(-1), w, b, e=i+1, loss=ce) plt.plot(losses) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show()  Our classifier made a good classification on this toy dataset. Now Let\u0026rsquo;s use Tensorflow to make a Logistic Regression model and train on this dataset.\nLogistic Regression in TensorFlow import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np def CrossEntropy(y_hat, y): return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean() from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=10000, n_features=2, centers=2, random_state=9) model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[2]), keras.layers.Activation('sigmoid')]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X, y, epochs=10, verbose=True)  Epoch 1/10 10000/10000 [==============================] - 0s 42us/sample - loss: 1.8787 - acc: 0.5026 Epoch 2/10 10000/10000 [==============================] - 0s 34us/sample - loss: 0.8342 - acc: 0.5618 . . Epoch 9/10 10000/10000 [==============================] - 0s 35us/sample - loss: 0.0172 - acc: 1.0000 Epoch 10/10 10000/10000 [==============================] - 0s 34us/sample - loss: 0.0135 - acc: 1.0000  We used a new metric called accuray_score.\n$Accuracy = \\dfrac{No\\ of\\ Correct\\ Predictions}{Total\\ no\\ of\\ Predictions}$\nso if out of 100 data, we made 64 correct predictions and 36 incorrect then the accuracy score is $\\dfrac{64}{100} = 0.64$.\nThis dataset was very easy, so it got accuracy score of 1.0 easily.\nLet\u0026rsquo;s Try another dataset.\nimport tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np def CrossEntropy(y_hat, y): return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean() from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=10000, n_features=2, centers=2, random_state=27) class_0 = np.where(y == 0) class_1 = np.where(y == 1) X_0 = X[class_0] X_1 = X[class_1] plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show() model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[2]), keras.layers.Activation('sigmoid')]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X, y, epochs=50, verbose=True)  Epoch 1/50 10000/10000 [==============================] - 0s 44us/sample - loss: 5.3699 - acc: 0.5000 Epoch 2/50 10000/10000 [==============================] - 0s 37us/sample - loss: 3.3782 - acc: 0.5000 . . Epoch 49/50 10000/10000 [==============================] - 0s 38us/sample - loss: 0.0044 - acc: 0.9992 Epoch 50/50 10000/10000 [==============================] - 0s 37us/sample - loss: 0.0043 - acc: 0.9993  You can see from the plot, that is not possible to get a accuracy of 1.0 with a linear classifier like Logistic Regression for this dataset as few of the class points are overlapping. But it still gives a very good result with just few lines of code.\n","date":1567033200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567033200,"objectID":"e9a551b72dcc3016d7629c160ace580d","permalink":"/google-ml-academy/deeplearning/1.3/","publishdate":"2019-08-29T00:00:00+01:00","relpermalink":"/google-ml-academy/deeplearning/1.3/","section":"google-ml-academy","summary":"Google ML Academy 2019 Instructor: Shangeth Rajaa \nLogistic Regression Logistic Regression is one of the most commonly used classification model. Unlike Linear Regression which predicts a real unbounded value $\\hat{y} = f(X) = WX+b$, Logistic Regression predicts the probability of a data belonging to a particular class.\n\nFor example for a given data (X,y) where X is a recieved email and y is 0 if email is spam and 1 if email is not spam.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":"","date":1562528221,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562528221,"objectID":"835012fefc81d462e93431e390609db4","permalink":"/project/facial-emotion-recognition-pytorch/","publishdate":"2019-07-08T01:07:01+05:30","relpermalink":"/project/facial-emotion-recognition-pytorch/","section":"project","summary":"Recognizing the facial emotions with Deep learning model trained on PyTorch and deployed with TF.js model converted with ONNX.","tags":["Deep Learning","Computer Vision","Python","PyTorch","Tensorflow","JavaScript"],"title":"Facial Emotion Recognition PyTorch ONNX","type":"project"},{"authors":["Zhengying\tLiu","Zhen\tXu","Julio\tJacques Junior","Meysam\tMadadi","Sergio\tEscalera","Shangeth\tRajaa"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1560470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560470400,"objectID":"293b9a3850ef93d4b029d5635dfddd5c","permalink":"/publication/ads/","publishdate":"2019-06-14T00:00:00Z","relpermalink":"/publication/ads/","section":"publication","summary":"Framework and Mathematical formulation of AutoML","tags":["Source Themes"],"title":"Overview and unifying conceptualization of Automated Machine Learning","type":"publication"},{"authors":null,"categories":null,"content":"","date":1560470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560470400,"objectID":"ceba210875c59b1eaa91353e1e8ea687","permalink":"/project/pneumonia-diagnosis-with-deep-learning/","publishdate":"2019-06-14T00:00:00Z","relpermalink":"/project/pneumonia-diagnosis-with-deep-learning/","section":"project","summary":"Web Application for Diagnosis of Pnuemonia with deep learning model trained and backed with PyTorch framework.","tags":["Deep Learning","Python","Computer Vision","PyTorch","Flask"],"title":"Pneumonia Diagnosis with Deep Learning","type":"project"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Python","PyTorch"],"content":" As we have seen in GAN 1, GAN 2, GAN 3, GAN 4 that GANs have 2 network, the Generator G and the Discriminator D. Given a latent vector z, the G generates a new sample from the distribution of the training data. D classifies a data sample as real(from the training data) or fake(generated by G).\nIn the starting, the G generates a random data sample(as it didnt learn the data distribution) and the D is not a good classifier now. As the training process goes, the G starts learning the data distribution and D becomes a good classifier. D tries to classify all sampels generated by D as fake, G tries to generate samples such that D classifies that as real. In the process, both the networks become better and Generator learns the distribution of the data and can now generate realistic samples. D becomes good at classifying real/fake data samples. Conditional GAN Let us consider MNIST GAN, after we trained a MNIST dataset on a GAN model, the generator(G) can now generate some images which look alike of the MNIST numbers.\nBut what if we want the G to generate images of a specific digit?. The G which we trained generated images samples depending on the latent vector z. But we used a random z. So we cannot choose a map from random z - \u0026gt; Specific image.\nSo we introduce a conditional label y, such that for a condition label y the generator have to generate sample. Now the Generator learns the distribution of the dataset and generates samples based on the condition y or c(condition).\nThe representation may vary, but the concept is the same.\nWe can also generate a output for a specific input, G : x -\u0026gt; y. Here we generate an image y given an inpu image x. This is a Pix2Pix GAN.\nCheck this amazing demo of Pix2Pix GAN\nThis kind of image to image (pix2pix) can be done with the help of Encoder-Decoder architecture, where the input image is encoded to a feature representation vector anf this vector is decoded to the target image.\nSo the generator will learn the mapping from G: x-\u0026gt;y with autoencoder architecture, and generate new samples for the given x.\nThe generator G will get a pair of images:\n training x and training y\nG will classify as real training x and generated y(for x)\nG will classify as fake training x and generated y (different x)\nG will classify as fake  This way a conditional GAN(CGAN) or pix2pix GAN is trained, which has massive applications. In the next post we will see how to train a GAN to do a image to image translation(pix2pix) without labelled pair.\n","date":1558759380,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558759380,"objectID":"57bde7a314f54aa0646c1282cb1f5762","permalink":"/post/gan-5/","publishdate":"2019-05-25T04:43:00Z","relpermalink":"/post/gan-5/","section":"post","summary":"Conditional GAN","tags":["Deep Learning post","GAN post"],"title":"GAN 5","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Python","PyTorch"],"content":" DGGAN is exactly the same as Linear GANs, excpet they use COnvolutional neural networks. As we all know CNNs are the best feature extractor for many kind of data like image, videos, audio, etc.\nWe will use Street View House (SVHN) Dataset and generate new home numbers using DCGAN. The model architecture will be the same, except the CNNs will be used in the Discriminator to classify images as real or fake. Generator will use a transpose convolutional layers to upsample/generate new image samples from a given latent vector z.\nDiscriminator In the original paper, no max-pooling layers are used with the CNN layers, rather a stride of 2 is used. Batch Normalization and Leaku ReLU are also used. Linear layers are connected at the end of flattened cnn layers and sigmoid activation is used to make the output in the range 0 to 1.\nGenerator In generator as we need to upsample a latent vector to an image of size [3, 32, 32], we use transposed convolutional layer with ReLU activation and Batch Normalization. Tanh activation in the output layer. Let\u0026rsquo;s code the DCGAN Data Pytorch have SVHN dataset built-in to the datset library, we will use that for the dataset.\nimport torch from torchvision import datasets from torchvision import transforms transform = transforms.ToTensor() svhn = datasets.SVHN(root='data/', split='train', download=True, transform=transform) train_loader = torch.utils.data.DataLoader(dataset=svhn_train, batch_size=256, shuffle=True)  We want to scale the images to have the value in the range -1 to 1.\ndef scale_img(x, feature_range=(-1, 1)): min, max = feature_range x = x * (max - min) + min return x  Discriminator We will build a model with CNN and batch norm layers, it is a normal CNN classifier.\nimport torch.nn as nn import torch.nn.functional as F # function to return conv and batchnorm together def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True): layers = [] conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False) layers.append(conv_layer) if batch_norm: layers.append(nn.BatchNorm2d(out_channels)) return nn.Sequential(*layers) class Discriminator(nn.Module): def __init__(self, conv_dim=32): super(Discriminator, self).__init__() self.conv_dim = conv_dim self.conv1 = conv(3, conv_dim, 4, batch_norm=False) # self.conv2 = conv(conv_dim, conv_dim*2, 4) self.conv3 = conv(conv_dim*2, conv_dim*4, 4) self.fc = nn.Linear(conv_dim*4*4*4, 1) def forward(self, x): out = F.leaky_relu(self.conv1(x), 0.2) out = F.leaky_relu(self.conv2(out), 0.2) out = F.leaky_relu(self.conv3(out), 0.2) out = out.view(-1, self.conv_dim*4*4*4) out = self.fc(out) return out  Generator Generator need transposed Convolutioanl layer with Batch Norm and relu to upsample the latent vector to a image sample.\ndef deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True): layers = [] transpose_conv_layer = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)r layers.append(transpose_conv_layer) if batch_norm: layers.append(nn.BatchNorm2d(out_channels)) return nn.Sequential(*layers) class Generator(nn.Module): def __init__(self, z_size, conv_dim=32): super(Generator, self).__init__() self.conv_dim = conv_dim self.fc = nn.Linear(z_size, conv_dim*4*4*4) self.t_conv1 = deconv(conv_dim*4, conv_dim*2, 4) self.t_conv2 = deconv(conv_dim*2, conv_dim, 4) self.t_conv3 = deconv(conv_dim, 3, 4, batch_norm=False) def forward(self, x): out = self.fc(x) out = out.view(-1, self.conv_dim*4, 4, 4) # ( out = F.relu(self.t_conv1(out)) out = F.relu(self.t_conv2(out)) out = self.t_conv3(out) out = F.tanh(out) return out  Building the models conv_dim = 32 z_size = 100 D = Discriminator(conv_dim) G = Generator(z_size=z_size, conv_dim=conv_dim) print(D) print() print(G) train_on_gpu = torch.cuda.is_available() if train_on_gpu: G.cuda() D.cuda() print('GPU available for training. Models moved to GPU') else: print('Training on CPU.')  Output :\n Discriminator( (conv1): Sequential( (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) ) (conv2): Sequential( (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (conv3): Sequential( (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (fc): Linear(in_features=2048, out_features=1, bias=True) ) Generator( (fc): Linear(in_features=100, out_features=2048, bias=True) (t_conv1): Sequential( (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (t_conv2): Sequential( (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (t_conv3): Sequential( (0): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) ) )  Loss Loss and training loop is exactly same as Linear GANs. we scale the images in range -1 to 1 inside training loop. Check GAN2 and GAN3 is you dont understand about the loss and training loop.\ndef real_loss(D_out, smooth=False): batch_size = D_out.size(0) if smooth: labels = torch.ones(batch_size)*0.9 else: labels = torch.ones(batch_size) if train_on_gpu: labels = labels.cuda() criterion = nn.BCEWithLogitsLoss() loss = criterion(D_out.squeeze(), labels) return loss def fake_loss(D_out): batch_size = D_out.size(0) labels = torch.zeros(batch_size) if train_on_gpu: labels = labels.cuda() criterion = nn.BCEWithLogitsLoss() loss = criterion(D_out.squeeze(), labels) return loss  Optimizers check this optimizer post by ruder\nimport torch.optim as optim lr = 1e-4 beta1=0.5 beta2=0.999 d_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2]) g_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])  Training loop num_epochs = 50 samples = [] losses = [] print_every = 300 sample_size=16 fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size)) fixed_z = torch.from_numpy(fixed_z).float() for epoch in range(num_epochs): for batch_i, (real_images, _) in enumerate(train_loader): batch_size = real_images.size(0) real_images = scale(real_images) # rescale image d_optimizer.zero_grad() if train_on_gpu: real_images = real_images.cuda() D_real = D(real_images) d_real_loss = real_loss(D_real) z = np.random.uniform(-1, 1, size=(batch_size, z_size)) z = torch.from_numpy(z).float() if train_on_gpu: z = z.cuda() fake_images = G(z) D_fake = D(fake_images) d_fake_loss = fake_loss(D_fake) d_loss = d_real_loss + d_fake_loss d_loss.backward() d_optimizer.step() g_optimizer.zero_grad() z = np.random.uniform(-1, 1, size=(batch_size, z_size)) z = torch.from_numpy(z).float() if train_on_gpu: z = z.cuda() fake_images = G(z) D_fake = D(fake_images) g_loss = real_loss(D_fake) g_loss.backward() g_optimizer.step() print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format( epoch+1, num_epochs, d_loss.item(), g_loss.item()))  Output:\nEpoch [ 1/ 50] | d_loss: 1.3871 | g_loss: 0.7894 Epoch [ 1/ 50] | d_loss: 0.8700 | g_loss: 2.5015 Epoch [ 2/ 50] | d_loss: 1.0024 | g_loss: 1.4002 Epoch [ 2/ 50] | d_loss: 1.2057 | g_loss: 1.1445 Epoch [ 3/ 50] | d_loss: 0.9766 | g_loss: 1.0346 Epoch [ 3/ 50] | d_loss: 0.9508 | g_loss: 0.9849 Epoch [ 4/ 50] | d_loss: 1.0338 | g_loss: 1.2916 Epoch [ 4/ 50] | d_loss: 0.7476 | g_loss: 1.7354 Epoch [ 5/ 50] | d_loss: 0.8847 | g_loss: 1.9047 Epoch [ 5/ 50] | d_loss: 0.9131 | g_loss: 2.6848 Epoch [ 6/ 50] | d_loss: 0.3747 | g_loss: 2.0961 Epoch [ 6/ 50] | d_loss: 0.5761 | g_loss: 1.4796 Epoch [ 7/ 50] | d_loss: 1.0538 | g_loss: 2.5600 Epoch [ 7/ 50] | d_loss: 0.5655 | g_loss: 1.1675  View generated Samples def view_samples(epoch, samples): fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True) for ax, img in zip(axes.flatten(), samples[epoch]): img = img.detach().cpu().numpy() img = np.transpose(img, (1, 2, 0)) img = ((img +1)*255 / (2)).astype(np.uint8) ax.xaxis.set_visible(False) ax.yaxis.set_visible(False) im = ax.imshow(img.reshape((32,32,3)))  Training a DCGAN is same as the Linear/Vanilla GAN, DCGANs can extract more features in an image with the CNN and can help in generating the distributions well.\nIn the next post, we will look at Pix2Pix GAN and its applications.\n","date":1558669380,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558669380,"objectID":"9aa73addb97c4bd545885afb2579256c","permalink":"/post/gan-4/","publishdate":"2019-05-24T03:43:00Z","relpermalink":"/post/gan-4/","section":"post","summary":"Deep Convolutional GAN","tags":["Deep Learning post","GAN post"],"title":"GAN 4","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Python","PyTorch"],"content":" We saw an Intro to GANs and the Theory of Game between Generator and Discriminator in the previous posts. In this post we are going to implement and learn about how to train GANs in PyTorch. We will start with MNIST dataset and in the future posts we will implement different applications of GANs and also my research paper on one of the application of GANs.\nSo the task is to use the MNIST dataset to generate new MNIST alike data samples with GANs. Let\u0026rsquo;s Code GAN Get the Data Import all the necessary libraries like Numpy, Matplotlib, torch, torchvision.\nimport numpy as np import torch import matplotlib.pyplot as plt from torchvision import datasets import torchvision.transforms as transforms  Now lets get the MNIST data from the torchvision datasets.\ntransform = transforms.ToTensor() data = datasets.MNIST(root='data', train=True, download=True, transform=transform) data_loader = torch.utils.data.DataLoader(data, batch_size=1024)  The Model As we have already seen in Theory of Game between Generator and Discriminator, the GAN models generally have 2 networks Discriminator D and Generator G. We will code both of these network as seperate classes in PyTorch. Discriminator The discriminator is a just a classifier , which takes input images and classifies the images as real or fake generated images. So lets make a classifier network in PyTorch.\nimport torch.nn as nn import torch.nn.functional as F class D(nn.Module): def __init__(self, input_size, hidden_dim, output_size): super(D, self).__init__() self.fc1 = nn.Linear(input_size, hidden_dim*4) self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2) self.fc3 = nn.Linear(hidden_dim*2, hidden_dim) self.fc4 = nn.Linear(hidden_dim, output_size) self.dropout = nn.Dropout(0.3) def forward(self, x): # flatten image x = x.view(-1, 28*28) x = F.leaky_relu(self.fc1(x), 0.2) x = self.dropout(x) x = F.leaky_relu(self.fc2(x), 0.2) x = self.dropout(x) x = F.leaky_relu(self.fc3(x), 0.2) x = self.dropout(x) out = F.log_softmax(self.fc4(x)) return out  The D network has 4 linear layers with leaky relu and dropout layers in between.\nHere the input size will be 28*28*1 (size of MNIST image)\nhidden dim can be anything of your choice.\noutput_size = 2 (real or fake)\nI am also adding a log softmax in the end for computation purpose.\nLets make a Discriminator object\nD_network = D(28*28*1, 50, 2) print(D_network)  output :\nD( (fc1): Linear(in_features=784, out_features=200, bias=True) (fc2): Linear(in_features=200, out_features=100, bias=True) (fc3): Linear(in_features=100, out_features=50, bias=True) (fc4): Linear(in_features=50, out_features=2, bias=True) (dropout): Dropout(p=0.3) )  Generator The Generator takes a random vector(z)(also called latent vector) and generates a sample image with a distribution close to the training data distribution. We want to upsample z to an image of size 1*28*28. Tanh was used as activation in the output layer(as used in the original paper) , but feel free to try other activations and check which gives good result.\nclass G(nn.Module): def __init__(self, input_size, hidden_dim, output_size): super(G, self).__init__() self.fc1 = nn.Linear(input_size, hidden_dim) self.fc2 = nn.Linear(hidden_dim, hidden_dim*2) self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4) self.fc4 = nn.Linear(hidden_dim*4, output_size) self.dropout = nn.Dropout(0.3) def forward(self, x): x = F.leaky_relu(self.fc1(x), 0.2) x = self.dropout(x) x = F.leaky_relu(self.fc2(x), 0.2) x = self.dropout(x) x = F.leaky_relu(self.fc3(x), 0.2) x = self.dropout(x) out = F.tanh(self.fc4(x)) return out  The G network architecture is same as D\u0026rsquo;s architecture except now we upsample the z to 28*28*1 size image.\nG_network = G(100, 50, 1*28*28) print(G_network)  G( (fc1): Linear(in_features=100, out_features=50, bias=True) (fc2): Linear(in_features=50, out_features=100, bias=True) (fc3): Linear(in_features=100, out_features=200, bias=True) (fc4): Linear(in_features=200, out_features=784, bias=True) (dropout): Dropout(p=0.3) )  Loss The discriminator wants the probability of fake images close to 0 and the generator wants the probability of the fake images generated by it to be close to 1.\nSo we define 2 losses\n Real Loss (loss btw p and 1) Fake loss (loss btw p and 0)  p is the probability of image to be real.\n For Generator : minimize real_loss(p) or p to be closer to 1. ie: fool generator by making realistic images.\n For Discriminator : minimize real_loss + fake loss. ie: p of real image close to 1 and p of fake image close to 0.\n  def real_loss(D_out, smooth=False): batch_size = D_out.size(0) # label smoothing if smooth: # smooth, real labels = 0.9 labels = torch.ones(batch_size)*0.9 else: labels = torch.ones(batch_size) # real labels = 1 criterion = nn.NLLLoss() loss = criterion(D_out.squeeze(), labels.long().cuda()) return loss def fake_loss(D_out): batch_size = D_out.size(0) labels = torch.zeros(batch_size) # fake labels = 0 criterion = nn.NLLLoss() loss = criterion(D_out.squeeze(), labels.long().cuda()) return loss  label smoothing is also done for better convergence.\nTraining We will use 2 optimizers\n One for Generator, which optimizes the real_loss of fake images. ie: it tries to make the classification prediction of fake images equal to 1. Next is discriminator, which tries to optimize real+fake loss. ie: it tries to make the prediciton of fake images to 0 and real images to 1.  Adjust the no of epochs, latent vector size, optimizer parameters, dimensions etc.\nnum_epochs = 100 print_every = 400 # train the network D.train() G.train() for epoch in range(num_epochs): for batch_i, (images, _) in enumerate(train_loader): batch_size = images.size(0) ## Important rescaling step ## real_images = images*2 - 1 # rescale input images from [0,1) to [-1, 1) d_optimizer.zero_grad() D_real = D(real_images) d_real_loss = real_loss(D_real, smooth=True) z = np.random.uniform(-1, 1, size=(batch_size, z_size)) z = torch.from_numpy(z).float() fake_images = G(z) D_fake = D(fake_images) d_fake_loss = fake_loss(D_fake) d_loss = d_real_loss + d_fake_loss d_loss.backward() d_optimizer.step() g_optimizer.zero_grad() z = np.random.uniform(-1, 1, size=(batch_size, z_size)) z = torch.from_numpy(z).float() fake_images = G(z) D_fake = D(fake_images) g_loss = real_loss(D_fake) g_optimizer.step() if batch_i % print_every == 0: print('Epoch {:5d}/{:5d}\\td_loss: {:6.4f}\\tg_loss: {:6.4f}'.format( epoch+1, num_epochs, d_loss.item(), g_loss.item()))  Epoch 1/ 100 d_loss: 1.3925 g_loss: 0.6747 Epoch 2/ 100 d_loss: 1.2275 g_loss: 0.6837 Epoch 3/ 100 d_loss: 1.0829 g_loss: 0.6959 Epoch 4/ 100 d_loss: 1.0295 g_loss: 0.7128 Epoch 5/ 100 d_loss: 1.0443 g_loss: 0.7358 Epoch 6/ 100 d_loss: 1.0362 g_loss: 0.7625 Epoch 7/ 100 d_loss: 0.9942 g_loss: 0.8000 Epoch 8/ 100 d_loss: 0.9445 g_loss: 0.8455 Epoch 9/ 100 d_loss: 0.9005 g_loss: 0.9073 Epoch 10/ 100 d_loss: 0.8604 g_loss: 0.9908 ...  Generate new MNIST Samples def view_samples(epoch, samples): fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True) for ax, img in zip(axes.flatten(), samples[epoch]): img = img.detach() ax.xaxis.set_visible(False) ax.yaxis.set_visible(False) im = ax.imshow(img.reshape((28,28)), cmap='Greys_r') sample_size=16 rand_z = np.random.uniform(-1, 1, size=(sample_size, z_size)) rand_z = torch.from_numpy(rand_z).float() G.eval() rand_images = G(rand_z) view_samples(0, [rand_images])  Linear GAN Model does a decent job in generating MNIST images. In next post we will look into DCGAN(Deep Convolutional GAN), to use CNNs for generating new samples.\nCheck this Awesome Repo on comparing Linear GAN and DCGAN for MNIST. Also this notebook for pytorch implementation of vanilla GAN(Linear).\n","date":1558410180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558410180,"objectID":"14121bcbbeb0a6d310e1ce83d1b3402e","permalink":"/post/gan-3/","publishdate":"2019-05-21T03:43:00Z","relpermalink":"/post/gan-3/","section":"post","summary":"MNIST Linear GAN","tags":["Deep Learning post","GAN post## Supervised learning"],"title":"GAN 3","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Python","PyTorch"],"content":" As we saw in Intro and application of Generative Adversarial Network, GANs generate new data from a given dataset by learning the distribution of the dataset through adversarial process.\nWhat is an adversarial process/learning? A google search can tell you that adversarial machine learning is a technique used in Machine Learning which tries to fool the model by giving in false/malicious input.\nComponents of GANs:  Generator The Generator network takes random noise as input and convert it to a data sample(image/music) . The output of generator is a fake but realistic data sample. The choice of the random noise determines the distribution into which the data sample generated falls.\nBut the generator network have to be trained to produce samples for the given random noise. ie: the generator have to learn the distribution of the dataset, so it generates new data samples from the distribution.\nAs this is not a supervised learning, we cannot use labels to learn the parameters of generator. So we use adversarial learning technique to learn the distribution of the dataset.\nThe idea is to maximize the probability that the data sample generated by the generator is from the training dataset. But it is not easy as its un labelled , so we use the help of another network called Discriminator.\n Discriminator Discriminator is a normal Neural Network classifier. The discriminator finds if a data sample is from the training dataset or not.\nDuring the training process, the discriminator is given data from the training dataset 50% of the time and data samples generated by generator other 50% of the time. The discriminator classifies the generated data samples as fake and data from the training dataset as real data.\n  The Game Theory As the disciminator classifies the data sample from the generator as fake, the generator tries to fool the discriminator by generating more realistic data sample(learns the training data distribution well). The generator starts generating samples more close to the distribution of the training dataset.\nAs the generator tries to fool the discriminator, the discriminator learns to classify the more realistic(fake) data generated by the generator as fake. By this process both the networks learn the parameters which gives best results. This creates a competition between Generator(G) and Discriminator(D), this makes this an adversarial learning.\nCheck this\nIn game theory, an equlibrium is reached in a 2 player game when both the players recieve 0 payoff. When a player(P) wins, P gets a positive payoff of 1 and gets a negative payoff of -1 when loses. When a player loses, the player changes the stratergy to win the next round. As this continues the player becomes better but as the other player also gets better , an equilibrium is reached when both players uses random uniform stratergies. At equilibrium, neither of the players can improve further.\nMost of the machine learning models we used so far depends on optimization algorithms. We finds a set of parameters for which the cost function is minimum. But GANs have 2 players G \u0026amp; D. The G is trying to fool D and D is trying to classify G\u0026rsquo;s sample as fake data. As we can see D is trying to minimize the probability of G\u0026rsquo;s output as true data, whereas G is trying to increase the probability.\nCost of D = minimize(P(generated sample data is real)) Cost of G = maximize(P(generated sample data is real))  Theoritically equlibrium occurs when both probabilities are equal.\nP(generated sample data is real) = 0.5  This occurs for a set of parameters for which the G got the maximum probability and D got minimum probability. ie: a saddle point.\nsaddle point - both local maxima and local minima\nGenerator gets a local maxima when the distribution learned by generator is equal to the distribution of the training dataset.\nWe will use 2 seperate optimization algorithms for D and G, so it is not possible for us to find the equilibrium. But if we can use a single optimization algorithm which reduces both D \u0026amp; G costs together, then we may encounter perfect equilibirum.\nIn the next post, we will look into the practical implementation of GANs by coding and training it in PyTorch.\n","date":1558406580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558406580,"objectID":"d9d7cbd874401a4d0fb6f47eb32582a2","permalink":"/post/gan-2/","publishdate":"2019-05-21T02:43:00Z","relpermalink":"/post/gan-2/","section":"post","summary":"Theory of Game between Generator and Discriminator","tags":["Deep Learning post","GAN post"],"title":"GAN 2","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning"],"content":" GANs are ML models that can imagine new things. GANs can generate new data with a given dataset by learning its distribution. GANs have been mostly used with image data but can also be used on any kind of data. GANs draw a sample from the learned probability distribution of the dataset which is a completely new sample.\nGANs are unsupervised machine learning models , which learns the distribution of the data through adverserial process and generate new sample from the learned distribution.\nSome Recent Research in GAN  Stack GAN Stack GAN can take a description of an image and can generate new images matching that description. GAN picks a sample from a distribution of images which matches the description. slide\n iGAN iGANs can search for realistic possible image as the user draws the rough sketch.\nGithub\n Pix2Pix Images in one domain can be changed to image in another domain with GANS. Rough sketches can be made into a realistic image which are generated by GANs. Blue Prints of a building can be changed to an image of finished building with GANs. Github\n Many other applications like photos to cartoons, daylight image to night scene image, Cycle GAN.\n  Check out all of these Generative models.\nIn the next few posts, we will look deep into how GANs work and code GANs with PyTorch for different applications.\n","date":1558317600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558317600,"objectID":"029dbda1937caf638b86fb2868753ab1","permalink":"/post/gan-1/","publishdate":"2019-05-20T02:00:00Z","relpermalink":"/post/gan-1/","section":"post","summary":"Intro and applications of Generative Adversarial Network","tags":["Deep Learning post","GAN post## Supervised learning"],"title":"GAN 1","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Unsupervised Learning"],"content":" Machine Learning is broadly divided into 3 types:  Supervised learning Unsupervised learning Reinforcement learning  \nSupervised Learning The task in supervised learning is to learn a function to map a data X to a label y. All the classification, regression, object detection/recognition/segmentation generally comes under supervised learning.\nIn supervised learning we have a dataset which contains data X and label y and we need to learn how to find y given X.\nUnsupervised Learning In unsupervised learning, we only have X and not the respective y. The goal is to learn the underlying structure/features of the dataset without any label.\nSome examples of Unsupervised Learning are\n Clustering Clustering is dividing the data into groups through some distance metric, like kmeans clustering.  Feature Learning As the name suggest, learning the features of each of the given data, without its label. This is generally done with a help of a model called Autoencoders.\nAutoencoders take the data X as the label y , it try to recreate the data X given data X and learns some underlying features in that process. We generally take the one of the middle layers of the autoencoder as the encoded feature.  Dimensionality Reduction As we know data can be multi dimensional which can extent even to millions. Computation and visualization of such multi dimensioanl data is difficult and thus we want to reduce the dimension of the data (to pick the dimensions which can represent the data more).\nDimensionality reduction is done by choosing the axis in the data space along which variance of the data is high.  and many other examples like data compression(using auto encoders), Generative models, density estimation, etc.\n  Why Unsupervised Learning?  Unsupervised learning doesn\u0026rsquo;t need labels. Making the training data for supervised learning is not easy. Its expensive, time consuming, labour consuming. The world has a lot of unlabelled data, which can be used directly or with a little pre processing for unsupervised learning.  Unsupervised Learning is still an ameature area of research, which has a lot of potential. Unsupervised learning is less expensive and can accelerate the AI field so much.\n","date":1558310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558310400,"objectID":"27d1b98c303c07483ea9cb766eb656b5","permalink":"/post/unsupervised-learning/","publishdate":"2019-05-20T00:00:00Z","relpermalink":"/post/unsupervised-learning/","section":"post","summary":"Intro to Unsupervised Learning","tags":["Deep Learning post","Unsupervised Learning post"],"title":"Unsupervised Learning 101","type":"post"},{"authors":["Shangeth Rajaa","JK Sahoo"],"categories":null,"content":"Cite as :\nRajaa S., Sahoo J.K. (2019) Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction. In: Singh M., Gupta P., Tyagi V., Flusser J., Ören T., Kashyap R. (eds) Advances in Computing and Data Sciences. ICACDS 2019. Communications in Computer and Information Science, vol 1045. Springer, Singapore\n","date":1558137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558137600,"objectID":"24ca6bd6ed9bcd6819f9c40eefb45e6c","permalink":"/publication/icacds/","publishdate":"2019-05-18T00:00:00Z","relpermalink":"/publication/icacds/","section":"publication","summary":"Stock Prediction with CNN and Neural Arithmetic Logic Units.","tags":["Deep Learning","PyTorch"],"title":"Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction","type":"publication"},{"authors":[],"categories":null,"content":"","date":1555160400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555160400,"objectID":"d13b1f263203cf0884ea5e3c60769dad","permalink":"/talk/icacds/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/icacds/","section":"talk","summary":"Presentation of my paper \"Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction\" at ICACDS 2019 Conference.","tags":[],"title":"Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction","type":"talk"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":[],"categories":null,"content":"","date":1547236800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547236800,"objectID":"381eff0b30d0969cc6d6ae0655de9911","permalink":"/talk/multi-tasking-learning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/multi-tasking-learning/","section":"talk","summary":"A Talk on Multitasking learning as best project chosen by FacebookAI and Udacity.","tags":[],"title":"Multi Tasking Learning","type":"talk"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"33c96636b53dc0f2a4842dacad9785a8","permalink":"/project/character-generating-rnn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/character-generating-rnn/","section":"project","summary":"Character level language model of RNN(LSTM) in PyTorch.","tags":["Deep Learning","Python","NLP","PyTorch"],"title":"Character Generating RNN","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bf3e111ff5fa7319b68614400b61b81d","permalink":"/project/computer-vision-security-system/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/computer-vision-security-system/","section":"project","summary":"Computer vision security system server build with Python, OpenCV, Flask web server.","tags":["Computer Vision","Python","JavaScript"],"title":"Computer Vision Security System","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"282a173f5eca67ca5c3b6c142509f212","permalink":"/project/emojification/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/emojification/","section":"project","summary":"Emojify a sentence with NLP, flask server to emojify a sentence.","tags":["Deep Learning","NLP","Python","Keras","JavaScript"],"title":"Emojification","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9e55cd99254ae71ee3e62fd187a803e2","permalink":"/project/hand-gesture-recognition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/hand-gesture-recognition/","section":"project","summary":"Recognizing the hand gesture using CNN feature extraction.","tags":["Deep Learning","Computer Vision","Python","Tensorflow"],"title":"Hand Gesture Recognition","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"34b1e8cd4adc5242c33b8c3a394f1aa7","permalink":"/project/lane-detection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/lane-detection/","section":"project","summary":"Lane Detection for self driving cars with Deep Learning(CNN) with the camera image data.","tags":["Deep Learning","Python","Computer Vision","Self Driving Cars","Keras"],"title":"Lane Detection with Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a5a90b199d4d8fa71f69281c9c521b93","permalink":"/project/multitasking-learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/multitasking-learning/","section":"project","summary":"Multitasking learning to use the CNN extracted features for multiple tasks like predicting age, sex, face direction, etc.","tags":["Deep Learning","Python","Computer Vision","Keras"],"title":"Multi Tasking Learning for face characterization","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ec641199efbb6bc51972564333aa628f","permalink":"/project/nytimes-topic-modelling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/nytimes-topic-modelling/","section":"project","summary":"Topic Modelling for New York Times news articles for given dates using NYTimes API.","tags":["Topic Modelling","Python","NLP"],"title":"NYTimes Topic Modelling","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9b5611b35d0fe55dcb89e7cf0cd27239","permalink":"/project/neural-style-transfer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/neural-style-transfer/","section":"project","summary":"Neural Style transfer of images in PyTorch.","tags":["Deep Learning","Python","Computer Vision","PyTorch"],"title":"Neural Style Transfer","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"05559a29ede4e1058072fa375b325d5e","permalink":"/project/pyrevshell/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/pyrevshell/","section":"project","summary":"A server client Reverse shell using python, can use any device's shell using this from another device in the network.","tags":["Python"],"title":"PyRevShell","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"befd31eb9f5cdc14aed99a7aac7c42ce","permalink":"/project/self-driving-cars-steering-angle-prediction/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/self-driving-cars-steering-angle-prediction/","section":"project","summary":"Prediction of which direction the car should change the steering direction in autonomous mode with the camera image as the input using transfer learning and fine tuning.","tags":["Deep Learning","Computer Vision","Self Driving Cars","Python","Tensorflow"],"title":"Self Driving Cars Steering Angle Prediction","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"90fc0166029130a6e9e179b6951e6cca","permalink":"/project/self-driving-cars-vehicle-detection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/self-driving-cars-vehicle-detection/","section":"project","summary":"Detection of other vehicles for self driving cars with YOLO in tensorflow.","tags":["Deep Learning","Computer Vision","Self Driving Cars","Python","Tensorflow"],"title":"Self Driving Cars Vehicle Detection","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8491d5ad45e23efbef2c021b6d1d1f3e","permalink":"/project/seq2seq-machine-translation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/seq2seq-machine-translation/","section":"project","summary":"Machine Translation english to french using Seq2Seq Attention model in PyTorch.","tags":["Deep Learning","Python","NLP","PyTorch"],"title":"Seq2Seq Machine Translation","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f116c6b6180f2fdf083da45996812e52","permalink":"/project/signature-verification/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/signature-verification/","section":"project","summary":"Signature verification with siamese network.","tags":["Deep Learning","Computer Vision"],"title":"Signature Verification with Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d67e120ff9736002127131016086076c","permalink":"/project/sockchat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/sockchat/","section":"project","summary":"Sock Chat is a server-client chat application with database which can be used to any web server or software applications,","tags":["Python"],"title":"SockChat","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"28c0d4e4fc2f501dcb486c837c012cc7","permalink":"/project/tictactoe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/tictactoe/","section":"project","summary":"2 player Tic Tac Toe game programmed in JavaScript.","tags":["JavaScript"],"title":"Tic Tac Toe","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cafd875167c47b3bd805f99a1d48fd56","permalink":"/project/nltk-sentiment-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/nltk-sentiment-analysis/","section":"project","summary":"Sentiment analysis of tweets of any topic fetched with twitter API and sentiment analysis of the tweets with NLTK.","tags":["NLP","Python","JavaScript"],"title":"Twitter Sentiment analysis","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"34c0ac182533d3a863fb69d13e227a5a","permalink":"/project/vehicle-speed-estimation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/vehicle-speed-estimation/","section":"project","summary":"Estimation of a vehicle's speed with its camera frames using deep leanring in PyTorch.","tags":["Deep Learning","Python","Computer Vision","Self Driving Cars","PyTorch"],"title":"Vehicle Speed Estimation","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bb5ee3b1d3ce8470980e92fe2fd3ca24","permalink":"/project/web-builder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/web-builder/","section":"project","summary":"Live HTML, CSS, JavaScript code output.","tags":["JavaScript"],"title":"Web Builder","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5b420fd0a369ab016ae61c3b944068f2","permalink":"/project/weekly-scheduler/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/weekly-scheduler/","section":"project","summary":"This a web weekly scheduler build with JavaScript.","tags":["JavaScript"],"title":"Weekly Scheduler","type":"project"}]