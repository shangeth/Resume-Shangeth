[{"authors":["admin"],"categories":null,"content":" I am a final year Undergrad Dual Degree student at BITS Pilani Goa Campus. Research Intern at IBM Research Labs. Actively Looking for full time research opportunities in Deep Learning to start from Jan 2021.  ","date":1580004000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1580004000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":" I am a final year Undergrad Dual Degree student at BITS Pilani Goa Campus. Research Intern at IBM Research Labs. Actively Looking for full time research opportunities in Deep Learning to start from Jan 2021.  ","tags":null,"title":"Shangeth Rajaa","type":"authors"},{"authors":null,"categories":null,"content":" Explore Machine Learning (ML) is a Google-sponsored program for university students to get started with Machine Learning. The curriculum offers 3 tracks of machine learning content (Beginner, Intermediate, Advanced) and relies on university student facilitators to train other students on campus.\nContent This course will be part of Explore Machine Learning by Google. The Basic and Intermediate track will be taught in the Fall 2019 at BITS Pilani Goa Campus by Shangeth. The Basic track introduces to deep learning problems(regression and classification) and training Deep Neural Networks for a variety of real world problems like Text Sentiment analysis with ANNs.\nThe intermediate track will continue from basic track and introduce many deep learning architectures like CNNs, RNNs, GANs,..etc. Projects involving Computer vision and Natural language processing will be part of the Intermediate track.\nStudents will be certified separately for Basic and Intermediate track by Google AI. Attending all offline lectures at the campus is a must to be eligible for the certification. Venue and Timing of lectures will be decided based on availability of instructor and students every week.\nShangeth Rajaa\nDeep Learning Content Developer, OpenCV.org   -- Prerequisites  Python3 Basic Programming knowledge of functions, Object Oriented Programming. Scientific libraries like Numpy, Pandas, Matplotlib. Basic knowledge of Multi Variable Calculus, Linear Algebra, Probability \u0026amp; Statistics. If you are BITS Pilani student the following course will be more than enough to get into the course.  CS F111 MATH F111 MATH F112 MATH F113   Study Materials and References If you do not have the prerequisite stated above, check these open study materials.\n Python\nIf you are just starting Python, Do not spend much time on it. Just learn the basic syntax, functions, classes. That will be enough, you will have a lot of practice in this course through mini-assignments and mini-projects. These are some free courses for Python.\n Python for Absolute Beginners Introduction To Python Programming  Basic Data Science Packages\nYou also need to learn about how to use packages like Numpy, Pandas, Matplotlib, sklearn..etc. You can also use their docs to learn these.A very basic understanding of these packages will be enough to start this course.\n Into to Data Analysis Numpy and Matplotlib Pandas  Mathematics\nYou only need a basic understanding of simple concepts like differentiation, gradients, matrix multiplication, inverse of matrix,..etc. Youtube has loads of good videos on these concepts.\n Mathematics for ML Youtube channels like Khan Academy, 3Blue1Brown have some good content to start.   Help Me improve the Course Content Loadingâ€¦ ","date":1567036800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1567036800,"objectID":"c141e591b3033d9b3e1ddb2c54a76985","permalink":"/courses/deeplearning/","publishdate":"2019-08-29T00:00:00Z","relpermalink":"/courses/deeplearning/","section":"courses","summary":"Explore Machine Learning (ML) is a Google-sponsored program for university students to get started with Machine Learning. The curriculum offers 3 tracks of machine learning content (Beginner, Intermediate, Advanced) and relies on university student facilitators to train other students on campus.\nContent This course will be part of Explore Machine Learning by Google. The Basic and Intermediate track will be taught in the Fall 2019 at BITS Pilani Goa Campus by Shangeth.","tags":null,"title":"Course Overview","type":"docs"},{"authors":null,"categories":null,"content":" On It Check out the Basic Track  Deep Learning - Basic Track  ","date":1567036800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1568246400,"objectID":"70dd0682bfa5bca3a91fe4d0eda2af2c","permalink":"/courses/deeplearning-2/","publishdate":"2019-08-29T00:00:00Z","relpermalink":"/courses/deeplearning-2/","section":"courses","summary":" On It Check out the Basic Track  Deep Learning - Basic Track  ","tags":null,"title":"Course Overview","type":"docs"},{"authors":null,"categories":null,"content":" Will be added after Fall 2019 ðŸ˜„ Check out the Basic and Intermediate Tracks  Deep Learning - Basic Track Deep Learning - Intermediate Track  ","date":1567036800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1568246400,"objectID":"f3524eeccc2eb5df9b4c74625ab91257","permalink":"/courses/deeplearning-3/","publishdate":"2019-08-29T00:00:00Z","relpermalink":"/courses/deeplearning-3/","section":"courses","summary":" Will be added after Fall 2019 ðŸ˜„ Check out the Basic and Intermediate Tracks  Deep Learning - Basic Track Deep Learning - Intermediate Track  ","tags":null,"title":"Course Overview","type":"docs"},{"authors":null,"categories":null,"content":"Deep Learning - Beginners Track Instructor: Shangeth Rajaa \n ","date":1568761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568761200,"objectID":"adf42e96bfbe69cb95baf8bfcb1b91da","permalink":"/courses/deeplearning/0.1/","publishdate":"2019-09-18T00:00:00+01:00","relpermalink":"/courses/deeplearning/0.1/","section":"courses","summary":"Deep Learning - Beginners Track Instructor: Shangeth Rajaa \n ","tags":null,"title":"Data and Learning Problem","type":"docs"},{"authors":null,"categories":null,"content":"","date":1568242800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568242800,"objectID":"c0585eff1ef74b468f099af58f52218a","permalink":"/courses/deeplearning-2/3.1/","publishdate":"2019-09-12T00:00:00+01:00","relpermalink":"/courses/deeplearning-2/3.1/","section":"courses","summary":"","tags":null,"title":"Neural Networks for Vision","type":"docs"},{"authors":null,"categories":null,"content":" \n Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nBefore starting with Neural Networks, we will look into 2 important machine learning models to understand regression and classification tasks\n Linear Regression (Regression) Logistic Regression (Classification)  \nYou can think of Linear Regression model as a curve fitting or function approximation model. Given a dataset $(X, y)$, the task is to find a relation $f$ between $X$ and $y$ such that $y = f(X)$. We are interested in this mapping $f: X \\rightarrow y$, as for any given $X$ in the future we can find $y = f(X)$.\nFor example, given a dataset about housing prices vs area of the house(Toy Dataset)\n   House Price($) y Area(1000 sqft) X     1034 2.4   1145 2.7   1252 3.04   2231 4.67   3423 5.3   \u0026hellip; \u0026hellip;    If we can find any the mapping between $X$ and $y$, $y = f(X)$, then its easy to predict the values of $y$ for any given $X$.\n# example dataset for linear regression %matplotlib inline import numpy as np import matplotlib.pyplot as plt x = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 plt.figure(figsize=(12,7)) plt.scatter(x, y, label='Data $(x,y)$') plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  So the prediction will be a line in case of 2-D data like above.\nThe predicted model will be line $ y = m x + c $ or in ML world popularly $ y = w x + b $.\n$y = f(X) = wX+b$ will be the Linear regression model and the objective is to find the best $w$ and $b$ that will give the nearest values for each $(X,y=wX+b)$ pair in the dataset.\n# example dataset for linear regression import numpy as np import matplotlib.pyplot as plt x = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 plt.figure(figsize=(12,7)) plt.scatter(x, y, label='Data $(x,y)$') plt.plot(x, 2 * x + 1, label='Predicted Line $y = f(X)$', color='r',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  What about Multi Dimensional Data? In the real world, the dataset is going to be multi dimensional, ie: the price of a House will not just depend on the area of the house, it may also depend on multiple factors like no of rooms, locality, distance from airport, etc.\n   House Price($) y Area(1000 sqft) x1 No of Rooms x2 Distance from airport x3     1034 2.4 2 5.4   1145 2.7 3 3.1   1252 3.04 3 4.21   2231 4.67 2 2.3   3423 5.3 1 12   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;    In this case the model will be $y = f(X) = w_1x_1 + w_2x_2+w_3x_3+\u0026hellip;+w_nx_n + b$\nThis can also be represented in matrix form as $y = f(X) = X.W + b$\n$$X = \\begin{bmatrix} x_1 \u0026amp; x_2 \u0026amp; x_3 \u0026amp; \\dots \u0026amp; x_n \\end{bmatrix}$$\n$$W = \\begin{bmatrix}x_1\\\\x_2\\\\ \\vdots\\\\x_n\\end{bmatrix}$$\n$$b = [b]$$\nHow good is the Model? Before learning how to find $W$ and $b$ of a model. Let us assume we have a calculated values for optimal $W$ and $b$, how do we know how good is this model $\\hat{y} = f(X) = X.W + b$.\nNotice it is not $y$, its $\\hat{y}$. Because W and b are not the exact values, they are calculated approximate values to get $f(X)$ close to $y$. So we represent the prediction as $\\hat{y} = X.W + b$ and the true target as $y$.\nOur goal is to make $\\hat{y}$ as close as possible to $y$.\nSo we use a metric function call Mean Squared Error (MSE)\n$MSE(y, \\hat{y}) = \\mathcal{L}(y , \\hat{y}) = \\dfrac{1}{n} \\sum_{i=1}^{i=n}{(y_i - \\hat{y}_i)^2}$\nWhy MSE? Why not just subtraction? - model with 5 is better than -5 in subtraction which actually is not both models are equally bad, but the predictions are in opposite direction. - MSE takes care of -5 and 5 ; $(-5)^2$ = $5^2$ = 25.\nimport numpy as np def MSE(y, y_hat): num_ex = len(y) mse_loss = np.sum((y - y_hat)**2)/num_ex return mse_loss  y = np.array([1.02, 2.3, 6.7, 3]) y_hat1 = np.array([10.43, 23.4, 12, 11]) y_hat2 = np.array([1, 1.9, 7, 3.1]) MSE(y,y), MSE(y, y_hat1), MSE(y, y_hat2)  (0.0, 156.46202499999998, 0.06509999999999996)  # example dataset for linear regression import numpy as np import matplotlib.pyplot as plt x = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 w = -1 b = 0 y_hat = w * x + b mse_loss = MSE(y, y_hat) plt.figure(figsize=(12,7)) plt.scatter(x, y, label='Data $(x,y)$') plt.plot(x, y_hat, label='Predicted Line $y = f(X)$', color='r',linewidth=4.0) plt.text(0,-40,'MSE = {:.3f}'.format(mse_loss), fontsize=20) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  # example dataset for linear regression import numpy as np import matplotlib.pyplot as plt x = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 w = 1 b = 0.5 y_hat = w * x + b mse_loss = MSE(y, y_hat) plt.figure(figsize=(12,7)) plt.scatter(x, y, label='Data $(x,y)$') plt.plot(x, y_hat, label='Predicted Line $y = f(X)$', color='r',linewidth=4.0) plt.text(0,-40,'MSE = {:.3f}'.format(mse_loss), fontsize=20) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  You can evidently see that a better model have less MSE loss. So the object of finding $W$ and $b$ will be to reduce the MSE Loss.\nIts is not generally possible to get a MSE of 0 in real world data, as the real world data is always noisy and with outliers.\nHow to find the Best Model? (W, b)  randomly initialize W, b in loop for n steps/epochs{  find $\\hat{y} = X.W + b$ find $MSE = \\mathcal{L}(y, \\hat{y})$ find $ \\frac{\\partial \\mathcal{L}}{\\partial w} $ and $ \\frac{\\partial \\mathcal{L}}{\\partial b} $ Update W and b with $w := w - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w}$ and $b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$ }   This is called Gradient Descent.\nLet\u0026rsquo;s try it first and see why it works.\n$\\mathcal{L}= \\dfrac{1}{n} \\sum_{i=1}^{i=n}{(y_i - wx^i-b)^2} $\n$ \\dfrac{\\partial \\mathcal{L}}{\\partial w} = \\dfrac{1}{n} \\sum_{i=1}^{i=n}{ 2 (y_i - wx^i-b) (-x^i)} $\n$= \\dfrac{2}{n} \\sum_{i=1}^{i=n}{ (wx^i+b - y_i) (x^i)}$\n$ \\dfrac{\\partial \\mathcal{L}}{\\partial b} = \\dfrac{1}{n} \\sum_{i=1}^{i=n}{ 2 (y_i - wx^i-b) (-1)}$\n$= \\dfrac{2}{n} \\sum_{i=1}^{i=n}{ (wx^i+b - y_i)}$\nTherefore\n$w := w - \\alpha \\dfrac{2}{n} (wX+b - y) (X)$\n$b := b - \\alpha \\dfrac{2}{n} \\sum_{i=1}^{i=n}{ (wX+b - y_i)}$\ndef model_forward(x, w, b): y_hat = w * x + b return y_hat def gradient_descent(w, b, X, y, a): w = w - a * 2 / X.shape[0] * np.dot(X.T, model_forward(X, w, b)- y) b = b - a * 2 / X.shape[0] * np.sum(model_forward(X, w, b)- y) return w, b  w, b = np.random.random(1), np.random.random(1) X = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 y_hat = model_forward(X, w, b) MSE(y, y_hat) # of randomly initialized model  831.0459153266066  losses = [] alpha = 0.00001 for i in range(1000): y_hat = model_forward(X, w, b) mse = MSE(y_hat, y) losses.append(mse) w, b = gradient_descent(w, b, X, y, alpha) if i%500 == 0: y_hat = model_forward(X, w, b) mse = MSE(y_hat, y) plt.figure(figsize=(12,7)) plt.scatter(X, y, label='Data $(X, y)$') plt.plot(X, y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,-30,'Epoch = {}'.format(i+1), fontsize=20) plt.text(0,-40,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show() plt.plot(losses) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show()  You can see how the loss decreases and the model prediction becomes better with the number of epochs.\nTry to run the code with different alpha, different no of epochs and different initialization of w and b.\nLinear Regression in TensorFlow import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np def MSE(y, y_hat): num_ex = len(y) mse_loss = np.sum((y - y_hat)**2)/num_ex return mse_loss X = np.arange(-25, 25, 0.5).astype('float32') y = (2 * X + 1 + np.random.randn(100)*5).astype('float32') model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])]) model.compile(optimizer='adam', loss='mean_squared_error') tf_history = model.fit(X, y, epochs=1000, verbose=False) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X, y, label='Data $(X, y)$') plt.plot(X, y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,-40,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Lets look into each line - model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n Sequential is like a container or wrapper which holds all the operations to be performed on your input.   Compile configures the training process. It defines the metrics, optimizers,..etc. You will understand more about this as we go on with more examples. - ``` tf_history = model.fit(x, y, epochs=1000, verbose=False) ``` model.fit actually trains the model for given number of epochs. Note: We've used Verbose=False(as the output will be long), set it to true to check the loss and metrics for each epoch. Just 3-4 line of Tensorflow code can train a machine learning model, this is the advantage of using packages like Tensorflow/PyTorch. These packages are best optimized for speed and performance.  python\n  model.summary()\n Model: \u0026quot;sequential\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 1) 2 ================================================================= Total params: 2 Trainable params: 2 Non-trainable params: 0 _________________________________________________________________  python tf.keras.Sequential? # if you are not sure of any library, its better to look into the docs. ```\n","date":1567033200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567033200,"objectID":"a10d6f2f26bcdd489fcd40ec7117470c","permalink":"/courses/deeplearning/1.1/","publishdate":"2019-08-29T00:00:00+01:00","relpermalink":"/courses/deeplearning/1.1/","section":"courses","summary":"Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nBefore starting with Neural Networks, we will look into 2 important machine learning models to understand regression and classification tasks\n Linear Regression (Regression) Logistic Regression (Classification)  \nYou can think of Linear Regression model as a curve fitting or function approximation model. Given a dataset $(X, y)$, the task is to find a relation $f$ between $X$ and $y$ such that $y = f(X)$.","tags":null,"title":"Linear Regression","type":"docs"},{"authors":null,"categories":null,"content":"   Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nSolutions Here:\n\n Open in GitHub\nIt\u0026rsquo;s a very simple assignment, you can finish it in less than 10 minutes. If you are stuck somewhere refer this for solutions.\nTask-1 : Linear Regression on Non-Linear Data  Get X and y from dataset() function Train a Linear Regression model for this dataset. Visualize the model prediction  Dataset Call dataset() function to get X, y\nimport numpy as np import matplotlib.pyplot as plt def dataset(show=True): X = np.arange(-25, 25, 0.1) # Try changing y to a different function y = X**3 + 20 + np.random.randn(500)*1000 if show: plt.scatter(X, y) plt.show() return X, y X, y = dataset()  Scaling Dataset The maximum value of y in the dataset goes upto 15000 and the minimum values is less than -15000. The range of y is very large which makes the convergence/loss reduction slower. So will we scale the data, scaling the data will help the model converge faster. If all the features and target are in the same range, there will be symmetry in the curve of Loss vs weights/bias, which makes the convergence faster.\nWe will do a very simple type of scaling, we will divide all the values of the data with the maximum values for X and y respectively.\nX, y = dataset() print(max(X), max(y), min(X), min(y)) X = X/max(X) y = y/max(y) print(max(X), max(y), min(X), min(y))  24.90000000000071 16694.307606867886 -25.0 -16126.103960535462 1.0 1.0 -1.0040160642569995 -0.9659642280642613  This is not a great scaling method, but good to start. We will see many more scaling/normalization methods later.\nTry training the model with and without scaling and see the difference yourself.\nLinear Regression in TensorFlow import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np X, y = dataset(show=False) X_scaled = X/max(X) y_scaled = y/max(y) model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])]) # you can also define optimizers in this way, so you can change parameters like lr. optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_scaled, y_scaled, epochs=500, verbose=True) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_scaled) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_scaled, y_scaled, label='Data $(X, y)$') plt.plot(X_scaled, y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  WARNING: Logging before flag parsing goes to stderr. W0829 03:45:55.755687 139671975163776 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor Epoch 1/500 500/500 [==============================] - 0s 951us/sample - loss: 0.2951 Epoch 2/500 500/500 [==============================] - 0s 41us/sample - loss: 0.2856 . . Epoch 499/500 500/500 [==============================] - 0s 41us/sample - loss: 0.0264 Epoch 500/500 500/500 [==============================] - 0s 39us/sample - loss: 0.0263  Looks the model Prediction for this dataset is not very great, but that is expected as the model is a straight line, it cannot predict non linear regression data. Is there a way to train a regression model for this task?\nPolynomial Regression So when the dataset is not linear, linear regression cannot learn the dataset and make good predictions.\nSo we need a polynomial model which considered the polynomial terms as well. So we need terms like $x^2$, $x^3$, \u0026hellip;, $x^n$ for the model to learn a polynomial of $n^{th}$ degree.\n$\\hat{y} = w_0 + w_1x + w_2x^2 + \u0026hellip; + w_nx^n$\nOne down side of this model is that, We will have to decide the value of n. But this is better than a linear regression model. We can get an idea of the value of n by visualizing a dataset, but for multi variable dataset, we will have to try different values of n and check which is better.\nPolynomial Features you can calculate the polynomial features for each feature by programming it or you can try sklearn.preprocessing.PolynomialFeatures which allows us to make polynomial terms of our data.\nWe will try degree 2, 3 and 4\nX, y = dataset(show=False) X_scaled = X/max(X) y_scaled = y/max(y)  from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=2) X_2 = poly.fit_transform(X_scaled.reshape(-1,1)) print(X_2.shape) print(X_2[0])  (500, 3) [ 1. -1.00401606 1.00804826]  from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=3) X_3 = poly.fit_transform(X_scaled.reshape(-1,1)) print(X_3.shape) print(X_3[0])  (500, 4) [ 1. -1.00401606 1.00804826 -1.01209664]  from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=4) X_4 = poly.fit_transform(X_scaled.reshape(-1,1)) print(X_4.shape) print(X_4[0])  (500, 5) [ 1. -1.00401606 1.00804826 -1.01209664 1.01616129]  The PolynomialFeatures returns $[1, x, x^2, x^3,\u0026hellip;]$.\nTask - 2  Train a model with polynomial terms in the dataset. Visualize the prediction of the model  The code remains the same except the no of input features will be 3, 4, 5 respectively.\nTensorflow Model with 2nd Degree import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[3])]) optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_2, y_scaled, epochs=500, verbose=True) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_2) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_2[:, 1], y_scaled, label='Data $(X, y)$') plt.plot(X_2[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Epoch 1/500 500/500 [==============================] - 0s 332us/sample - loss: 0.5278 Epoch 2/500 500/500 [==============================] - 0s 38us/sample - loss: 0.4773 . . Epoch 499/500 500/500 [==============================] - 0s 44us/sample - loss: 0.0242 Epoch 500/500 500/500 [==============================] - 0s 43us/sample - loss: 0.0242  Why is the polynomial regression with 2-d features look like a straight line?\nWell because the model thinks that a straight line(look like, we can\u0026rsquo;t be sure if it\u0026rsquo;s a straight like, it can a parabola as well) better fits the dataset than a parabola. If you train the model for less epochs you can notice the model tries to fit the data with a parabola(2-d) but it improves as it moves to a line.\nTrain the same model for may be 50 epochs.\nimport tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[3])]) optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_2, y_scaled, epochs=50, verbose=True) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_2) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_2[:, 1], y_scaled, label='Data $(X, y)$') plt.plot(X_2[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Epoch 1/50 500/500 [==============================] - 0s 370us/sample - loss: 0.8566 Epoch 2/50 500/500 [==============================] - 0s 38us/sample - loss: 0.7970 . . Epoch 49/50 500/500 [==============================] - 0s 38us/sample - loss: 0.1027 Epoch 50/50 500/500 [==============================] - 0s 35us/sample - loss: 0.1000  You can clearly see that the model tries to fit the data with a parabola which doesn\u0026rsquo;t seem to fit well, so it changes the parameters to fit a line.\nTensorflow Model with 3rd Degree import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np model = tf.keras.Sequential([keras.layers.Dense(unit's=1, input_shape=[4])]) optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_3, y_scaled, epochs=500, verbose=True) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_3) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_3[:, 1], y_scaled, label='Data $(X, y)$') plt.plot(X_3[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Epoch 1/500 500/500 [==============================] - 0s 456us/sample - loss: 0.4445 Epoch 2/500 500/500 [==============================] - 0s 40us/sample - loss: 0.3993 . . Epoch 499/500 500/500 [==============================] - 0s 38us/sample - loss: 0.0040 Epoch 500/500 500/500 [==============================] - 0s 37us/sample - loss: 0.0039  3-D features perfectly fit the data with a 3rd degree polynomial as expected.\nTensorflow Model with 4th Degree import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[5])]) optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_4, y_scaled, epochs=500, verbose=True) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_4) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_4[:, 1], y_scaled, label='Data $(X, y)$') plt.plot(X_4[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Epoch 1/500 500/500 [==============================] - 0s 240us/sample - loss: 0.5839 Epoch 2/500 500/500 [==============================] - 0s 37us/sample - loss: 0.5453 . . Epoch 499/500 500/500 [==============================] - 0s 37us/sample - loss: 0.0040 Epoch 500/500 500/500 [==============================] - 0s 39us/sample - loss: 0.0040  4th degree poly-regression also did a good job in fitting the data as it also have the 3rd degree terms.\nimport tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[5])]) optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_4, y_scaled, epochs=50, verbose=True) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_4) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_4[:, 1], y_scaled, label='Data $(X, y)$') plt.plot(X_4[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Epoch 1/50 500/500 [==============================] - 0s 181us/sample - loss: 0.6724 Epoch 2/50 500/500 [==============================] - 0s 35us/sample - loss: 0.6104 . . Epoch 48/50 500/500 [==============================] - 0s 34us/sample - loss: 0.0661 Epoch 49/50 500/500 [==============================] - 0s 35us/sample - loss: 0.0655 Epoch 50/50 500/500 [==============================] - 0s 38us/sample - loss: 0.0648  If you run the 4th degree poly-regression for fewer epochs, you can notice, the model tries to fit a 4th(or higher than 3rd) degree polynomial but as the loss is high, the model changes it parameters to set the 4th degree terms to almost 0 and thus giving a 3rd degree polynomial as you train for more epochs.\nThis is polynomial regression. Yes, it\u0026rsquo;s easy. But one issue, as this was a toy dataset we know it\u0026rsquo;s a 3rd degree data, so we tried 2,3,4. But when the data is multi dimensional we cannot visualize the dataset, so it\u0026rsquo;s difficult to decide the degree. This is why you will see Neural Networks are awesome. They are End-End, they do not need several feature extraction from our side, they can extract necessary features of their own.\nMake a Higher degree (4th/5th degree) data and try polynomial regression on it. Also try different functions like exponents, trigonometric..etc.\n","date":1567033200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567033200,"objectID":"82460438687106b7ac34b6715cde5de0","permalink":"/courses/deeplearning/1.2/","publishdate":"2019-08-29T00:00:00+01:00","relpermalink":"/courses/deeplearning/1.2/","section":"courses","summary":"Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nSolutions Here:\n\n Open in GitHub\nIt\u0026rsquo;s a very simple assignment, you can finish it in less than 10 minutes. If you are stuck somewhere refer this for solutions.\nTask-1 : Linear Regression on Non-Linear Data  Get X and y from dataset() function Train a Linear Regression model for this dataset. Visualize the model prediction  Dataset Call dataset() function to get X, y","tags":null,"title":"Polynomial Regression","type":"docs"},{"authors":null,"categories":null,"content":" \n Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nLogistic Regression is one of the most commonly used classification models. Unlike Linear Regression which predicts a real unbounded value $\\hat{y} = f(X) = WX+b$, Logistic Regression predicts the probability of a data belonging to a particular class.\n\nFor example for a given data (X,y) where X is a received email and y is 0 if email is spam and 1 if email is not spam. Logistic regression predicts the probability of the email X to be not spam. $\\hat{y} = f(X) = P(y=1|X)$\n\nSo if $\\hat{y} = P(y=1|X) \u0026gt; 0.5$ then the probability of the email to be spam ih high, so $X$ is a spam email.\nOther examples of classification can be\n Classification of image as cat, dog, parrot $ y = {0, 1, 2}$. Here y can be a 0 or 1 or 2 depending on the probability of model prediction. (Multi class Classification) Classification of Cancer report as Malignant/Benign $y = {0, 1}$. (Binary Classification)  Why not use Linear Regression Model for classification? $\\hat{y}_{linreg} = WX+b$ where $\\hat{y} \\in \\mathbf{R} $, so the prediction can take value from $-\\infty$ to $\\infty$.\n$\\hat{y}_{classification} \\in { 0, 1, 2, \u0026hellip;, n }$, Classification prediction takes discrete values depending on number of classes.\nSo we need a model which limits the prediction in the range ${0,1}$ for binary classification and ${0,1, 2, \u0026hellip;, n}$ for multi-class classification.\nClassification Data Let\u0026rsquo;s use sklearn.datasets.make_blobs to make a random classification dataset in 2D space so we can visualize it.\nWe are generating less data for visualization, for training we will use more data.\nfrom sklearn.datasets import make_blobs # 10 examples, (X)2 features, (y)2 classes X, y = make_blobs(n_samples=10, n_features=2, centers=2, random_state=0) X.shape, y.shape  ((10, 2), (10,))  Let\u0026rsquo;s separate the 0 and 1 class to visualize it.\nimport numpy as np class_0 = np.where(y == 0) class_1 = np.where(y == 1) X_0 = X[class_0] X_1 = X[class_1] X_0.shape, X_1.shape  ((5, 2), (5, 2))  import matplotlib.pyplot as plt plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  One possible linear classifier for this dataset can be\nimport matplotlib.pyplot as plt x = np.arange(-1, 5, 0.5) y = 3 - 0.5 * x plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.plot(x, y, label='Classifier', color='black') plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Let us consider the line $y = 3-x/2$ is the best classifier which separates the data.\nLet $C(x,y) = y + x/2-3 $.\nBy basic school geometry, we know points in opposite side of a line $C(x,y)$ will give opposite values on $C(x,y)$.\nie: $C(x_1,y_1).C(x_2,y_2) \u0026lt; 0$, then $(x_1, y_1)$ and $(x_2, y_2)$ lies in the opposite side of $C(x,y)$.\nWhy are we even talking about this property? Well, this can tell something about how good a classifier is.\n Let us take 2 classifiers  one which classifies all points correctly one which misclassified few points  calculate the $C(x,y)$ for every point and check how its different for both classifiers.\nimport matplotlib.pyplot as plt x = np.arange(-1, 5, 0.5) y = 3 - 0.5 * x plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.plot(x, y, label='Classifier', color='black') for i in range(len(X_0)): plt.text(X_0[i, 0]+0.1, X_0[i, 1], '$C = {:.3f}$'.format(X_0[i, 0]/2 + X_0[i, 1]- 3), fontsize=20) for i in range(len(X_1)): plt.text(X_1[i, 0]+0.1, X_1[i, 1], '$C = {:.3f}$'.format(X_1[i, 0]/2 + X_1[i, 1]- 3), fontsize=20) plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()   Now let\u0026rsquo;s try with a bad classifier\nimport matplotlib.pyplot as plt x = np.arange(-1, 5, 0.5) y = 3 - x plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.plot(x, y, label='Classifier', color='black') for i in range(len(X_0)): plt.text(X_0[i, 0]+0.1, X_0[i, 1], '$C = {:.3f}$'.format(X_0[i, 0] + X_0[i, 1]- 3), fontsize=20) for i in range(len(X_1)): plt.text(X_1[i, 0]+0.1, X_1[i, 1], '$C = {:.3f}$'.format(X_1[i, 0] + X_1[i, 1]- 3), fontsize=20) plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  You can see, the same class points have the same sign when the classifier is good. So this metric C is actually a good way to find out how good a classifier is.\nWe would like to know the probability of a point to be class 1. $P(y=1|X)$\nWe can do this by converting $C(x,y)$ into a range $[0,1]$ using a function called Sigmoid/Logistic.\nSigmoid/Logistic Function $g(x) = \\dfrac{1}{1+e^{-x}}$ Sigmoid can convert a number in Real range to $[0, 1]$ which is what we need to convert the score $C(x,y)$ to probability $P(y=1|X)$.\nLet\u0026rsquo;s code sigmoid in Numpy.\nimport numpy as np def sigmoid(x): return 1/(1+np.exp(-x))  a = np.array([-200,980, 0.1, -23, 1e-3]) sigmoid(a)  array([1.38389653e-87, 1.00000000e+00, 5.24979187e-01, 1.02618796e-10, 5.00250000e-01])  Sigmoid converted all the number from range 980 to -200 into a range of [0,1].\nProbabilities with Sigmoid import matplotlib.pyplot as plt x = np.arange(-1, 5, 0.5) y = 3 - 0.5 * x plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.plot(x, y, label='Classifier', color='black') for i in range(len(X_0)): plt.text(X_0[i, 0]+0.1, X_0[i, 1], '$\\hat{{y}} = {:.3f}$'.format(sigmoid(X_0[i, 0]/2 + X_0[i, 1]- 3)), fontsize=20) for i in range(len(X_1)): plt.text(X_1[i, 0]+0.1, X_1[i, 1], '$\\hat{{y}} = {:.3f}$'.format(sigmoid(X_1[i, 0]/2 + X_1[i, 1]- 3)), fontsize=20) plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  So sigmoid gives the probability of the point to be class \u0026lsquo;1\u0026rsquo; ie: $P(y=1|X)$.\nNote: this can also be $\\hat{y} = P(y=0|X)$, it depends on how you define 1 and 0 class. you can define it either way, but usually we use $\\hat{y} = P(y=1|X)$.\nWhen $\\hat{y} \u0026gt; 0.5$ we classify it as class \u0026ldquo;1\u0026rdquo; and when $\\hat{y} \u0026lt;= 0.5$ we classify it as class \u0026ldquo;0\u0026rdquo;.\nHow to compare the models? We still didn\u0026rsquo;t learn about how to find the best model. But let\u0026rsquo;s say we have 2 models, how do we compare which one is the best?\nBy comparing the $\\hat{y}$(prediction of the model) and $y$(true label) of each data.\nimport matplotlib.pyplot as plt x = np.arange(-1, 5, 0.5) y = 3 - 0.5 * x plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.plot(x, y, label='Classifier', color='black') for i in range(len(X_0)): plt.text(X_0[i, 0]+0.1, X_0[i, 1]-0.2, '$\\hat{{y}} = {:.3f}$\\n$y=1$'.format(sigmoid(X_0[i, 0]/2 + X_0[i, 1]- 3)), fontsize=20) for i in range(len(X_1)): plt.text(X_1[i, 0]+0.1, X_1[i, 1]-0.2, '$\\hat{{y}} = {:.3f}$\\n$y=0$'.format(sigmoid(X_1[i, 0]/2 + X_1[i, 1]- 3)), fontsize=20) plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Maximum Likelihood $\\hat{y} = g(W.X+b)$ gives the probability of $X$ belonging to class ${1}$. We want every point to have maximum predicted probability of that point to have its true label.ie: - if true label $y=1$, then we want to maximize $\\hat{y}_1 = \\hat{y} = g(W.X+b)$. - if true label $y=0$, then we want to maximize $\\hat{y}_0 = 1 - \\hat{y} = 1 - g(W.X+b)$.\nAs a point can have only 2 options either ${0, 1}$. so $\\hat{y}_0 + \\hat{y}_1 = 1$.\nLikelihood Product of predicted probabilities of every point to have its true label.\n$L = \\prod_{i=1}^{m}{P(\\hat{y}^i|X^i)}$\nwhere $\\hat{y}^i$ means $i^{th}$ data prediction and $X^i$ means $i^{th}$ data.\nSo the objective of any classification model in Machine Learning is to maximize this Likelihood $L$ thus its called Maximum Likelihood.\nLet\u0026rsquo;s take an example to calculate the Maximum Likelihood.\nimport numpy as np y_hat = np.array([0.94, 0.0271, 0.43, 0.56, 0.012]) MaxL = 1 for i in range(len(y_hat)): MaxL *= y_hat[i] MaxL  7.360967039999999e-05  The maximum Likelihood of just 5 numbers goes to an order of $10^{-5}$, in real dataset we will have thousands, sometimes millions of data which will give a MaximumLikelihood beyond the range of computation.\nSo we use the property of logarithm $log(a.b) = log(a) + log(b)$ to make this multiplication to addition, so it remains in the range of computation.\n$Log(MaxL) = log(\\prod_{i=1}^{m}{P(\\hat{y}^i|X^i))}$\n$LogLikelihood = \\sum_{i=1}^{m}{log(P(\\hat{y}^i|X^i))}$\nLet\u0026rsquo;s try this Log likelihood with Numpy\nimport numpy as np y_hat = np.array([0.94, 0.0271, 0.43, 0.56, 0.012]) LogL = 1 for i in range(len(y_hat)): LogL += np.log(y_hat[i]) LogL  -8.516734149556177  This number can be used in computation easily and you can observe for any dataset Log likelihood will be in a good range of computation and it will be a negative number as log of any number less than one is negative.\nSo we introduce a negative sign to make it positive. Why? We would like to make this problem to find minimum loss, optimization is relatively easier for convex functions than concave.\n$NegLogLikelihood = -\\sum_{i=1}^{m}{log(P(\\hat{y}^i|X^i))}$\nThis is called as **Negative Log Likelihood Loss **or also as Cross Entropy Loss.\nNow there are 2 cases - $y = 0$, then we want $P(\\hat{y}^i|X^i) = \\hat{y}_0 = 1 - \\hat{y} = 1 - g(W.X+b)$ - $y = 1$, then $P(\\hat{y}^i|X^i) = \\hat{y}_1 = \\hat{y} = g(W.X+b)$\nSo we generalize this 2 cases with\n$ NLL(y, \\hat{y}) = -\\dfrac{1}{m} \\sum_{i=1}^{m}{y^i log(\\hat{y}^i) + (1-y^i) log(1 - \\hat{y}^i)}$\nWe divide the loss with m, to get the average, so the number of example may not affect the loss.\nLet\u0026rsquo;s code this loss with Numpy.\ndef CrossEntropy(y_hat, y): return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean()  y = np.array([0, 1, 1, 0, 0, 1, 1, 1 ]) y_hat_1 = np.array([0.11, 0.76, 0.56, 0.21, 0.04, 0.7, 0.64, 0.95]) y_hat_2 = np.array([0.71, 0.36, 0.16, 0.61, 0.34, 0.5, 0.14, 0.8]) CrossEntropy(y_hat_1, y),CrossEntropy(y_hat_2, y)  (0.26269860327583516, 1.041454329918626)  See how a bad prediction gives more CrossEntropy loss than a better prediction.\nFinding the best Model : Gradient Descent We are going to use the same optimization algorithm which we used for Linear Regression. In almost every deep learning problem, we will use gradient descent or a variation or better version of Gradient Descent. Adam and SGD are better versions of Gradient descent which also uses something called momentum. We will learn more about it later.\n randomly initialize W, b in loop for n steps/epochs{  find $\\hat{y} = g(X.W + b)$ find $ \\mathcal{L}(y, \\hat{y}) = NLL(y, \\hat{y})$ find $ \\frac{\\partial \\mathcal{L}}{\\partial w} $ and $ \\frac{\\partial \\mathcal{L}}{\\partial b} $ Update W and b with $w := w - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w}$ and $b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$ }   Every thing remains same as Linear Regression except now the Loss function $\\mathcal{L}(y, \\hat{y})$ is different, so $ \\frac{\\partial \\mathcal{L}}{\\partial w} $ and $ \\frac{\\partial \\mathcal{L}}{\\partial b} $ may be different. But it can easily be calculated with chain rule.\nIf you didn\u0026rsquo;t understand how we calculated $ \\frac{\\partial \\mathcal{L}}{\\partial w} $ and $ \\frac{\\partial \\mathcal{L}}{\\partial b} $ in Linear Regression , I strongly recommend you to learn multi variable calculus. Its very easy and interesting.\nOne good thing about frameworks like Tensorflow, PyTorch is that they have something called Automatic Gradient. So you don\u0026rsquo;t need to perform these gradient calculation by hand and code it, you give the loss function and parameters, the framework will calculate the gradients of Loss wrt every parameter and update the parameter.\nSo after chain rule,\n$\\dfrac{\\partial \\mathcal{L}}{\\partial w_i} = \\dfrac{1}{m} \\sum_{j=1}^{m}{x_i^j(\\hat{y}^j - y^j)}$\n$\\dfrac{\\partial \\mathcal{L}}{\\partial b} = \\dfrac{1}{m} \\sum_{j=1}^{m}{(\\hat{y}^j - y^j)}$\nWe will update the parameters with\n$w_i := w_i - \\alpha \\dfrac{1}{m} \\sum_{j=1}^{m}{x_i^j(\\hat{y}^j - y^j)}$\n$b := b - \\alpha \\dfrac{1}{m} \\sum_{j=1}^{m}{((\\hat{y}^j - y^j)}$\nwhere $\\alpha$ is called learning rate, if the learning rate is very high the model will learn faster, but may not converge well. if the learning rate is less, the model may take more time but will converge well ie: will get to less loss.\nNow Let\u0026rsquo;s code Logistic Regression model in Numpy and train it. Then use Tensorflow\u0026rsquo;s Keras API to train Logistic Regression Model.\nimport numpy as np def gradient_descent(w, b, X, y, a): w = w - a / X.shape[0] * np.dot(X.T, Log_Reg_model(X, w, b)- y) b = b - a / X.shape[0] * np.sum(Log_Reg_model(X, w, b)- y) return w, b  So far we have seen the math which is used in Logistic Regression model. But when you code, you will have to take care of the dimensions as well. we have seen $\\hat{y} = g(X.W+b)$ here $X.W$ need to be in correct dimension for matrix multiplication.\ndef sigmoid(x): return 1/(1+np.exp(-x)) def Log_Reg_model(x, w, b): y_hat = sigmoid(np.matmul(x, w) + b) return y_hat  # here i am initializing w as (2,1) to match X(1000,2) # you can also initialize w as (1,2) and use np.matmul(X, w.T) + b w, b = np.random.random((2, 1)), np.random.random((1, 1)) from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=1000, n_features=2, centers=2, random_state=9) y = y.reshape(-1,1) print(X.shape, y.shape, w.shape, b.shape) print(sigmoid(np.matmul(X, w) + b).shape) # to check the dimension  (1000, 2) (1000, 1) (2, 1) (1, 1) (1000, 1)  # shape of prediction and label should match Log_Reg_model(X, w, b).shape, y.shape  ((1000, 1), (1000, 1))  # Test the Cross entropy loss def CrossEntropy(y_hat, y): return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean() y_hat = Log_Reg_model(X, w, b) CrossEntropy(y_hat, y)  0.5361011603546345  Let\u0026rsquo;s code a function to visualize\nimport matplotlib.pyplot as plt def visualize_classification(X, y, w, b, e=None, loss=None): class_0 = np.where(y == 0) class_1 = np.where(y == 1) X_0 = X[class_0] X_1 = X[class_1] # change this according to the plot scale x0 = np.arange(-15, 5, 0.01) y0 = ((-b - x0 * w[1])/w[0]).reshape(-1) plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.plot(x0, y0, label='Classifier', color='black') if e is not None: plt.text(-4,3,'Epoch = {}'.format(e), fontsize=20) if loss is not None: plt.text(-4,2,'CE = {:.3f}'.format(loss), fontsize=20) plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show() visualize_classification(X, y.reshape(-1), w, b, e=1, loss=1.1)  losses = [] alpha = 0.001 w, b = np.random.random((2, 1)), np.random.random(1) for i in range(10000): y_hat = Log_Reg_model(X, w, b) ce = CrossEntropy(y_hat, y) losses.append(ce) w, b = gradient_descent(w, b, X, y, alpha) if i%2000 == 0: y_hat = Log_Reg_model(X, w, b) ce = CrossEntropy(y_hat, y) visualize_classification(X, y.reshape(-1), w, b, e=i+1, loss=ce) plt.plot(losses) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show()  Our classifier made a good classification on this toy dataset. Now Let\u0026rsquo;s use Tensorflow to make a Logistic Regression model and train on this dataset. \nLogistic Regression in TensorFlow import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np def CrossEntropy(y_hat, y): return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean() from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=10000, n_features=2, centers=2, random_state=9) model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[2]), keras.layers.Activation('sigmoid')]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X, y, epochs=10, verbose=True)  Epoch 1/10 10000/10000 [==============================] - 0s 42us/sample - loss: 1.8787 - acc: 0.5026 Epoch 2/10 10000/10000 [==============================] - 0s 34us/sample - loss: 0.8342 - acc: 0.5618 . . Epoch 9/10 10000/10000 [==============================] - 0s 35us/sample - loss: 0.0172 - acc: 1.0000 Epoch 10/10 10000/10000 [==============================] - 0s 34us/sample - loss: 0.0135 - acc: 1.0000  We used a new metric called accuray_score.\n$Accuracy = \\dfrac{No\\ of\\ Correct\\ Predictions}{Total\\ no\\ of\\ Predictions}$\nso if out of 100 data, we made 64 correct predictions and 36 incorrect then the accuracy score is $\\dfrac{64}{100} = 0.64$.\nThis dataset was very easy, so it got accuracy score of 1.0 easily.\nLet\u0026rsquo;s Try another dataset.\nimport tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np def CrossEntropy(y_hat, y): return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean() from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=10000, n_features=2, centers=2, random_state=27) class_0 = np.where(y == 0) class_1 = np.where(y == 1) X_0 = X[class_0] X_1 = X[class_1] plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_1[:, 0], X_1[:, 1], marker='x', s=150, color='red', label='0') plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show() model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[2]), keras.layers.Activation('sigmoid')]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X, y, epochs=50, verbose=True)  Epoch 1/50 10000/10000 [==============================] - 0s 44us/sample - loss: 5.3699 - acc: 0.5000 Epoch 2/50 10000/10000 [==============================] - 0s 37us/sample - loss: 3.3782 - acc: 0.5000 . . Epoch 49/50 10000/10000 [==============================] - 0s 38us/sample - loss: 0.0044 - acc: 0.9992 Epoch 50/50 10000/10000 [==============================] - 0s 37us/sample - loss: 0.0043 - acc: 0.9993  You can see from the plot, that is not possible to get an accuracy of 1.0 with a linear classifier like Logistic Regression for this dataset as few of the class points are overlapping. But it still gives a very good result with just a few lines of code.\n","date":1567119600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567119600,"objectID":"d7c13049ddc62663642eff8f4579a0de","permalink":"/courses/deeplearning/1.3/","publishdate":"2019-08-30T00:00:00+01:00","relpermalink":"/courses/deeplearning/1.3/","section":"courses","summary":"Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nLogistic Regression is one of the most commonly used classification models. Unlike Linear Regression which predicts a real unbounded value $\\hat{y} = f(X) = WX+b$, Logistic Regression predicts the probability of a data belonging to a particular class.\n\nFor example for a given data (X,y) where X is a received email and y is 0 if email is spam and 1 if email is not spam.","tags":null,"title":"Logistic Regression","type":"docs"},{"authors":null,"categories":null,"content":" \n Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nSolutions Here: \n Open in GitHub\nIn the previous notebook we used logistic regression for Binary Classification, now we will see how to train a classifier model for Multi-Class Classification.\nWhat is Multi-Class Classification? If the target values have n discrete classification classes ie: y can take discrete value from 0 to n-1. If $y \\in {0, 1, 2, 3, \u0026hellip;, n-1}$, then the classification task is n-Multi-Class.\nTask - 1 Visualizing Data Create a 3-Multi-Class dataset with sklearn.datasets and visualize it.\nIt\u0026rsquo;s very easy, use the same code from previous notebook and make changes for 3 classes.\nfrom sklearn.datasets import make_blobs X, y = make_blobs(n_samples=300, n_features=2, centers=3, random_state=42) X.shape, y.shape, set(y)  ((300, 2), (300,), {0, 1, 2})  If you made 3 centers, you can see set(y) will return {0, 1, 2}. where 0 represents the first class, 1 represents second and 2 represents the third class.\nimport numpy as np # getting the index of each class class_0 = np.where(y == 0) class_1 = np.where(y == 1) class_2 = np.where(y == 2) X_0 = X[class_0] X_1 = X[class_1] X_2 = X[class_2] X_0.shape, X_1.shape, X_2.shape  ((100, 2), (100, 2), (100, 2))  import matplotlib.pyplot as plt plt.figure(figsize=(12,9)) plt.scatter(X_0[:, 0], X_0[:, 1], marker='x', s=150, color='red', label='0') plt.scatter(X_1[:, 0], X_1[:, 1], marker='o', s=150, color='blue', label='1') plt.scatter(X_2[:, 0], X_2[:, 1], marker='s', s=150, color='red', label='2') plt.xlabel('$x1$', fontsize=20) plt.ylabel('$x2$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Softmax Regression How to get the probabilities? : SoftMax Now the target is going to be $y \\in {0, 1, 2}$, so sigmoid cannot be used here, as sigmoid will convert any number to range 0 to 1 , so it can only be used for binary classification.\nWe need a function which converts the scores/logits of linear mapping into probabilities for all n classes.\nThat function should have some properties:\n all probabilities should be \u0026gt;0 probabilities should be in range $[0,1]$ some of all class probabilities = 1  \nOne possible function can be class_logit/sum of all class logits. Lets try it\nExample:\n$logits = [-100, 40, -10]$\nDon\u0026rsquo;t bother how do we get 3 logits, we will discuss it below.\n$probabilities = [\\dfrac{-100}{(-100+40-10)}, \\dfrac{40}{(-100+40-10)}, \\dfrac{-10}{(-100+40-10)}]$\n$probabilities = [\\dfrac{100}{70}, \\dfrac{-40}{70}, \\dfrac{10}{70}]$\nyou can see this example satisfies only the third property(sum=1). So we need a function which gives positive numbers. Exponential function can help us.\n$Logits = [l_0, l_1, l_2, \u0026hellip;, l_{n-1}]$\n$Probabilities = [\\dfrac{e^{l_0}}{e^{l_1} + e^{l_2}+ \u0026hellip; + e^{l_{n-1}}}, \\dfrac{e^{l_1}}{e^{l_1} + e^{l_2}+ \u0026hellip; + e^{l_{n-1}}}, \u0026hellip;, \\dfrac{e^{l_{n-1}}}{e^{l_1} + e^{l_2}+ \u0026hellip; + e^{l_{n-1}}}]$\nThis function is called Softmax, and this gives the probability that a data belongs to class j, given the logits.\n$P(y=j|z) = \\dfrac{e^{z_j}}{\\sum_{i=0}^{n-1}e^{z_i}}$\nLet\u0026rsquo;s code softmax function in Numpy.\nimport numpy as np def softmax(x): exp = np.exp(x) exp_sum = exp.sum(axis=1).reshape(-1,1) return exp/exp_sum  x = np.array([[22, 40, 10]]) softmax(x)  array([[1.52299795e-08, 9.99999985e-01, 9.35762283e-14]])  Now we know, replacing sigmoid with softmax will help in the case of multi class classification. This softmax model is also called Softmax Regression.\nLoss Function As we have already seen, for classification task we will use Cross Entropy loss. The prediction of softmax regression $\\hat{y} = [0.129, 0.8, 0.071]$, whereas the true label will be one of $y \\in {0, 1, 2}$. We cannot directly use Cross Entropy loss with $\\hat{y}$ and $y$.\nSo we convert the true label into One-Hot Encoding form. One hot encoding is a vector representation of the label which has \u0026lsquo;1\u0026rsquo; at the index corresponding to the label and \u0026lsquo;0\u0026rsquo; elsewhere.\nExample:\nLet $y \\in {0, 1, 2, 3, 4}$, then\n \u0026lsquo;4\u0026rsquo; is represented as $[0, 0, 0, 0, 1]$ \u0026lsquo;3\u0026rsquo; is represented as $[0, 0, 0, 1, 0]$ \u0026lsquo;2\u0026rsquo; is represented as $[0, 0, 1, 0, 0]$ \u0026lsquo;1\u0026rsquo; is represented as $[0, 1, 0, 0, 0]$ \u0026lsquo;0\u0026rsquo; is represented as $[1, 0, 0, 0, 0]$  Let\u0026rsquo;s code the label to one hot conversion\nimport numpy as np def to_one_hot(labels, num_classes): return np.eye(num_classes)[labels] num_classes = 5 labels = np.array([0, 1, 2, 3, 4]) to_one_hot(labels, num_classes)  array([[1., 0., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]])  You can also use sklearn.preprocessing.OneHotEncoder to convert labels to one hot vectors.\nfrom sklearn.preprocessing import OneHotEncoder labels = np.array([[0], [1], [2], [3], [4]]) OneHotEncoder(categories='auto').fit_transform(labels).toarray()  array([[1., 0., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]])  Keras also have some utils functions which can help in one-hot encoding\nfrom keras.utils.np_utils import to_categorical labels = np.array([0, 1, 2, 3, 4]) to_categorical(labels, num_classes=5)  Using TensorFlow backend. array([[1., 0., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]], dtype=float32)  Softmax Regression Model From what we discussed so far, if the number of classes = 3, then we expect model to give a prediction $\\hat{y} = Softmax(z)$ and $z$ will be like $z = [-10, 20, 5]$(Example).\n$z = X.W + b$ will only give one number like $z=[4]$ in logistic regression. But now we are using softmax regression which expect a model which gives 3 output for a 3 class classifier.\nIf the no of input features = 2 and no of out[ut classes = 3 So we will use 3 linear classifier.\n$z_1 = X.W_1 + b_1$, $z_2 = X.W_2 + b_2$, $z_3 = X.W_3 + b_3$ which can be combined together with\nz = $\\begin{bmatrix}z_1\u0026amp;z_2\u0026amp;z_3\\ \\end{bmatrix}$\n$ z = X . W + b $\n$W = \\begin{bmatrix}W_1\u0026amp;W_2\u0026amp;W_3\\ \\end{bmatrix} $\n$b = \\begin{bmatrix}b_1\u0026amp;b_2\u0026amp;b_3\\ \\end{bmatrix}$\neach $W_i = \\begin{bmatrix}W_{i1}\\ W_{i2}\\ W_{i3}\\ \\end{bmatrix} $\nso the final $W = \\begin{bmatrix}W_{11}\u0026amp;W_{12}\u0026amp;W_{13} \\\\ W_{21}\u0026amp;W_{22}\u0026amp;W_{23} \\ \\end{bmatrix}$\n\n\nLet $X = \\begin{bmatrix}x_1\u0026amp;x_2\\ \\end{bmatrix}$\n$z =\\begin{bmatrix}z_1\u0026amp;z_2\u0026amp;z_3\\ \\end{bmatrix} = \\begin{bmatrix}x_1\u0026amp;x_2 \\ \\end{bmatrix} . \\begin{bmatrix}W_{11}\u0026amp;W_{12}\u0026amp;W_{13} \\\\ W_{21}\u0026amp;W_{22}\u0026amp;W_{23} \\\\ \\end{bmatrix} + \\begin{bmatrix}b_1\u0026amp;b_2\u0026amp;b_3\\ \\end{bmatrix}$\n\n For 1 data, $(1,2).(2,3) + (1,3) = (1,3)$ For n data, $(n,2).(2,3) + (1,3) = (n,3)$, b will be added to all n data, this is called broadcasting.  \nFrameworks like Tensorflow, PyTorch will take care of this matrix form of $W$ and $b$ for you.\nTask-2 Train a Softmax Regression with Tensorflow\n# Data from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=1000, n_features=2, centers=3, random_state=42) X.shape, y.shape, set(y)  ((1000, 2), (1000,), {0, 1, 2})  We need to convert the labels into one hot vectors to train the model. Let\u0026rsquo;s use keras to_categorical function\nprint(y.shape) y = to_categorical(y, num_classes=3) print(y.shape)  (1000,) (1000, 3)  Train-Validation-Test Split So far we had a dataset and we used it for training and checked the accuracy/loss to see the performance. But it\u0026rsquo;s not the right way to check the performance of a model. Usually any dataset is split into 3 parts namely Train-Validation-Test.\ntrain set This dataset is used to train the model. If the training is good, metrics of this dataset will be always good. Almost 60-70% of dataset is given to training set.\nvalidation set After every epoch(generally) of training, metrics of this dataset is checked to ensure the model is also performing well on unseen data as much as it performs on the training dataset. If the model performs well on training dataset but not good on validation set, it means the model has a problem called \u0026lsquo;Overfitting\u0026rsquo; which we will look in more detail later. Some hyper parameters are adjusted to make the model perform well in validation set as well during training. 10-20% of the data is given to validation set.\ntesting set After the training is over for n epochs, when the model performs well in both training and validation sets, a final check is done to see the performance of the model on new unseen dataset. 20-30% of the data is given to Test set.\nThe percentage numbers depends on the total number of data we have access to, which you will understand as you work on more projects.\nWe can split the dataset into train-test using sklearn.model_selection.train_test_split\nfrom sklearn.model_selection import train_test_split print(X.shape, y.shape) # test_size is the percent of split 0.2 means 20% of data is for testset. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)  (1000, 2) (1000, 3) (800, 2) (200, 2) (800, 3) (200, 3)  Tensorflow Model  Make a Dataset with 2 input features, 3 output classes one hot encode y Split Dataset into Train-Validation-Test Train Model with Validation Dataset, check the docs on how to use validation data. predict X_test with the trained model, refer the docs(model.predict function) convert the prediction of X_test and y_test from one-hot to labels using np.argmax(pred, axis=1) use sklearn.metrics.accuracy_score on prediction of X_test and y_test to find the accuracy on Test set.\nimport tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import make_blobs # Make the Dataset num_classes = 3 num_input_features = 2 X, y = make_blobs(n_samples=2000, n_features=num_input_features, centers=num_classes, random_state=42) # to categorical y = to_categorical(y, num_classes=num_classes) # train-test split # 20% of dataset to testset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True) # train-validation split # 20% of trainset to valset X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True) # Model model = tf.keras.Sequential([keras.layers.Dense(units=num_classes, input_shape=[num_input_features]), keras.layers.Activation('softmax')]) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, epochs=30, verbose=True, validation_data=(X_val, y_val)) # Prediction for Test set with trained Model from sklearn.metrics import accuracy_score y_test_pred = model.predict(X_test) test_accuracy = accuracy_score(np.argmax(y_test_pred, axis=1), np.argmax(y_test, axis=1)) print('\\nTest Accuracy = ', test_accuracy)  Train on 1280 samples, validate on 320 samples Epoch 1\u0026frasl;30 1280\u0026frasl;1280 [==============================] - 0s 300us/sample - loss: 5.9285 - acc: 0.3305 - val_loss: 5.8832 - val_acc: 0.3281 Epoch 2\u0026frasl;30 1280\u0026frasl;1280 [==============================] - 0s 40us/sample - loss: 5.3884 - acc: 0.3305 - val_loss: 5.3271 - val_acc: 0.3281 . . Epoch 29\u0026frasl;30 1280\u0026frasl;1280 [==============================] - 0s 41us/sample - loss: 0.0755 - acc: 0.9969 - val_loss: 0.0691 - val_acc: 0.9969 Epoch 30\u0026frasl;30 1280\u0026frasl;1280 [==============================] - 0s 41us/sample - loss: 0.0699 - acc: 0.9969 - val_loss: 0.0637 - val_acc: 0.9969\nTest Accuracy = 0.995\n  Train the model with different - no of input features - no of output classes - no of data - split of train-validation-test - epochs\n","date":1567119600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567119600,"objectID":"1abb08ef4e81c40d1bccd384b95c7c07","permalink":"/courses/deeplearning/1.4/","publishdate":"2019-08-30T00:00:00+01:00","relpermalink":"/courses/deeplearning/1.4/","section":"courses","summary":"Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nSolutions Here: \n Open in GitHub\nIn the previous notebook we used logistic regression for Binary Classification, now we will see how to train a classifier model for Multi-Class Classification.\nWhat is Multi-Class Classification? If the target values have n discrete classification classes ie: y can take discrete value from 0 to n-1. If $y \\in {0, 1, 2, 3, \u0026hellip;, n-1}$, then the classification task is n-Multi-Class.","tags":null,"title":"Multi Class Classification","type":"docs"},{"authors":null,"categories":null,"content":" \n Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nNon-Linearity Note: We will look mostly at classification examples, but the same concepts apply to regression problems as well with a little change in using activation function(Sigmoid, Softmax which we learned in previous sections).\nSo far the datasets we have used are linearly separable, which means they can be separated by line(2-d), plane(3-d) and linear multi dimensional classifiers.\nBut in the real world, not all datasets are 2-d(visualizable) and linearly separable.\nImage Source\nLogistic Regression(Binary Classification) and Softmax Regression(Multi Class Classification) are linear models, they can only predict lines/planes/linear n-dim models to classify the data. They are note good at classifying a non-linear data.\nLet\u0026rsquo;s visualize it on linear and non-linear data.\nVisualize Linear Dataset and Linear Model  Make a Linear Dataset Train a Logistic Regression Predict the probabilities using the model for whole plot grid, and colour it based on if p\u0026gt;0.5 or not.  It\u0026rsquo;s fine if you don\u0026rsquo;t understand how we are going to visualize it, it\u0026rsquo;s just a matplotlib plot.\nfrom sklearn.datasets import make_blobs import matplotlib.pyplot as plt X, y = make_blobs(n_samples=1000, n_features=2, centers=2, random_state=3) # remember the code we wrote to get the index of 0, 1 classes and scatter it separately # that was to understand how to plot each class # that plotting can also be done easily in a single line :D :p plt.figure(figsize=(10,10)) plt.scatter(X[:,0], X[:,1],c=y) plt.grid(True) plt.show()  Train a Logistic Regression Model with TensorFlow for Linearly Separable Dataset import tensorflow as tf from tensorflow import keras from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split import numpy as np # get the dataset X, y = make_blobs(n_samples=2000, n_features=2, centers=2, random_state=3) # make train-validation split, let's ignore test set for now. X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=3) model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[2]), keras.layers.Activation('sigmoid')]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, epochs=20, verbose=True, validation_data=(X_val, y_val))  Train on 2000 samples, validate on 600 samples Epoch 1/20 2000/2000 [==============================] - 0s 204us/sample - loss: 0.4580 - acc: 0.7000 - val_loss: 0.3898 - val_acc: 0.7650 Epoch 2/20 2000/2000 [==============================] - 0s 49us/sample - loss: 0.3796 - acc: 0.7930 - val_loss: 0.3236 - val_acc: 0.8450 . . Epoch 19/20 2000/2000 [==============================] - 0s 43us/sample - loss: 0.0602 - acc: 0.9970 - val_loss: 0.0521 - val_acc: 1.0000 Epoch 20/20 2000/2000 [==============================] - 0s 41us/sample - loss: 0.0566 - acc: 0.9970 - val_loss: 0.0490 - val_acc: 1.0000  import numpy as np xx, yy = np.mgrid[-10:10:.1, -10:10:.1] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show()  This plot shows the probability of each point in the grid belongs to class \u0026lsquo;1\u0026rsquo;. We can see the white line is able to separate almost all the points and accuracy is also good.\nVisualize Non-Linear Dataset and Linear Model  Make a Non-Linear Dataset with sklearn.datasets.make_gaussian_quantiles. Train a Logistic Regression Predict the probabilities using the model for whole plot grid, and visualize the model classifier\nfrom sklearn.datasets import make_moons import matplotlib.pyplot as plt X, y = make_moons(n_samples=1000, random_state=3, noise=0.1) idx = np.where(X[:,0] \u0026lt; 1.1) X = X[idx] y = y[idx] plt.figure(figsize=(10,10)) plt.scatter(X[:,0], X[:,1],c=y) plt.grid(True) plt.show()   Train a Logistic Regression Model with TensorFlow for Non-Linear Dataset import tensorflow as tf from tensorflow import keras from sklearn.datasets import make_gaussian_quantiles from sklearn.model_selection import train_test_split import numpy as np # get the dataset # X, y = make_gaussian_quantiles(n_samples=1000, n_features=2, n_classes=2, random_state=3, cov=0.1) X, y = make_moons(n_samples=1000, random_state=3, noise=0.1) idx = np.where(X[:,0] \u0026lt; 1.1) X = X[idx] y = y[idx] # make train-validation split, let's ignore test set for now. X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=3) model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[2]), keras.layers.Activation('sigmoid')]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, epochs=20, verbose=True, validation_data=(X_val, y_val))  Train on 764 samples, validate on 230 samples Epoch 1/20 764/764 [==============================] - 0s 469us/sample - loss: 0.4796 - acc: 0.8338 - val_loss: 0.4810 - val_acc: 0.8130 Epoch 2/20 764/764 [==============================] - 0s 46us/sample - loss: 0.4747 - acc: 0.8351 - val_loss: 0.4766 - val_acc: 0.8087 . . Epoch 19/20 764/764 [==============================] - 0s 44us/sample - loss: 0.4223 - acc: 0.8364 - val_loss: 0.4281 - val_acc: 0.8174 Epoch 20/20 764/764 [==============================] - 0s 44us/sample - loss: 0.4202 - acc: 0.8364 - val_loss: 0.4261 - val_acc: 0.8174  import numpy as np xx, yy = np.mgrid[-2:2:.01, -2:2:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show()  You can see the logistic regression model, cannot separate a non-linear data. The model(white area) separates the grid into 2 regions linearly which is not a good classifier for this dataset.\nLet\u0026rsquo;s make another dataset\nAnother Example (More difficult) from sklearn.datasets import make_gaussian_quantiles X, y = make_gaussian_quantiles(n_samples=1000, n_features=2, n_classes=2, random_state=3, cov=0.1) plt.figure(figsize=(10,10)) plt.scatter(X[:,0], X[:,1],c=y) plt.grid(True) plt.show() # make train-validation split, let's ignore test set for now. X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=3) model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[2]), keras.layers.Activation('sigmoid')]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, epochs=20, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-2:2:.01, -2:2:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show()  Train on 1000 samples, validate on 300 samples Epoch 1/20 1000/1000 [==============================] - 0s 375us/sample - loss: 0.7122 - acc: 0.5000 - val_loss: 0.6955 - val_acc: 0.5300 Epoch 2/20 1000/1000 [==============================] - 0s 44us/sample - loss: 0.7114 - acc: 0.4980 - val_loss: 0.6952 - val_acc: 0.5300 . . Epoch 19/20 1000/1000 [==============================] - 0s 48us/sample - loss: 0.7018 - acc: 0.4940 - val_loss: 0.6912 - val_acc: 0.5433 Epoch 20/20 1000/1000 [==============================] - 0s 48us/sample - loss: 0.7013 - acc: 0.4930 - val_loss: 0.6910 - val_acc: 0.5467  Logistic Regression Classifier for this dataset is worse. It uses a straight line to separate concentric circles. We cannot depend much on Linear classifiers on real world dataset(non-linear).\nNon-linear Dataset with Linear Classifier Hope you remember Polynomial Regression, when the dataset is non-linear, we gave the model the non linear terms, so the model can use it in linear fashion to make a non-linear regressor.\nWe are going to try the same for classifier.\nPolynomial Terms As the data is circular, 2nd degree polynomial terms will do good. Also try higher degree to see if accuracy increases.\nfrom sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=2) X, y = make_gaussian_quantiles(n_samples=2000, n_features=2, n_classes=2, random_state=3, cov=0.1) X_2 = poly.fit_transform(X) print(X_2.shape) print(X_2[0])  (2000, 6) [1. 0.16654979 0.21185179 0.02773883 0.03528387 0.04488118]  Train Logistic Regression Model with 2nd degree polynomial terms X, y = make_gaussian_quantiles(n_samples=2000, n_features=2, n_classes=2, random_state=3, cov=0.1) X_2 = poly.fit_transform(X) # make train-validation split, let's ignore test set for now. X_train, X_val, y_train, y_val = train_test_split(X_2, y, test_size=0.3, random_state=3) model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[6]), keras.layers.Activation('sigmoid')]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, epochs=200, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-2:2:.01, -2:2:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(poly.fit_transform(grid))[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show()  Train on 1400 samples, validate on 600 samples Epoch 1/200 1400/1400 [==============================] - 1s 387us/sample - loss: 0.7530 - acc: 0.4914 - val_loss: 0.7292 - val_acc: 0.5200 Epoch 2/200 1400/1400 [==============================] - 0s 48us/sample - loss: 0.7375 - acc: 0.4914 - val_loss: 0.7169 - val_acc: 0.5200 . . Epoch 199/200 1400/1400 [==============================] - 0s 50us/sample - loss: 0.3552 - acc: 0.9164 - val_loss: 0.3886 - val_acc: 0.8933 Epoch 200/200 1400/1400 [==============================] - 0s 50us/sample - loss: 0.3543 - acc: 0.9136 - val_loss: 0.3876 - val_acc: 0.8967  The model with 2nd degree data performed well, it got an accuracy of 0.91 in 200 epochs and seems to increase if you train longer.\nThe problem is, as we were able to visualize this dataset, we concluded to include 2nd degree terms, but real world datasets are multi-dimensional, you cannot visualize those.\nMulti-Layer Perceptron or usually called as Artificial neural Networks or simply Neural Networks can help us with this problem.\nNote: Linear Regression, Logistic Regression are also Neurons(not Network). Softmax Regression is a network of several linear classifiers so it is also a network. But we usually denote something as neural network when it has a hidden layer, which we will discuss later.\nCombination of Linear Classifiers Non-linear data and Linear Classifier classifier 1 from sklearn.datasets import make_moons import matplotlib.pyplot as plt X, y = make_moons(n_samples=1000, random_state=3, noise=0.1) idx = np.where(X[:,0] \u0026lt; 1.1) X = X[idx] y = y[idx] xline = np.arange(-1.3, 1.2, 0.01) yline = xline + 0.5 # random classifier plt.figure(figsize=(10,10)) plt.scatter(X[:,0], X[:,1],c=y) plt.plot(xline, yline) plt.grid(True) plt.show()  This line can be one of the classifiers, but does not give a better accuracy, as yellow side also have few blue data points.\nclassifier 2 from sklearn.datasets import make_moons import matplotlib.pyplot as plt X, y = make_moons(n_samples=1000, random_state=3, noise=0.1) idx = np.where(X[:,0] \u0026lt; 1.1) X = X[idx] y = y[idx] xline = np.arange(-1.3, 1.2, 0.01) yline = - xline + 0.6 # random classifier plt.figure(figsize=(10,10)) plt.scatter(X[:,0], X[:,1],c=y) plt.plot(xline, yline) plt.grid(True) plt.show()  This can also be a classifier but again it has few data mis classified.\nLet\u0026rsquo;s say there are 2 logistic regression classifiers - classifier 1 - clf1 - classifier 2 - clf2\nboth are fine, but they also misclassified some data points.\nSo both the classifiers individually cannot classify the data very well because the data is non-linear and the classifiers are linear.\nBut if we combine these 2 classifiers that can make a non-linear classifier.\nfrom sklearn.datasets import make_moons import matplotlib.pyplot as plt X, y = make_moons(n_samples=1000, random_state=3, noise=0.1) idx = np.where(X[:,0] \u0026lt; 1.1) X = X[idx] y = y[idx] xline1 = np.arange(-1.3, 0.01, 0.01) yline1 = xline1 + 0.6 xline2 = np.arange(0, 1.2, 0.01) yline2 = - xline2 + 0.6 # random classifier plt.figure(figsize=(10,10)) plt.scatter(X[:,0], X[:,1],c=y) plt.plot(xline1, yline1, color='b') plt.plot(xline2, yline2, color='b') plt.grid(True) plt.show()  This will be one of the best classifiers, by moving the 2 lines a little more, we can get an accuracy of nearly 1.0 for this dataset. Also we can make this classifier better by combining more number of linear classifiers.\nSo combination of linear classifiers can be a good non-linear classifier.\nThis is the primary motivation behind Artificial neural networks.\nHow do we combine the Linear Classifiers? We saw that combining multiple linear classifiers can give a non-linear classifier. But how do we do that?\nLet\u0026rsquo;s assume we have 2 classifiers: $C_1, C_2$\n$C_1 : \\hat{y}_1 = \\sigma(X.W_1+b_1)$\n$C_2 : \\hat{y}_2 = \\sigma(X.W_2+b_2)$\nSo both $C_1, C_2$ gives the probability that a point belongs to class \u0026lsquo;1\u0026rsquo;. We need to combine $C_1, C_2$, so we can take the outputs of $C_1, C_2$ ie: $\\hat{y}_1$ and $\\hat{y}_2$ and send it to a new logistic regression model $C_3$.\nBut we no more need probabilities from $C_1, C_2$, so we may/maynot use sigmoid function. But we need to use some non-linearity as combination of linear function directly will be a linear function, which is of no use to us. So we can also use some other famous activations like tanh, ReLU, etc. $C_1 : \\hat{y}_1 = g(X.W_1+b_1)$\n$C_2 : \\hat{y}_2 = g(X.W_2+b_2)$\ng can be any activation, sigmoid, tanh, relu,..etc.\n$X_{new} = [\\hat{y}_1, \\hat{y}_2]$\n$C_3 : \\hat{y} = \\sigma(X_{new}.W_3 + b_3)$\nThis combination of Neurons is called Multi Layer Perceptron or ANN or Neural networks in general.\n\nLet\u0026rsquo;s implement this in Tensorflow.\nBefore that try this Tensorflow Playground\nTensorflow Playground  Linear Classifier Combination of 2 Linear Classifiers  MLP in Tensorflow X, y = make_moons(n_samples=2000, random_state=3, noise=0.1) idx = np.where(X[:,0] \u0026lt; 1.1) X = X[idx] y = y[idx] # make train-validation split, let's ignore test set for now. X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=3) model = tf.keras.Sequential([ keras.layers.Dense(units=2, input_shape=[2]), keras.layers.Activation('tanh'), keras.layers.Dense(units=1), keras.layers.Activation('sigmoid')]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, epochs=500, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-1.5:1.5:.1, -1:1.5:.1] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show()  Train on 1066 samples, validate on 457 samples Epoch 1/500 1066/1066 [==============================] - 1s 776us/sample - loss: 0.5274 - acc: 0.7739 - val_loss: 0.5217 - val_acc: 0.7856 Epoch 2/500 1066/1066 [==============================] - 0s 61us/sample - loss: 0.5105 - acc: 0.7927 - val_loss: 0.5075 - val_acc: 0.7812 . . Epoch 499/500 1066/1066 [==============================] - 0s 63us/sample - loss: 0.0167 - acc: 0.9991 - val_loss: 0.0138 - val_acc: 1.0000 Epoch 500/500 1066/1066 [==============================] - 0s 62us/sample - loss: 0.0165 - acc: 0.9991 - val_loss: 0.0137 - val_acc: 1.0000  Yay! We just trained our first Neural Network(at least for this course).\nYou can see the neural network model is as expected, it combined 2 linear models to give a better non-linear model.\nNeural Networks are the state of the art(SOTA) for almost all the modern learning problems, there are many types of neural networks which we will learn later in the course.\nTrain the model with: - different units in the first layer - another linear layer and activation function - different activation function - different number of epochs - different dataset(especially for the concentric circles dataset)\n","date":1567292400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567292400,"objectID":"997d961b2be4f80b530caadf89611813","permalink":"/courses/deeplearning/1.5/","publishdate":"2019-09-01T00:00:00+01:00","relpermalink":"/courses/deeplearning/1.5/","section":"courses","summary":"Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nNon-Linearity Note: We will look mostly at classification examples, but the same concepts apply to regression problems as well with a little change in using activation function(Sigmoid, Softmax which we learned in previous sections).\nSo far the datasets we have used are linearly separable, which means they can be separated by line(2-d), plane(3-d) and linear multi dimensional classifiers.","tags":null,"title":"Motivation for Multi Layer Perceptron","type":"docs"},{"authors":null,"categories":null,"content":" \n Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nWe trained our first neural network in the previous notebook which had 3 layers\n Input Layer Hidden Layer Output Layer  Multiple Nodes The network had 2 linear layers($C_1$,0 $C_2$) in the hidden layer each of which gave us a linear classifier, then we used another linear layer($C_3$) in the output layer to combine $C_1$ and $C_2$ to give us a non-linear classifier($C$). That was amazing, now we can get non-linear classifiers.\nWon\u0026rsquo;t it be even amazing it we can combine more than 2 linear classifiers?. Ofc yes, that will give a more accurate classifier. But how to combine 3 linear classifiers?\nJust increase the number of nodes in the hidden layer. This network will consider 3 linear layers and combine them to give a non-linear classifier/regressor.\nMultiple layers (Deeper Neural Network) Combination of multiple linear classifiers gives us a non-linear classifier, What if we combine multiple non-linear classifiers? Won\u0026rsquo;t that give us a more complex non-linear classifier which can fit/classify our data more accurately?\nHow to do that? We will take multiple copies of linear classifiers and combine them differently using another hidden layer instead of output layer and combine these non-linear classifiers using output layers.\n Input(Input Layer) n linear classifiers(Hidden Layer 1) m non linear classifier = m combinations of (n linear classifiers) output MORE COMPLEX classifier = combination of (m non-linear classifier)  This network, will take 3 linear classifiers and make 4 different combination of linear classifiers to get 4 non-linear classifiers and combine them to get a more complex Non-Linear Classifier.\nBy stacking more Hidden Layers and nodes, we can get a more and more complex non-linear classifier/regressor. We visualized only for 2-D Data, this also applies for multi dimensional data, but unfortunately we cannot visualize multidimensioanl data other than 2\u0026frasl;3. In 3-d data the linear classifiers will be planes and the non-linear classifier will be some surface which are combinations of planes.\nThis is Deep Neural network.\nNote: This is a mathematical analysis of how a neural network can make a more complex classifier/regressor, but we really do not know what features a neural network will consider to make the complex function. That is why it\u0026rsquo;s called Machine Learning, we can try to understand what features the model is using to classify, but can\u0026rsquo;t be sure.\nNow Let\u0026rsquo;s compare neural network with a deeper neural network and try to visualize the classifier.\nDeep Neural Networks in Tensorflow Dataset from sklearn.datasets import make_gaussian_quantiles from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt X, y = make_gaussian_quantiles(n_samples=2000, n_features=2, n_classes=2, random_state=3, cov=0.1) plt.figure(figsize=(10,10)) plt.scatter(X[:,0], X[:,1],c=y) plt.grid(True) plt.show() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True) X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True) print('Train = {}\\nTest = {}\\nVal = {}'.format(len(X_train), len(X_test), len(X_val)))  Train = 1120 Test = 600 Val = 280  Linear Classifier (0 hidden layer) import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() # random number initialized to be the same, for reproducing the same results. np.random.seed(0) tf.set_random_seed(0) model = tf.keras.Sequential([ keras.layers.Dense(units=1, input_shape=[2]), keras.layers.Activation('sigmoid') ]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, epochs=20, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-2:2:.01, -2:2:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show() # test accuracy from sklearn.metrics import accuracy_score y_test_pred = model.predict(X_test) test_accuracy = accuracy_score((y_test_pred \u0026gt; 0.5), y_test) print('\\nTest Accuracy = ', test_accuracy)  Train on 1120 samples, validate on 280 samples Epoch 1/20 1120/1120 [==============================] - 0s 107us/sample - loss: 0.7116 - acc: 0.4839 - val_loss: 0.7083 - val_acc: 0.5000 Epoch 2/20 1120/1120 [==============================] - 0s 40us/sample - loss: 0.7108 - acc: 0.4857 - val_loss: 0.7074 - val_acc: 0.4893 . . Epoch 19/20 1120/1120 [==============================] - 0s 38us/sample - loss: 0.7001 - acc: 0.5071 - val_loss: 0.6994 - val_acc: 0.5107 Epoch 20/20 1120/1120 [==============================] - 0s 37us/sample - loss: 0.6997 - acc: 0.5071 - val_loss: 0.6991 - val_acc: 0.5071  Test Accuracy = 0.5183333333333333  You can train for more epochs, but the model won\u0026rsquo;t really improve the metrics.\nNeural network (1 hidden layer, 3 hidden units) import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() # random number initialized to be the same, for reproducing the same results. np.random.seed(0) tf.set_random_seed(0) model = tf.keras.Sequential([ keras.layers.Dense(units=3, input_shape=[2]), keras.layers.Activation('tanh'), keras.layers.Dense(units=1), keras.layers.Activation('sigmoid') ]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, epochs=500, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show() # test accuracy from sklearn.metrics import accuracy_score y_test_pred = model.predict(X_test) test_accuracy = accuracy_score((y_test_pred \u0026gt; 0.5), y_test) print('\\nTest Accuracy = ', test_accuracy)  Train on 1120 samples, validate on 280 samples Epoch 1/500 1120/1120 [==============================] - 0s 120us/sample - loss: 0.7086 - acc: 0.4982 - val_loss: 0.6955 - val_acc: 0.5429 Epoch 2/500 1120/1120 [==============================] - 0s 43us/sample - loss: 0.7065 - acc: 0.5080 - val_loss: 0.6948 - val_acc: 0.5679 . . Epoch 499/500 1120/1120 [==============================] - 0s 45us/sample - loss: 0.4358 - acc: 0.7973 - val_loss: 0.4756 - val_acc: 0.7643 Epoch 500/500 1120/1120 [==============================] - 0s 41us/sample - loss: 0.4355 - acc: 0.8000 - val_loss: 0.4755 - val_acc: 0.7643  Test Accuracy = 0.7683333333333333  This model learnt some complex classifers, but you can see by adding a hidden layer with 3 nodes, the accuracy increased a lot.\nNow Let\u0026rsquo;s increase the no of nodes in the hidden layer\nNeural network (1 hidden layer, 10 hidden units) import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() # random number initialized to be the same, for reproducing the same results. np.random.seed(0) tf.set_random_seed(0) model = tf.keras.Sequential([ keras.layers.Dense(units=10, input_shape=[2]), keras.layers.Activation('tanh'), keras.layers.Dense(units=1), keras.layers.Activation('sigmoid') ]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, epochs=500, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show() # test accuracy from sklearn.metrics import accuracy_score y_test_pred = model.predict(X_test) test_accuracy = accuracy_score((y_test_pred \u0026gt; 0.5), y_test) print('\\nTest Accuracy = ', test_accuracy)  Train on 1120 samples, validate on 280 samples Epoch 1/500 1120/1120 [==============================] - 0s 115us/sample - loss: 0.6930 - acc: 0.5321 - val_loss: 0.6990 - val_acc: 0.5393 Epoch 2/500 1120/1120 [==============================] - 0s 41us/sample - loss: 0.6924 - acc: 0.5670 - val_loss: 0.6982 - val_acc: 0.5286 . . Epoch 499/500 1120/1120 [==============================] - 0s 47us/sample - loss: 0.1024 - acc: 0.9732 - val_loss: 0.1357 - val_acc: 0.9643 Epoch 500/500 1120/1120 [==============================] - 0s 47us/sample - loss: 0.1023 - acc: 0.9723 - val_loss: 0.1358 - val_acc: 0.9643  Test Accuracy = 0.97  Neural network (2 hidden layers, 10 hidden units, 10 hidden units) import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() # random number initialized to be the same, for reproducing the same results. np.random.seed(0) tf.set_random_seed(0) model = tf.keras.Sequential([ keras.layers.Dense(units=10, input_shape=[2]), keras.layers.Activation('tanh'), keras.layers.Dense(units=10), keras.layers.Activation('tanh'), keras.layers.Dense(units=1), keras.layers.Activation('sigmoid') ]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, epochs=500, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show() # test accuracy from sklearn.metrics import accuracy_score y_test_pred = model.predict(X_test) test_accuracy = accuracy_score((y_test_pred \u0026gt; 0.5), y_test) print('\\nTest Accuracy = ', test_accuracy)  Train on 1120 samples, validate on 280 samples Epoch 1/500 1120/1120 [==============================] - 0s 139us/sample - loss: 0.7011 - acc: 0.5357 - val_loss: 0.6926 - val_acc: 0.6036 Epoch 2/500 1120/1120 [==============================] - 0s 50us/sample - loss: 0.6947 - acc: 0.6000 - val_loss: 0.6925 - val_acc: 0.6321 . . Epoch 499/500 1120/1120 [==============================] - 0s 52us/sample - loss: 0.0250 - acc: 0.9973 - val_loss: 0.0382 - val_acc: 0.9929 Epoch 500/500 1120/1120 [==============================] - 0s 53us/sample - loss: 0.0245 - acc: 0.9964 - val_loss: 0.0386 - val_acc: 0.9893  Test Accuracy = 0.9916666666666667  Performance of Different NN Architectures    Model Architecture Test Accuracy     0 hidden layer 0.518   1 hidden layer, 3 hidden units 0.768   1 hidden layer, 10 hidden units 0.970   2 hidden layers, 10-10 hidden units 0.991    you have seen how deeper model can help. As the model goes more deeper and complex, the performance of the model increases(although this may not be the case every time, especially when we have less data and there is something called vanishing gradients, which we will discuss later). But in general, deeper models improves performance.\nTrain the models in the notebooks with \u0026lsquo;relu\u0026rsquo; activation function instead of \u0026lsquo;tanh\u0026rsquo; and compare the performance.\n","date":1567378800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567378800,"objectID":"eee105361210329a6b2760f851aaf0ba","permalink":"/courses/deeplearning/2.1/","publishdate":"2019-09-02T00:00:00+01:00","relpermalink":"/courses/deeplearning/2.1/","section":"courses","summary":"Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nWe trained our first neural network in the previous notebook which had 3 layers\n Input Layer Hidden Layer Output Layer  Multiple Nodes The network had 2 linear layers($C_1$,0 $C_2$) in the hidden layer each of which gave us a linear classifier, then we used another linear layer($C_3$) in the output layer to combine $C_1$ and $C_2$ to give us a non-linear classifier($C$).","tags":null,"title":"Neural Network Architectures","type":"docs"},{"authors":null,"categories":null,"content":" \n Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nBatch Training Batch Training is something very important but we didn\u0026rsquo;t use before as our dataset was smaller and we were just learning how to train models.\nSo far,\n we took each example $(X^i, y^i)$ made prediction with $\\hat{y}^i = g(X^i.W+b)$ calculated the loss $\\mathcal{L}(y^i , \\hat{y}^i)$ used back propagation to update $W$ and $b$ (all parameters in the model) with $w := w - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w}$ and $b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$ and repeated this process.  What is the problem here?\nWe used many toy datasets which were in 1000s, so it will take few seconds/milliseconds for training the model for several epoch. But real world datasets may have millions of data and updating model with one example at a time may take a lot of time.\nThis is for multi class classification\n$z =\\begin{bmatrix}z_1\u0026amp;z_2\u0026amp;z_3\\ \\end{bmatrix} = \\begin{bmatrix}x_1\u0026amp;x_2 \\ \\end{bmatrix} . \\begin{bmatrix}W_{11}\u0026amp;W_{12}\u0026amp;W_{13} \\\\ W_{21}\u0026amp;W_{22}\u0026amp;W_{23} \\\\ \\end{bmatrix} + \\begin{bmatrix}b_1\u0026amp;b_2\u0026amp;b_3\\ \\end{bmatrix}$\n$\\hat{y} = \\sigma(z)$\nThis uses one example $X = [x_1, x_2]$ at a time, but it is also possible to use all the data together as a matrix(Advantage of matrix multiplication and linear algebra).\n$z = \\begin{bmatrix}z_1^1\u0026amp;z_2^1\u0026amp;z_3^1\\\\ z_1^2\u0026amp;z_2^2\u0026amp;z_3^2\\\\ \\vdots\u0026amp;\\vdots\u0026amp;\\vdots \\\\ z_1^n\u0026amp;z_2^n\u0026amp;z_3^n \\ \\end{bmatrix} = \\begin{bmatrix}x_1^1\u0026amp;x_2^1\\\\ x_1^2\u0026amp;x_2^2\\\\ \\vdots\u0026amp;\\vdots \\\\ x_1^n\u0026amp;x_2^n\\\\ \\end{bmatrix} . \\begin{bmatrix}W_{11}\u0026amp;W_{12}\u0026amp;W_{13}\\\\ W_{21}\u0026amp;W_{22}\u0026amp;W_{23}\\\\ \\end{bmatrix} + \\begin{bmatrix}b_1\u0026amp;b_2\u0026amp;b_3\\\\ \\end{bmatrix}$\n$ \\hat{y} = \\begin{bmatrix}\\sigma(z_1^1)\u0026amp;\\sigma(z_2^1)\u0026amp;\\sigma(z_3^1)\\\\ \\sigma(z_1^2)\u0026amp;\\sigma(z_2^2)\u0026amp;\\sigma(z_3^2)\\\\ \\vdots\u0026amp;\\vdots\u0026amp;\\vdots \\\\ \\sigma(z_1^n)\u0026amp;\\sigma(z_2^n)\u0026amp;\\sigma(z_3^n) \\ \\end{bmatrix} $\nThis is called batch gradient descent. There is a problem with this. When your data is in millions, holding all the millions of data into a matrix may cause memory issues. It\u0026rsquo;s not possible for the RAM to hold millions of data into the memory altogether.\nSo we do something called mini-batch gradient descent. We take few number of examples say m(batch size) which the memory can hold at a time and use that for training and during the next step we use the next m examples and continue till the complete dataset is used for training.\nExample:\n no of examples (n) = 2300 batch size (m) = 500 then we use [500, 500, 500, 500, 300] batches for training at a time.  By this way we use the advantage of matrix multiplication and do not fill the RAM memory.\nWe can change the batch size with a parameter called batch_size in model.fit.\nLets try different batch size.\nBatch Training in TensorFlow from sklearn.datasets import make_gaussian_quantiles from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt X, y = make_gaussian_quantiles(n_samples=2000, n_features=2, n_classes=2, random_state=3, cov=0.1) plt.figure(figsize=(10,10)) plt.scatter(X[:,0], X[:,1],c=y) plt.grid(True) plt.show() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True) X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True) print('Train = {}\\nTest = {}\\nVal = {}'.format(len(X_train), len(X_test), len(X_val)))  Train = 1120 Test = 600 Val = 280  Batch Size = 1 %%time #magic function to measure time of the cell import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() # random number initialized to same, for reproducing the same results. np.random.seed(0) tf.set_random_seed(0) model = tf.keras.Sequential([ keras.layers.Dense(units=10, input_shape=[2]), keras.layers.Activation('tanh'), keras.layers.Dense(units=10), keras.layers.Activation('tanh'), keras.layers.Dense(units=1), keras.layers.Activation('sigmoid') ]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, batch_size=1, epochs=10, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show() # test accuracy from sklearn.metrics import accuracy_score y_test_pred = model.predict(X_test) test_accuracy = accuracy_score((y_test_pred \u0026gt; 0.5), y_test) print('\\nTest Accuracy = ', test_accuracy)  WARNING: Logging before flag parsing goes to stderr. W0902 14:17:32.553351 139709616596864 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor W0902 14:17:32.656815 139709616596864 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.\u0026lt;locals\u0026gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 1120 samples, validate on 280 samples Epoch 1/10 1120/1120 [==============================] - 2s 2ms/sample - loss: 0.6972 - acc: 0.4634 - val_loss: 0.6945 - val_acc: 0.5321 Epoch 2/10 1120/1120 [==============================] - 2s 1ms/sample - loss: 0.6942 - acc: 0.5312 - val_loss: 0.6923 - val_acc: 0.4250 Epoch 3/10 1120/1120 [==============================] - 2s 1ms/sample - loss: 0.6922 - acc: 0.4821 - val_loss: 0.6834 - val_acc: 0.6607 . . Epoch 8/10 1120/1120 [==============================] - 2s 1ms/sample - loss: 0.5767 - acc: 0.7554 - val_loss: 0.5457 - val_acc: 0.7964 Epoch 9/10 1120/1120 [==============================] - 2s 1ms/sample - loss: 0.5507 - acc: 0.7571 - val_loss: 0.5231 - val_acc: 0.7929 Epoch 10/10 1120/1120 [==============================] - 2s 1ms/sample - loss: 0.5296 - acc: 0.7607 - val_loss: 0.4991 - val_acc: 0.8071  Test Accuracy = 0.7583333333333333 CPU times: user 21.5 s, sys: 1.52 s, total: 23 s Wall time: 19.5 s  Batch size of 1 for 10 epochs uses 23s. Let\u0026rsquo;s increase the batch size to 100 and check the time.\nBatch Size = 100 %%time #magic function to measure time of the cell import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() # random number initialized to same, for reproducing the same results. np.random.seed(0) tf.set_random_seed(0) model = tf.keras.Sequential([ keras.layers.Dense(units=10, input_shape=[2]), keras.layers.Activation('tanh'), keras.layers.Dense(units=10), keras.layers.Activation('tanh'), keras.layers.Dense(units=1), keras.layers.Activation('sigmoid') ]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, batch_size=100, epochs=10, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show() # test accuracy from sklearn.metrics import accuracy_score y_test_pred = model.predict(X_test) test_accuracy = accuracy_score((y_test_pred \u0026gt; 0.5), y_test) print('\\nTest Accuracy = ', test_accuracy)  Train on 1120 samples, validate on 280 samples Epoch 1/10 1120/1120 [==============================] - 0s 104us/sample - loss: 0.6993 - acc: 0.5116 - val_loss: 0.7020 - val_acc: 0.5107 Epoch 2/10 1120/1120 [==============================] - 0s 21us/sample - loss: 0.6960 - acc: 0.5134 - val_loss: 0.6988 - val_acc: 0.5179 Epoch 3/10 1120/1120 [==============================] - 0s 20us/sample - loss: 0.6946 - acc: 0.5223 - val_loss: 0.6963 - val_acc: 0.5143 . . Epoch 8/10 1120/1120 [==============================] - 0s 22us/sample - loss: 0.6921 - acc: 0.4384 - val_loss: 0.6926 - val_acc: 0.4214 Epoch 9/10 1120/1120 [==============================] - 0s 22us/sample - loss: 0.6921 - acc: 0.4848 - val_loss: 0.6920 - val_acc: 0.5071 Epoch 10/10 1120/1120 [==============================] - 0s 19us/sample - loss: 0.6920 - acc: 0.4902 - val_loss: 0.6924 - val_acc: 0.4821  Test Accuracy = 0.47333333333333333 CPU times: user 2.56 s, sys: 387 ms, total: 2.94 s Wall time: 2.34 s  Batch size of 100 took just 2.94s. Hope now you understood the advantage of Batch training.\nGradient Descent in Batch Training In batch training, we calculate the loss as average of loss of the batch, $\\mathcal{L}(y , \\hat{y}) = \\dfrac{1}{n} \\sum_{i=1}^{n}\\mathcal{L}(y^i , \\hat{y}^i)$\n$\\therefore \\dfrac{\\partial \\mathcal{L}}{\\partial W} = \\dfrac{1}{n} \\sum_{i=1}^{n}\\dfrac{\\partial \\mathcal{L}(y^i , \\hat{y}^i)}{\\partial W}$\nand $\\dfrac{\\partial \\mathcal{L}}{\\partial b} = \\dfrac{1}{n} \\sum_{i=1}^{n}\\dfrac{\\partial \\mathcal{L}(y^i , \\hat{y}^i)}{\\partial b}$\nWhich means the gradients are average of gradients over the batch of data.\nThen update the parameters normally with $w := w - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w}$ and $b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$.\n","date":1567378800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567378800,"objectID":"244bceb7dc5e8fc88c166bccc1b53eb9","permalink":"/courses/deeplearning/2.2/","publishdate":"2019-09-02T00:00:00+01:00","relpermalink":"/courses/deeplearning/2.2/","section":"courses","summary":"Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nBatch Training Batch Training is something very important but we didn\u0026rsquo;t use before as our dataset was smaller and we were just learning how to train models.\nSo far,\n we took each example $(X^i, y^i)$ made prediction with $\\hat{y}^i = g(X^i.W+b)$ calculated the loss $\\mathcal{L}(y^i , \\hat{y}^i)$ used back propagation to update $W$ and $b$ (all parameters in the model) with $w := w - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w}$ and $b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$ and repeated this process.","tags":null,"title":"Batch Training","type":"docs"},{"authors":null,"categories":null,"content":" \n Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nGradient Descent We have seen what are the steps followed in Gradient Descent to minimize the loss function. Now let\u0026rsquo;s get into more detail.\nThe objective of gradient descent is to find $W, b$ for which $\\mathcal{L}(W, b) $ is minimum for given pairs of data $(X, y)$.\nExample Let\u0026rsquo;s forget about Machine learning for sometime and use gradient descent algorithm for optimization of simple functions.\nGiven a function $y = f(x) = 0.08x^2+sin(x)$, the objective is to find $x$ at which $y = f(x)$ is minimum.\nLet\u0026rsquo;s use Numpy to solve this.\nimport numpy as np import matplotlib.pyplot as plt def f(x): return 0.08* x**2 + np.sin(x) x = np.arange(-10, 10, 0.01) y = f(x) plt.plot(x, y, label='$y = f(x)$') plt.xlabel('$x$') plt.ylabel('$y$') plt.legend() plt.show()  We are going to use gradient descent to find $x$ for which this function is minimum.\nStep - 1 Choose a random point\nrandom_idx = np.random.choice(2000) random_x = x[random_idx] plt.plot(x, y, label='$y = f(x)$') plt.scatter(random_x, f(random_x)) plt.xlabel('$x$') plt.ylabel('$y$') plt.legend() plt.show()  Step - 2 calculate $\\dfrac{\\partial \\mathcal{y}}{\\partial x}$.\nWhy? Let\u0026rsquo;s visualize it\nnp.random.seed(60) random_idx = np.random.choice(2000) random_x = x[random_idx] a = random_x h = 0.01 x_tan = np.arange(a-1, a+1, 0.01) fprime = (f(a+h)-f(a))/h tan = f(a)+fprime*(x_tan-a) plt.figure(figsize=(12,8)) plt.plot(x, y, label='$y = f(x)$') plt.plot(x_tan, tan, '--', label='gradient $\\dfrac{\\partial \\mathcal{y}}{\\partial x}$', linewidth=4.0) plt.text(a+0.2,f(a+0.2),'$\\dfrac{\\partial \\mathcal{y}}{\\partial x}$='+'{:.3f}'.format(fprime), fontsize=20) plt.scatter(random_x, f(random_x), s=200, color='black') plt.xlabel('$x$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.legend(fontsize = 'xx-large') plt.show()  The gradient of $y = f(x) = 0.08x^2+sin(x)$ is $\\dfrac{\\partial \\mathcal{y}}{\\partial x} = 0.16x+cos(x)$\nThe gradient or slope $\\dfrac{\\partial \\mathcal{y}}{\\partial x}$ of the curve at the randomly selected point is negative. Which means the function will decrease as x increases and the function will increase as x decreases.\nOur objective is to minimize the function, so we calculate $\\dfrac{\\partial \\mathcal{y}}{\\partial x}$, to decide in which direction to move, so we reach the minimum.\n if $\\dfrac{\\partial \\mathcal{y}}{\\partial x} \u0026gt; 0$, we move towards left of decrease x if $\\dfrac{\\partial \\mathcal{y}}{\\partial x} \u0026lt; 0$, we move towards right or increase x if $\\dfrac{\\partial \\mathcal{y}}{\\partial x} = 0$, which means we are already at a point of minimum  But how much to increase x?\nStep - 3 Learning rate We now know, given a point should we increase or decrease x to reach minimum.\nHow much to increase or decrease x?\nWe change x with a factor called learning rate $\\alpha$. So we update x with $x := x - \\alpha \\frac{\\partial \\mathcal{y}}{\\partial x}$\n when $\\alpha$ is small, x is increased in small steps, so more iterations are required to reach minimum, but this will lead to more accurate steps. when $\\alpha$ is larger, x will increase in larger steps, so x will reach minimum y in few steps, but there is a risk of x skipping the minimum point, as x is increased in larger value.  When do we stop? We can iterate of any number of iterations, but when x reaches minimum y , then $\\dfrac{\\partial \\mathcal{y}}{\\partial x} = 0$\n$\\therefore x := x - \\alpha . 0$, so x will remain the same, even if you iterate more.\nLet\u0026rsquo;s code this.\nimport numpy as np def gradient(x): return 0.16*x+np.cos(x)  np.random.seed(24) # change the seed to start from different points x = np.arange(-10, 10, 0.01) y = f(x) random_idx = np.random.choice(2000) x_iter = x[random_idx] epochs = 100 lr = 0.2 for i in range(epochs): dy_dx = gradient(x_iter) x_iter = x_iter - lr * dy_dx if i%20 == 0: a = x_iter h = 0.01 x_tan = np.arange(a-1, a+1, 0.01) fprime = (f(a+h)-f(a))/h tan = f(a)+fprime*(x_tan-a) plt.figure(figsize=(12,8)) plt.plot(x, y, label='$y = f(x)$') plt.plot(x_tan, tan, '--', label='gradient $\\dfrac{\\partial \\mathcal{y}}{\\partial x}$', linewidth=4.0) plt.text(a+0.2,f(a+0.2),'$\\dfrac{\\partial \\mathcal{y}}{\\partial x}$='+'{:.3f}'.format(fprime), fontsize=20) plt.scatter(a, f(a), s=200, color='black') plt.xlabel('$x$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.legend(fontsize = 'xx-large') plt.show()  np.random.seed(24) # change the seed to start from different points x = np.arange(-10, 10, 0.01) y = f(x) random_idx = np.random.choice(2000) x_iter = x[random_idx] epochs = 100 lr = 0.2 x_list = [] y_list = [] for i in range(epochs): dy_dx = gradient(x_iter) x_iter = x_iter - lr * dy_dx x_list.append(x_iter) y_list.append(f(x_iter)) plt.figure(figsize=(12,8)) plt.plot(x, y, label='$y = f(x)$') for i in range(len(x_list)): plt.scatter(x_list[i], y_list[i], s=200, color='black') plt.xlabel('$x$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.legend(fontsize = 'xx-large') plt.show()  This is the path followed to reach the minimum.\nRun the code with - large lr - small lr - more epochs - less epochs\nGD with Momentum The updation step is changed with\n$v = \\beta v + (1-\\beta) \\dfrac{\\partial \\mathcal{y}}{\\partial x}$\n$x = x - \\alpha v$\nThis algorithms smoothens the updation process, by averaging the movement in other the direction and forces x to move in the required direction which fastens the process.\nMomentum in Numpy np.random.seed(23) # change the seed to start from different points x = np.arange(-10, 10, 0.01) y = f(x) random_idx = np.random.choice(2000) x_iter = x[random_idx] epochs = 100 lr = 0.2 v = 0 b = 0.9 for i in range(epochs): dy_dx = gradient(x_iter) v = b * v + (1-b) * dy_dx x_iter = x_iter - lr * v if i%20 == 0: a = x_iter h = 0.01 x_tan = np.arange(a-1, a+1, 0.01) fprime = (f(a+h)-f(a))/h tan = f(a)+fprime*(x_tan-a) plt.figure(figsize=(12,8)) plt.plot(x, y, label='$y = f(x)$') plt.plot(x_tan, tan, '--', label='gradient $\\dfrac{\\partial \\mathcal{y}}{\\partial x}$', linewidth=4.0) plt.text(a+0.2,f(a+0.2),'$\\dfrac{\\partial \\mathcal{y}}{\\partial x}$='+'{:.3f}'.format(fprime), fontsize=20) plt.scatter(a, f(a), s=200, color='black') plt.xlabel('$x$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.legend(fontsize = 'xx-large') plt.show()  np.random.seed(24) # change the seed to start from different points x = np.arange(-10, 10, 0.01) y = f(x) random_idx = np.random.choice(2000) x_iter = x[random_idx] epochs = 200 lr = 0.1 v = 0 b = 0.9 x_list = [] y_list = [] for i in range(epochs): dy_dx = gradient(x_iter) v = b * v + (1-b) * dy_dx x_iter = x_iter - lr * v x_list.append(x_iter) y_list.append(f(x_iter)) plt.figure(figsize=(12,8)) plt.plot(x, y, label='$y = f(x)$') for i in range(len(x_list)): plt.scatter(x_list[i], y_list[i], s=200, color='black') plt.xlabel('$x$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.legend(fontsize = 'xx-large') plt.show()  Optimization algorithms with Tensorflow Check the Tensorflow docs.\nSo far we have been using \u0026lsquo;Adam\u0026rsquo; algorithm for optimization with default parameter values. We will now train models with different optimizers and hyper parameters.\nfrom sklearn.datasets import make_gaussian_quantiles from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt X, y = make_gaussian_quantiles(n_samples=2000, n_features=2, n_classes=2, random_state=3, cov=0.1) plt.figure(figsize=(10,10)) plt.scatter(X[:,0], X[:,1],c=y) plt.grid(True) plt.show() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True) X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True) print('Train = {}\\nTest = {}\\nVal = {}'.format(len(X_train), len(X_test), len(X_val)))  Train = 1120 Test = 600 Val = 280  SGD optimizer Stochastic gradient descent without momentum import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() # random number initialized to same, for reproducing the same results. np.random.seed(0) tf.set_random_seed(0) model = tf.keras.Sequential([ keras.layers.Dense(units=10, input_shape=[2]), keras.layers.Activation('tanh'), keras.layers.Dense(units=10), keras.layers.Activation('tanh'), keras.layers.Dense(units=1), keras.layers.Activation('sigmoid') ]) # we used the default optimizer parameters by using optimizer='adam' # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # we can also define optimizer with optimizer = tf.keras.optimizers.SGD(lr=0.01, nesterov=False) model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, batch_size=100, epochs=50, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show() # test accuracy from sklearn.metrics import accuracy_score y_test_pred = model.predict(X_test) test_accuracy = accuracy_score((y_test_pred \u0026gt; 0.5), y_test) print('\\nTest Accuracy = ', test_accuracy)  Train on 1120 samples, validate on 280 samples Epoch 1/50 1120/1120 [==============================] - 0s 96us/sample - loss: 0.7056 - acc: 0.4991 - val_loss: 0.7119 - val_acc: 0.4536 Epoch 2/50 1120/1120 [==============================] - 0s 20us/sample - loss: 0.7048 - acc: 0.5000 - val_loss: 0.7114 - val_acc: 0.4643 . . Epoch 49/50 1120/1120 [==============================] - 0s 19us/sample - loss: 0.6910 - acc: 0.6366 - val_loss: 0.6997 - val_acc: 0.5929 Epoch 50/50 1120/1120 [==============================] - 0s 22us/sample - loss: 0.6910 - acc: 0.6348 - val_loss: 0.6991 - val_acc: 0.5714  Test Accuracy = 0.6633333333333333  Stochastic gradient descent with momentum import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() # random number initialized to same, for reproducing the same results. np.random.seed(0) tf.set_random_seed(0) model = tf.keras.Sequential([ keras.layers.Dense(units=10, input_shape=[2]), keras.layers.Activation('tanh'), keras.layers.Dense(units=10), keras.layers.Activation('tanh'), keras.layers.Dense(units=1), keras.layers.Activation('sigmoid') ]) # we used the default optimizer parameters by using optimizer='adam' # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # we can also define optimizer with optimizer = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True) model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, batch_size=100, epochs=50, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show() # test accuracy from sklearn.metrics import accuracy_score y_test_pred = model.predict(X_test) test_accuracy = accuracy_score((y_test_pred \u0026gt; 0.5), y_test) print('\\nTest Accuracy = ', test_accuracy)  Train on 1120 samples, validate on 280 samples Epoch 1/50 1120/1120 [==============================] - 0s 104us/sample - loss: 0.7047 - acc: 0.5080 - val_loss: 0.7094 - val_acc: 0.4821 Epoch 2/50 1120/1120 [==============================] - 0s 20us/sample - loss: 0.7002 - acc: 0.5000 - val_loss: 0.7033 - val_acc: 0.4714 . . Epoch 49/50 1120/1120 [==============================] - 0s 20us/sample - loss: 0.6351 - acc: 0.7714 - val_loss: 0.6531 - val_acc: 0.7214 Epoch 50/50 1120/1120 [==============================] - 0s 21us/sample - loss: 0.6307 - acc: 0.7750 - val_loss: 0.6463 - val_acc: 0.7393  Test Accuracy = 0.7866666666666666  You can note the difference, optimizer with momentum got accuracy of 0.78 in 50 epochs, whereas optimizer without momentum got 0.663 in 50 epochs\nAdam optimizer Check this paper to know about adam.\nimport tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() # random number initialized to same, for reproducing the same results. np.random.seed(0) tf.set_random_seed(0) model = tf.keras.Sequential([ keras.layers.Dense(units=10, input_shape=[2]), keras.layers.Activation('tanh'), keras.layers.Dense(units=10), keras.layers.Activation('tanh'), keras.layers.Dense(units=1), keras.layers.Activation('sigmoid') ]) # we used the default optimizer parameters by using optimizer='adam' # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # we can also define optimizer with optimizer = tf.keras.optimizers.Adam(lr=0.01) model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, batch_size=100, epochs=50, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show() # test accuracy from sklearn.metrics import accuracy_score y_test_pred = model.predict(X_test) test_accuracy = accuracy_score((y_test_pred \u0026gt; 0.5), y_test) print('\\nTest Accuracy = ', test_accuracy)  Train on 1120 samples, validate on 280 samples Epoch 1/50 1120/1120 [==============================] - 0s 120us/sample - loss: 0.6971 - acc: 0.5080 - val_loss: 0.6996 - val_acc: 0.6036 Epoch 2/50 1120/1120 [==============================] - 0s 24us/sample - loss: 0.6908 - acc: 0.5545 - val_loss: 0.6979 - val_acc: 0.5357 . . Epoch 49/50 1120/1120 [==============================] - 0s 33us/sample - loss: 0.0785 - acc: 0.9804 - val_loss: 0.0694 - val_acc: 0.9821 Epoch 50/50 1120/1120 [==============================] - 0s 25us/sample - loss: 0.0748 - acc: 0.9804 - val_loss: 0.0729 - val_acc: 0.9786  Test Accuracy = 0.9833333333333333  Adam was able to give an accuracy of 0.98 in the same 50 epochs.\nSo Optimization algorithms affect the learning performance a lot. A good(right) optimization algorithm can improve your performance a lot.\nCheck out the other optimizers in Tensorflow in the docs.\n","date":1567465200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567465200,"objectID":"86562c789f546635079bec5272feab08","permalink":"/courses/deeplearning/2.3/","publishdate":"2019-09-03T00:00:00+01:00","relpermalink":"/courses/deeplearning/2.3/","section":"courses","summary":"Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nGradient Descent We have seen what are the steps followed in Gradient Descent to minimize the loss function. Now let\u0026rsquo;s get into more detail.\nThe objective of gradient descent is to find $W, b$ for which $\\mathcal{L}(W, b) $ is minimum for given pairs of data $(X, y)$.\nExample Let\u0026rsquo;s forget about Machine learning for sometime and use gradient descent algorithm for optimization of simple functions.","tags":null,"title":"Optimizers","type":"docs"},{"authors":null,"categories":null,"content":" \n Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nLearning Rate In Gradient Descent we update the parameters of the model with\n$w := w - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w}$ and $b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$.\nThe learning rate $\\alpha$ actually affects the learning process a lot.\n small $\\alpha$ is slow, but more accurate, as it does not miss the minimum, but it also get stuck in a local minimum. larger $\\alpha$ make huge steps, sometimes it may converge faster but it may miss the minima.  Learning rate is an important hyper parameter in training a Model. It is important to understand how to tune the learning rate.\nfrom sklearn.datasets import make_gaussian_quantiles from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt X, y = make_gaussian_quantiles(n_samples=2000, n_features=2, n_classes=2, random_state=3, cov=0.1) plt.figure(figsize=(10,10)) plt.scatter(X[:,0], X[:,1],c=y) plt.grid(True) plt.show() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True) X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True) print('Train = {}\\nTest = {}\\nVal = {}'.format(len(X_train), len(X_test), len(X_val)))  Train = 1120 Test = 600 Val = 280  Large Learning rate We will use adam optimizer with learning rate of 0.5\nimport tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() # random number initialized to same, for reproducing the same results. np.random.seed(0) tf.set_random_seed(0) model = tf.keras.Sequential([ keras.layers.Dense(units=10, input_shape=[2]), keras.layers.Activation('tanh'), keras.layers.Dense(units=10), keras.layers.Activation('tanh'), keras.layers.Dense(units=1), keras.layers.Activation('sigmoid') ]) # we used the default optimizer parameters by using optimizer='adam' # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # we can also define optimizer with optimizer = tf.keras.optimizers.Adam(lr=0.5) model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, batch_size=100, epochs=50, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show() # test accuracy from sklearn.metrics import accuracy_score y_test_pred = model.predict(X_test) test_accuracy = accuracy_score((y_test_pred \u0026gt; 0.5), y_test) print('\\nTest Accuracy = ', test_accuracy)  WARNING: Logging before flag parsing goes to stderr. W0903 06:12:55.336309 139678620190592 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor W0903 06:12:55.442114 139678620190592 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.\u0026lt;locals\u0026gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 1120 samples, validate on 280 samples Epoch 1/50 1120/1120 [==============================] - 0s 130us/sample - loss: 0.8366 - acc: 0.4786 - val_loss: 0.7390 - val_acc: 0.4464 Epoch 2/50 1120/1120 [==============================] - 0s 22us/sample - loss: 0.7115 - acc: 0.4902 - val_loss: 0.7462 - val_acc: 0.4464 . . Epoch 49/50 1120/1120 [==============================] - 0s 23us/sample - loss: 0.7400 - acc: 0.4920 - val_loss: 0.6896 - val_acc: 0.5536 Epoch 50/50 1120/1120 [==============================] - 0s 22us/sample - loss: 0.7273 - acc: 0.5009 - val_loss: 0.6900 - val_acc: 0.5536  Test Accuracy = 0.49666666666666665  This is one of the worst models you can train, as the learning rate is very high the optimizer skips all the minima and the loss does not really reduces much.\nSmall Learning rate We will use adam optimizer with learning rate of 0.00001\nimport tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() # random number initialized to same, for reproducing the same results. np.random.seed(0) tf.set_random_seed(0) model = tf.keras.Sequential([ keras.layers.Dense(units=10, input_shape=[2]), keras.layers.Activation('tanh'), keras.layers.Dense(units=10), keras.layers.Activation('tanh'), keras.layers.Dense(units=1), keras.layers.Activation('sigmoid') ]) # we used the default optimizer parameters by using optimizer='adam' # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # we can also define optimizer with optimizer = tf.keras.optimizers.Adam(lr=0.00001) model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, batch_size=100, epochs=50, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show() # test accuracy from sklearn.metrics import accuracy_score y_test_pred = model.predict(X_test) test_accuracy = accuracy_score((y_test_pred \u0026gt; 0.5), y_test) print('\\nTest Accuracy = ', test_accuracy)  Train on 1120 samples, validate on 280 samples Epoch 1/50 1120/1120 [==============================] - 0s 108us/sample - loss: 0.7051 - acc: 0.5018 - val_loss: 0.7056 - val_acc: 0.4821 Epoch 2/50 1120/1120 [==============================] - 0s 22us/sample - loss: 0.7050 - acc: 0.5018 - val_loss: 0.7055 - val_acc: 0.4821 . . Epoch 49/50 1120/1120 [==============================] - 0s 23us/sample - loss: 0.7027 - acc: 0.5009 - val_loss: 0.7036 - val_acc: 0.4786 Epoch 50/50 1120/1120 [==============================] - 0s 21us/sample - loss: 0.7026 - acc: 0.5009 - val_loss: 0.7035 - val_acc: 0.4786  Test Accuracy = 0.5116666666666667  The loss in this model seems to reduce, but at a very slow rate, so we need to train for more epochs to get a good model.\nMedium Learning rate We will use adam optimizer with learning rate of 0.005\nimport tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() # random number initialized to same, for reproducing the same results. np.random.seed(0) tf.set_random_seed(0) model = tf.keras.Sequential([ keras.layers.Dense(units=10, input_shape=[2]), keras.layers.Activation('tanh'), keras.layers.Dense(units=10), keras.layers.Activation('tanh'), keras.layers.Dense(units=1), keras.layers.Activation('sigmoid') ]) # we used the default optimizer parameters by using optimizer='adam' # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # we can also define optimizer with optimizer = tf.keras.optimizers.Adam(lr=0.005) model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, batch_size=100, epochs=50, verbose=True, validation_data=(X_val, y_val)) # contour plot xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show() # test accuracy from sklearn.metrics import accuracy_score y_test_pred = model.predict(X_test) test_accuracy = accuracy_score((y_test_pred \u0026gt; 0.5), y_test) print('\\nTest Accuracy = ', test_accuracy)  Train on 1120 samples, validate on 280 samples Epoch 1/50 1120/1120 [==============================] - 0s 114us/sample - loss: 0.6968 - acc: 0.5188 - val_loss: 0.6958 - val_acc: 0.3679 Epoch 2/50 1120/1120 [==============================] - 0s 28us/sample - loss: 0.6926 - acc: 0.4098 - val_loss: 0.6941 - val_acc: 0.4643 . . Epoch 49/50 1120/1120 [==============================] - 0s 22us/sample - loss: 0.1305 - acc: 0.9652 - val_loss: 0.1324 - val_acc: 0.9607 Epoch 50/50 1120/1120 [==============================] - 0s 22us/sample - loss: 0.1247 - acc: 0.9714 - val_loss: 0.1046 - val_acc: 0.9821  Test Accuracy = 0.96  See how tuning the model from an accuracy of 0.49 to 0.96. Hyper parameter tuning is so critical in getting a good learning model.\nImage Source\nLearning Rate Scheduling We have seen how crucial Learning rate is for training a model. It is also possible that near some minima our optimal learning rate may be big and it doesn\u0026rsquo;t reach the minima , at that point we may want to reduce the learning rate. This is learning rate scheduling. Instead of using a constant learning rate, we change the learning rate during training to reach the minima.\nThere are many types of lr scheduling\n step decay cosine decay reduce on plateau cycline decay  LR scheduling can easily be done in Tensorflow\n def scheduler(epoch): if epoch \u0026lt; 10: return 0.001 else: return 0.001 * tf.math.exp(0.1 * (10 - epoch)) callback = tf.keras.callbacks.LearningRateScheduler(scheduler) model.fit(data, labels, epochs=100, callbacks=[callback], validation_data=(val_data, val_labels))  Let\u0026rsquo;s train a model with ReduceOnPlateau lr scheduling. This checks the loss after every epoch, if the loss is not decreasing for few epochs(say 5) then it reduces the learning by a factor(say 0.1).\nReduceOnPlateau in TensorFlow import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() # random number initialized to same, for reproducing the same results. np.random.seed(0) tf.set_random_seed(0) model = tf.keras.Sequential([ keras.layers.Dense(units=10, input_shape=[2]), keras.layers.Activation('tanh'), keras.layers.Dense(units=10), keras.layers.Activation('tanh'), keras.layers.Dense(units=1), keras.layers.Activation('sigmoid') ]) optimizer = tf.keras.optimizers.Adam(lr=0.05) from tensorflow.keras.callbacks import ReduceLROnPlateau reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001, verbose=True) model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(X_train, y_train, batch_size=50, epochs=100, verbose=True, validation_data=(X_val, y_val), callbacks=[reduce_lr]) # contour plot xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01] grid = np.c_[xx.ravel(), yy.ravel()] probs = model.predict(grid)[:,0].reshape(xx.shape) f, ax = plt.subplots(figsize=(8, 6)) contour = ax.contourf(xx, yy, probs, 25, cmap=\u0026quot;RdBu\u0026quot;, vmin=0, vmax=1) ax_c = f.colorbar(contour) ax_c.set_label(\u0026quot;$P(y = 1)$\u0026quot;) ax_c.set_ticks([0, .25, .5, .75, 1]) ax.scatter(X[:,0], X[:, 1], c=y, s=50, cmap=\u0026quot;RdBu\u0026quot;, vmin=-.2, vmax=1.2, edgecolor=\u0026quot;white\u0026quot;, linewidth=1) plt.show() # test accuracy from sklearn.metrics import accuracy_score y_test_pred = model.predict(X_test) test_accuracy = accuracy_score((y_test_pred \u0026gt; 0.5), y_test) print('\\nTest Accuracy = ', test_accuracy)  Train on 1120 samples, validate on 280 samples Epoch 1/100 1120/1120 [==============================] - 0s 156us/sample - loss: 0.6948 - acc: 0.5321 - val_loss: 0.6631 - val_acc: 0.8321 Epoch 2/100 1120/1120 [==============================] - 0s 45us/sample - loss: 0.6391 - acc: 0.6527 - val_loss: 0.6036 - val_acc: 0.7821 . Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.02500000037252903. . Epoch 00078: ReduceLROnPlateau reducing learning rate to 0.012500000186264515. . Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575. . Epoch 99/100 1120/1120 [==============================] - 0s 42us/sample - loss: 0.0165 - acc: 0.9955 - val_loss: 0.0202 - val_acc: 0.9857 Epoch 100/100 1120/1120 [==============================] - 0s 43us/sample - loss: 0.0167 - acc: 0.9982 - val_loss: 0.0174 - val_acc: 0.9929  Test Accuracy = 0.9916666666666667  You can see how the learning rates are decreasing when validation loss is not decreasing for 10 steps. Learning rate scheduling will improve the model performance, you will observe it when you train a real world large dataset.\n","date":1567465200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567465200,"objectID":"095945bf40f68f73df10053a413bcf3d","permalink":"/courses/deeplearning/2.4/","publishdate":"2019-09-03T00:00:00+01:00","relpermalink":"/courses/deeplearning/2.4/","section":"courses","summary":"Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nLearning Rate In Gradient Descent we update the parameters of the model with\n$w := w - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w}$ and $b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$.\nThe learning rate $\\alpha$ actually affects the learning process a lot.\n small $\\alpha$ is slow, but more accurate, as it does not miss the minimum, but it also get stuck in a local minimum.","tags":null,"title":"Learning Rate","type":"docs"},{"authors":null,"categories":null,"content":" \n Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nBias \u0026amp; Variance Let us train a DNN model for a simple regression problem.\nimport numpy as np import matplotlib.pyplot as plt def dataset(show=True): X = np.arange(-5, 5, 0.01) y = 8 * np.sin(X) + np.random.randn(1000) if show: yy = 8 * np.sin(X) plt.figure(figsize=(15,9)) plt.scatter(X, y) plt.plot(X, yy, color='red', linewidth=7) plt.show() return X, y X, y = dataset(show=True)  Lets train 2 models for this dataset\n a very simple linear model a very complex DNN model  Simple Linear Model We are going to split the dataset into 5 groups(random shuffle) and use each of the 5 groups to train 5 different linear models. We will use sklearn\u0026rsquo;s StratifiedKFold to split the dataset into 5. Check the docs.\nimport tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() import random predictions = [] for i in range(5): idx = random.choices(np.arange(1000), k=700) X_train, y_train = X[idx], y[idx] model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1]) ]) optimizer = tf.keras.optimizers.Adam(lr=0.001) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_train, y_train, batch_size=100, epochs=200, verbose=False) prediction = model.predict(X) predictions.append(prediction) plt.figure(figsize=(12,9)) plt.plot(X, predictions[0]) plt.plot(X, predictions[1]) plt.plot(X, predictions[2]) plt.plot(X, predictions[3]) plt.plot(X, predictions[4]) plt.plot(X, 8 * np.sin(X), linewidth=5, label='True curve y') plt.legend() plt.show()  Deep Neural Network model import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() import random predictions = [] for i in range(5): idx = random.choices(np.arange(1000), k=100) X_train, y_train = X[idx], y[idx] model = tf.keras.Sequential([ keras.layers.Dense(units=50, input_shape=[1]), keras.layers.Activation('relu'), keras.layers.Dense(units=50), keras.layers.Activation('relu'), keras.layers.Dense(units=1), ]) optimizer = tf.keras.optimizers.Adam(lr=0.001) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_train, y_train, batch_size=100, epochs=200, verbose=False) prediction = model.predict(X) predictions.append(prediction) plt.figure(figsize=(12,9)) plt.plot(X, predictions[0]) plt.plot(X, predictions[1]) plt.plot(X, predictions[2]) plt.plot(X, predictions[3]) plt.plot(X, predictions[4]) plt.plot(X, 8 * np.sin(X), linewidth=5, label='True curve y') plt.legend() plt.show()  Bias Bias is defined as $ Bias = E[\\hat{y}] - y$\nIt is the difference between the expected value of prediction and the true curve. The expected value will be calculated by splitting the data into n parts and training n model on those n data parts and average of that n model prediction will be expected value.\nYou can see the bias for first model will be very high as the model predicts a straight line, but the true curve is sinusoidal. But the bias for 2nd model will be lower than 1st model.\nVariance Variance as you should know defines how much a data is varying. $Variance(\\hat{y}) = E[(\\hat{y} - E[\\hat{y}])^2]$ Although the predictions are not good, but the variance of 2nd model will be higher than 1st model, as the 2nd complex model will try to fit the data more.\n   Model Bias Variance     Simple Model High Low   Very Complex model Low High    Bias-Variance Tradeoff Let\u0026rsquo;s do some math first and discuss about it.\nBias-Variance Decomposition $MSE = E[(y - \\hat{y})^2] = E[y^2 - 2.y.\\hat{y} + \\hat{y}^2]$\nhere the random variable is $\\hat{y}$ as it is dependent on $X$.\n$ MSE = y^2 - 2.y.E[\\hat{y}] + E[\\hat{y}^2]$\n$Bias = E[\\hat{y}] - y$\n$Bias^2 = (E[\\hat{y}] - y)^2 = E[\\hat{y}]^2 + y^2 - 2yE[\\hat{y}]$\n$Variance = E[(\\hat{y} - E[\\hat{y}])^2] = = E[\\hat{y}^2] + E[\\hat{y}]^2 - 2E[\\hat{y} E[\\hat{y}]] = E[\\hat{y}^2] + E[\\hat{y}]^2 - 2E[\\hat{y}]^2 = E[\\hat{y}^2] - E[\\hat{y}]^2$\n$Bias^2 + Variance = y^2 - 2.y.E[\\hat{y}] + E[\\hat{y}^2] = MSE$\n$Bias^2 + Variance = MSE$\n when the bias is high(Simple Model), MSE is high, We don\u0026rsquo;t want high Loss, so we don\u0026rsquo;t want high bias when the variance is high(complex model), again MSE is high, so we don\u0026rsquo;t want high variance  Conclusion is that we need to choose a model which doesn\u0026rsquo;t have high bias or high variance, something optimal bias-variance in between will do good.\nImage Source\nUnderfitting When a model have high bias, then the model is \u0026ldquo;Underfitting\u0026rdquo;. Let\u0026rsquo;s look at an example first\nimport numpy as np import matplotlib.pyplot as plt def dataset(show=True): X = np.arange(-5, 5, 0.1) y = 8 * np.sin(X) + np.random.randn(100) if show: yy = 8 * np.sin(X) plt.figure(figsize=(15,9)) plt.scatter(X, y) plt.plot(X, yy, color='red', linewidth=7) plt.show() return X, y X, y = dataset(show=True)  import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True) model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1]) ]) optimizer = tf.keras.optimizers.Adam(lr=0.001) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_train, y_train, batch_size=100, epochs=200, verbose=True, validation_data=(X_test, y_test)) prediction = model.predict(X) plt.figure(figsize=(12,9)) plt.plot(X, prediction) plt.plot(X, 8 * np.sin(X), linewidth=5, label='True curve y') plt.legend() plt.show()  Train on 70 samples, validate on 30 samples Epoch 1/200 70/70 [==============================] - 0s 1ms/sample - loss: 33.6902 - val_loss: 41.1840 Epoch 2/200 70/70 [==============================] - 0s 57us/sample - loss: 33.6857 - val_loss: 41.1832 . . Epoch 199/200 70/70 [==============================] - 0s 54us/sample - loss: 33.1816 - val_loss: 41.3314 Epoch 200/200 70/70 [==============================] - 0s 59us/sample - loss: 33.1806 - val_loss: 41.3328  You can see the Training data loss and Validation data loss both are bad, the model performance is not good. This is called Underfitting.\nUnderfitting may happen because the model is not complex enough, or need more training. So, using a deeper network or training for more time may help.\nOverfitting Let\u0026rsquo;s train a more complex model with less training data.\nimport tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt tf.keras.backend.clear_session() from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, shuffle=True) model = tf.keras.Sequential([ keras.layers.Dense(units=50, input_shape=[1]), keras.layers.Activation('relu'), keras.layers.Dense(units=50), keras.layers.Activation('relu'), keras.layers.Dense(units=1), ]) optimizer = tf.keras.optimizers.Adam(lr=0.001) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_train, y_train, batch_size=100, epochs=1000, verbose=True, validation_data=(X_test, y_test)) prediction = model.predict(X_train) plt.figure(figsize=(12,9)) plt.scatter(X_train, prediction,label='Training Data Prediction') plt.scatter(X_test, model.predict(X_test), color='r', marker='x', label='Test Data Prediction') plt.plot(X, 8 * np.sin(X), linewidth=1, label='True curve y') plt.legend() plt.show()  Train on 10 samples, validate on 90 samples Epoch 1/1000 10/10 [==============================] - 0s 14ms/sample - loss: 31.7417 - val_loss: 37.6045 Epoch 2/1000 10/10 [==============================] - 0s 587us/sample - loss: 31.0950 - val_loss: 37.4865 . . Epoch 999/1000 10/10 [==============================] - 0s 561us/sample - loss: 0.5722 - val_loss: 17.3321 Epoch 1000/1000 10/10 [==============================] - 0s 497us/sample - loss: 0.5721 - val_loss: 17.3268  Here you can see, although the model is complex and can learn more complex features of the data, the Validation loss is way higher than training loss. This is called Overfitting. This means the model fits the training data so much that it does not generalize and perform very poorly in new unseen data. Adding more data can help to prevent overfitting.\n","date":1567465200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567465200,"objectID":"5a72059404eff674146cf90e5f571e95","permalink":"/courses/deeplearning/2.5/","publishdate":"2019-09-03T00:00:00+01:00","relpermalink":"/courses/deeplearning/2.5/","section":"courses","summary":"Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nBias \u0026amp; Variance Let us train a DNN model for a simple regression problem.\nimport numpy as np import matplotlib.pyplot as plt def dataset(show=True): X = np.arange(-5, 5, 0.01) y = 8 * np.sin(X) + np.random.randn(1000) if show: yy = 8 * np.sin(X) plt.figure(figsize=(15,9)) plt.scatter(X, y) plt.plot(X, yy, color='red', linewidth=7) plt.show() return X, y X, y = dataset(show=True)  Lets train 2 models for this dataset","tags":null,"title":"Bias \u0026 Variance","type":"docs"},{"authors":null,"categories":null,"content":" \n Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nMNIST Dataset The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\nLet\u0026rsquo;s get the dataset using tf.keras.datasets\nDownload MNIST import tensorflow as tf (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')  Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step  Visualize MNIST Let\u0026rsquo;s visualize what is in the dataset\nimport matplotlib.pyplot as plt num_imgs = 15 plt.figure(figsize=(num_imgs*2,3)) for i in range(1,num_imgs): plt.subplot(1,num_imgs,i).set_title('{}'.format(y_train[i])) plt.imshow(x_train[i], cmap='gray') plt.axis('off') plt.show()  Scale the data import numpy as np print('Max = {}\\nMin = {}'.format(np.max(x_train), np.min(x_train)))  Max = 255 Min = 0  The maximum value in the image if 255 and minimum value is 0. So we need to scale it to a smaller range for faster convergence.\nLet\u0026rsquo;s divide by maximum value of 255, so set it to a range of [0, 1].\nx_train = x_train/255 x_test = x_test/255 print('Max = {}\\nMin = {}'.format(np.max(x_train), np.min(x_train)))  Max = 1.0 Min = 0.0  One-Hot Encoding As this is a 10 class multiclass classification, we need to one hot encode the labels.\nprint(y_train[:5])  [5 0 4 1 9]  num_classes = 10 y_train = tf.keras.utils.to_categorical(y_train, num_classes) y_test = tf.keras.utils.to_categorical(y_test, num_classes) print(y_train[:5])  [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]  Flatten As we are going to use ANNs which takes in vectors and classify/ regress it. But the MNIST images are 2-D matrix, so we cannot directly pass them to ANNs. We need to flatten to the 2-D Matrix to 1-D vectors.\nThis 3x3 2-D matrix was flattened into a 9 unit vector. So our 28x28 MNIST image will be flattened into a 784 units vector.\nx = np.random.randn(3,3) print(x) print(x.shape) print() flattened_x = x.reshape(-1) print(flattened_x) print(flattened_x.shape)  [[-0.09460654 0.70636938 -0.73136131] [ 0.9414648 0.89831745 -0.03268361] [ 1.27416493 -0.37996 -0.31976928]] (3, 3) [-0.09460654 0.70636938 -0.73136131 0.9414648 0.89831745 -0.03268361 1.27416493 -0.37996 -0.31976928] (9,)  This can also be done by the model easily with Flatten layer, which we will add to the model.\nModel import tensorflow as tf from tensorflow import keras tf.keras.backend.clear_session() input_shape = (28,28) nclasses = 10 model = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape=input_shape), tf.keras.layers.Dense(units=50), tf.keras.layers.Activation('tanh'), tf.keras.layers.Dense(units=50), tf.keras.layers.Activation('tanh'), tf.keras.layers.Dense(units=nclasses), tf.keras.layers.Activation('sigmoid') ]) model.summary()  Model: \u0026quot;sequential\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 _________________________________________________________________ dense (Dense) (None, 50) 39250 _________________________________________________________________ activation (Activation) (None, 50) 0 _________________________________________________________________ dense_1 (Dense) (None, 50) 2550 _________________________________________________________________ activation_1 (Activation) (None, 50) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 510 _________________________________________________________________ activation_2 (Activation) (None, 10) 0 ================================================================= Total params: 42,310 Trainable params: 42,310 Non-trainable params: 0 _________________________________________________________________  Training the model optimizer = tf.keras.optimizers.Adam(lr=0.0005) model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) tf_history = model.fit(x_train, y_train, batch_size=100, epochs=100, verbose=True, validation_data=(x_test, y_test))  Train on 60000 samples, validate on 10000 samples Epoch 1/100 60000/60000 [==============================] - 3s 53us/sample - loss: 0.7443 - acc: 0.8594 - val_loss: 0.3127 - val_acc: 0.9262 Epoch 2/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.2590 - acc: 0.9319 - val_loss: 0.2154 - val_acc: 0.9402 . . Epoch 99/100 60000/60000 [==============================] - 3s 51us/sample - loss: 9.9574e-05 - acc: 1.0000 - val_loss: 0.1855 - val_acc: 0.9722 Epoch 100/100 60000/60000 [==============================] - 3s 49us/sample - loss: 9.1121e-05 - acc: 1.0000 - val_loss: 0.1879 - val_acc: 0.9719  plt.figure(figsize=(20,7)) plt.subplot(1,2,1) plt.plot(tf_history.history['loss'], label='Training Loss') plt.plot(tf_history.history['val_loss'], label='Validation Loss') plt.legend() plt.subplot(1,2,2) plt.plot(tf_history.history['acc'], label='Training Accuracy') plt.plot(tf_history.history['val_acc'], label='Validation Accuracy') plt.legend() plt.show()  You can observe from this learning curve, as we train more, the training performance improves, but the validation performance goes in reverse direction. This is because the model tries to overfit the training data to improve training data and does not generalize. We have few methods to avoid overfitting. Adding More data always work, but it\u0026rsquo;s not always possible to get more data, so few regularization techniques may help.\nWeight Regularization $\\mathcal{L} = \\dfrac{1}{m} \\sum_{i=1}^{m}\\mathcal{L}(\\hat{y}^i, y^i)$\nThis is the loss which we want to reduce suring gradient descent.\nWe will add some new terms to the loss function for regularization\n$\\mathcal{L} = \\dfrac{1}{m} \\sum_{i=1}^{m}\\mathcal{L}(\\hat{y}^i, y^i) + \\dfrac{\\lambda}{2m}\\sum_{l=1}^{L}||w^l||^2_2$\nwhere $\\lambda$ is called regularization parameter. This is L2 regularization as L2 norm is used.\nThe objective of any optimization algorithm is to reduce the Loss, now the loss has 2 terms\n MSE or Cross Entropy(can be other loss too) Regularization term  To reduce the loss, the optimizer should also reduce $\\dfrac{\\lambda}{2m}\\sum_{l=1}^{L}||w^l||^2_2$ term, which means the weights are also reduced or penalized. How much the weights are penalized depends on $\\lambda$, if $\\lambda$ is high then the weights are penalized more. Penalizing weights also penalize the activations, which makes a more complex activations to be simpler, so the model does not overfit.\nYou don\u0026rsquo;t need to implement it, you can regularize the weights in keras easily with kernel_regularizer.\nRegularization in Tensorflow import tensorflow as tf from tensorflow import keras tf.keras.backend.clear_session() input_shape = (28,28) nclasses = 10 model = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape=input_shape), tf.keras.layers.Dense(units=50, kernel_regularizer=tf.keras.regularizers.l2(0.0001)), tf.keras.layers.Activation('tanh'), tf.keras.layers.Dense(units=50, kernel_regularizer=tf.keras.regularizers.l2(0.0001)), tf.keras.layers.Activation('tanh'), tf.keras.layers.Dense(units=nclasses), tf.keras.layers.Activation('sigmoid') ]) model.summary()  Model: \u0026quot;sequential\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 _________________________________________________________________ dense (Dense) (None, 50) 39250 _________________________________________________________________ activation (Activation) (None, 50) 0 _________________________________________________________________ dense_1 (Dense) (None, 50) 2550 _________________________________________________________________ activation_1 (Activation) (None, 50) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 510 _________________________________________________________________ activation_2 (Activation) (None, 10) 0 ================================================================= Total params: 42,310 Trainable params: 42,310 Non-trainable params: 0 _________________________________________________________________  optimizer = tf.keras.optimizers.Adam(lr=0.0005) model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) tf_history_reg = model.fit(x_train, y_train, batch_size=100, epochs=100, verbose=True, validation_data=(x_test, y_test))  Train on 60000 samples, validate on 10000 samples Epoch 1/100 60000/60000 [==============================] - 3s 55us/sample - loss: 0.7775 - acc: 0.8574 - val_loss: 0.3297 - val_acc: 0.9241 Epoch 2/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.2782 - acc: 0.9320 - val_loss: 0.2294 - val_acc: 0.9450 . . Epoch 99/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.0249 - acc: 0.9988 - val_loss: 0.1390 - val_acc: 0.9724 Epoch 100/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.0279 - acc: 0.9977 - val_loss: 0.1359 - val_acc: 0.9741  plt.figure(figsize=(20,7)) plt.subplot(1,2,1) plt.plot(tf_history.history['loss'], label='Training Loss') plt.plot(tf_history.history['val_loss'], label='Validation Loss') plt.plot(tf_history_reg.history['loss'], label='Training Loss with Reg') plt.plot(tf_history_reg.history['val_loss'], label='Validation Loss with Reg', linestyle='--') plt.legend() plt.subplot(1,2,2) plt.plot(tf_history.history['acc'], label='Training Accuracy') plt.plot(tf_history.history['val_acc'], label='Validation Accuracy') plt.plot(tf_history_reg.history['acc'], label='Training Accuracy with Reg') plt.plot(tf_history_reg.history['val_acc'], label='Validation Accuracy with Reg', linestyle='--') plt.legend() plt.show()  L2 Regularization slightly improved the performance on validation set.\nTry changing the l2 regularization $\\lambda$ and observe the performance.\nDropouts Dropout drops certain nodes in our network during each pass with a defined probability p. So if p=0.5, then each node have a probability of 0.5 to get dropped.\nThis will make the network simpler during each pass depending on p and now the network cannot rely on any node as it may be dropped, so the network will not give high weights to any node as it may be dropped any time and every node will be utilized equally as all of them have equal probability of being dropped.\nAgain this can be easily used in keras with tf.keras.layers.Dropout(p).\nDropouts in Tensorflow import tensorflow as tf from tensorflow import keras tf.keras.backend.clear_session() input_shape = (28,28) nclasses = 10 model = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape=input_shape), tf.keras.layers.Dense(units=50), tf.keras.layers.Activation('tanh'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(units=50), tf.keras.layers.Activation('tanh'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(units=nclasses), tf.keras.layers.Activation('sigmoid') ]) model.summary()  Model: \u0026quot;sequential\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 _________________________________________________________________ dense (Dense) (None, 50) 39250 _________________________________________________________________ activation (Activation) (None, 50) 0 _________________________________________________________________ dropout (Dropout) (None, 50) 0 _________________________________________________________________ dense_1 (Dense) (None, 50) 2550 _________________________________________________________________ activation_1 (Activation) (None, 50) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 50) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 510 _________________________________________________________________ activation_2 (Activation) (None, 10) 0 ================================================================= Total params: 42,310 Trainable params: 42,310 Non-trainable params: 0 _________________________________________________________________  optimizer = tf.keras.optimizers.Adam(lr=0.0001) model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) tf_history_dp = model.fit(x_train, y_train, batch_size=100, epochs=100, verbose=True, validation_data=(x_test, y_test))  Train on 60000 samples, validate on 10000 samples Epoch 1/100 60000/60000 [==============================] - 4s 60us/sample - loss: 1.6479 - acc: 0.6138 - val_loss: 1.1078 - val_acc: 0.8455 Epoch 2/100 60000/60000 [==============================] - 3s 53us/sample - loss: 0.9871 - acc: 0.8051 - val_loss: 0.6767 - val_acc: 0.8874 . . Epoch 99/100 60000/60000 [==============================] - 3s 54us/sample - loss: 0.1294 - acc: 0.9605 - val_loss: 0.1068 - val_acc: 0.9677 Epoch 100/100 60000/60000 [==============================] - 3s 55us/sample - loss: 0.1294 - acc: 0.9605 - val_loss: 0.1065 - val_acc: 0.9680  plt.figure(figsize=(20,7)) plt.subplot(1,2,1) plt.plot(tf_history.history['loss'], label='Training Loss') plt.plot(tf_history.history['val_loss'], label='Validation Loss') plt.plot(tf_history_dp.history['loss'], label='Training Loss with Dp') plt.plot(tf_history_dp.history['val_loss'], label='Validation Loss with Dp', linestyle='--') plt.legend() plt.subplot(1,2,2) plt.plot(tf_history.history['acc'], label='Training Accuracy') plt.plot(tf_history.history['val_acc'], label='Validation Accuracy') plt.plot(tf_history_dp.history['acc'], label='Training Accuracy with Dp') plt.plot(tf_history_dp.history['val_acc'], label='Validation Accuracy with Dp', linestyle='--') plt.legend() plt.show()  ","date":1567724400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567724400,"objectID":"9b94112f21c8c8b5b58367421ec615f7","permalink":"/courses/deeplearning/2.6/","publishdate":"2019-09-06T00:00:00+01:00","relpermalink":"/courses/deeplearning/2.6/","section":"courses","summary":"Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nMNIST Dataset The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\nLet\u0026rsquo;s get the dataset using tf.keras.datasets\nDownload MNIST import tensorflow as tf (x_train, y_train), (x_test, y_test) = tf.","tags":null,"title":"Overfitting \u0026 Regularization","type":"docs"},{"authors":null,"categories":null,"content":" \n Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nWe will use ANNs to diagnose Breast Cancer with some characteristics of the cell nuclei.\nDataset Download the Dataset We will use a breast cancer diagnosis dataset from Opeml.org\n%%capture !wget https://www.openml.org/data/get_csv/5600/BNG_breast-w.arff  Explore the Dataset import pandas as pd df = pd.read_csv('/content/BNG_breast-w.arff') df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Clump_Thickness Cell_Size_Uniformity Cell_Shape_Uniformity Marginal_Adhesion Single_Epi_Cell_Size Bare_Nuclei Bland_Chromatin Normal_Nucleoli Mitoses Class     0 7.581819 9.745087 1.000000 4.503410 7.039930 10.0 4.412282 10.000000 5.055266 malignant   1 5.210921 8.169596 7.841875 6.033275 4.269619 10.0 4.236312 4.845350 1.000000 malignant   2 4.000000 4.594296 2.330380 2.000000 3.000000 1.0 10.701823 1.101305 1.000000 benign   3 2.428871 1.000000 1.000000 1.000000 4.099291 1.0 2.000000 1.000000 1.000000 benign   4 8.855971 2.697539 6.047068 3.301891 3.000000 1.0 5.297592 4.104791 3.115741 malignant      You can see all the features are real numbers, with different range, so they need to be scaled. Class has to be changes to number {0, 1}.  Label Encoder We can use sklearn\u0026rsquo;s Label Encoder to change malignant and benign to {0, 1}.\nfrom sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() df.loc[:, 'Class'] = label_encoder.fit_transform(df.loc[:, 'Class']) df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Clump_Thickness Cell_Size_Uniformity Cell_Shape_Uniformity Marginal_Adhesion Single_Epi_Cell_Size Bare_Nuclei Bland_Chromatin Normal_Nucleoli Mitoses Class     0 7.581819 9.745087 1.000000 4.503410 7.039930 10.0 4.412282 10.000000 5.055266 1   1 5.210921 8.169596 7.841875 6.033275 4.269619 10.0 4.236312 4.845350 1.000000 1   2 4.000000 4.594296 2.330380 2.000000 3.000000 1.0 10.701823 1.101305 1.000000 0   3 2.428871 1.000000 1.000000 1.000000 4.099291 1.0 2.000000 1.000000 1.000000 0   4 8.855971 2.697539 6.047068 3.301891 3.000000 1.0 5.297592 4.104791 3.115741 1     Scaling Features We will use sklearn\u0026rsquo;s MinMaxScaler to scale the features, it will convert each column into a range of [0,1], you can also specify in which range you want to convert the features, by default its [0,1].\nfrom sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() df.loc[:, df.columns != 'Class'] = scaler.fit_transform(df.loc[:, df.columns != 'Class']) df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Clump_Thickness Cell_Size_Uniformity Cell_Shape_Uniformity Marginal_Adhesion Single_Epi_Cell_Size Bare_Nuclei Bland_Chromatin Normal_Nucleoli Mitoses Class     0 0.527352 0.885428 0.000000 0.344875 0.450241 0.761964 0.310056 0.929549 0.367161 1   1 0.344729 0.733487 0.589599 0.495474 0.243731 0.761964 0.294066 0.411081 0.000000 1   2 0.251456 0.388683 0.114646 0.098440 0.149088 0.084182 0.881553 0.034496 0.000000 0   3 0.130438 0.042047 0.000000 0.000000 0.231034 0.084182 0.090865 0.024306 0.000000 0   4 0.625495 0.205758 0.434931 0.226597 0.149088 0.084182 0.390499 0.336594 0.191558 1     Dataframes to Arrays X = df.loc[:, df.columns != 'Class'].values y = df.loc[:, 'Class'].values print(X.shape, y.shape)  (39366, 9) (39366,)  Train-Validation Split from sklearn.model_selection import train_test_split X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True) print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)  (31492, 9) (7874, 9) (31492,) (7874,)  Model import tensorflow as tf from tensorflow import keras tf.keras.backend.clear_session() model = tf.keras.Sequential([ tf.keras.layers.Dense(units=50, input_shape=[9], kernel_regularizer=tf.keras.regularizers.l2(0.00001)), tf.keras.layers.Activation('relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(units=50, kernel_regularizer=tf.keras.regularizers.l2(0.00001)), tf.keras.layers.Activation('relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(units=1), tf.keras.layers.Activation('sigmoid') ]) model.summary()  Model: \u0026quot;sequential\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 50) 500 _________________________________________________________________ activation (Activation) (None, 50) 0 _________________________________________________________________ dropout (Dropout) (None, 50) 0 _________________________________________________________________ dense_1 (Dense) (None, 50) 2550 _________________________________________________________________ activation_1 (Activation) (None, 50) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 50) 0 _________________________________________________________________ dense_2 (Dense) (None, 1) 51 _________________________________________________________________ activation_2 (Activation) (None, 1) 0 ================================================================= Total params: 3,101 Trainable params: 3,101 Non-trainable params: 0 _________________________________________________________________  Training optimizer = tf.keras.optimizers.Adam(lr=0.001) model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) tf_history_dp = model.fit(X_train, y_train, batch_size=50, epochs=100, verbose=True, validation_data=(X_val, y_val))  Train on 31492 samples, validate on 7874 samples Epoch 1/100 31492/31492 [==============================] - 3s 104us/sample - loss: 0.1383 - acc: 0.9645 - val_loss: 0.0470 - val_acc: 0.9830 Epoch 2/100 31492/31492 [==============================] - 3s 101us/sample - loss: 0.0569 - acc: 0.9796 - val_loss: 0.0431 - val_acc: 0.9850 . . Epoch 99/100 31492/31492 [==============================] - 3s 97us/sample - loss: 0.0402 - acc: 0.9857 - val_loss: 0.0356 - val_acc: 0.9892 Epoch 100/100 31492/31492 [==============================] - 3s 92us/sample - loss: 0.0408 - acc: 0.9858 - val_loss: 0.0345 - val_acc: 0.9886  import matplotlib.pyplot as plt plt.figure(figsize=(20,7)) plt.subplot(1,2,1) plt.plot(tf_history_dp.history['loss'], label='Training Loss') plt.plot(tf_history_dp.history['val_loss'], label='Validation Loss') plt.legend() plt.subplot(1,2,2) plt.plot(tf_history_dp.history['acc'], label='Training Accuracy') plt.plot(tf_history_dp.history['val_acc'], label='Validation Accuracy') plt.legend() plt.show()  We were able to get an accuracy of 98.8% on Validation set, but in these kind of medical diagnosis tasks even a 0.1% improvement is very important.\nTry to improve the performance more.\n","date":1567724400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567724400,"objectID":"6f47eaa9ee1327df59e4557e3e365512","permalink":"/courses/deeplearning/2.7/","publishdate":"2019-09-06T00:00:00+01:00","relpermalink":"/courses/deeplearning/2.7/","section":"courses","summary":"Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nWe will use ANNs to diagnose Breast Cancer with some characteristics of the cell nuclei.\nDataset Download the Dataset We will use a breast cancer diagnosis dataset from Opeml.org\n%%capture !wget https://www.openml.org/data/get_csv/5600/BNG_breast-w.arff  Explore the Dataset import pandas as pd df = pd.read_csv('/content/BNG_breast-w.arff') df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .","tags":null,"title":"ANN - Medical Diagnosis","type":"docs"},{"authors":null,"categories":null,"content":" \n Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nWe will use ANNs for a basic computer vision application of image classification on CIFAR10 Dataset\nDataset CIFAR-10 The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\nDownload the Dataset Tensorflow has inbuilt dataset which makes it easy to get training and testing data.\nfrom tensorflow.keras.datasets import cifar10 import tensorflow as tf (x_train, y_train), (x_test, y_test) = cifar10.load_data()  class_name = { 0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck', }  Visualize the Dataset import matplotlib.pyplot as plt num_imgs = 10 plt.figure(figsize=(num_imgs*2,3)) for i in range(1,num_imgs): plt.subplot(1,num_imgs,i).set_title('{}'.format(class_name[y_train[i][0]])) plt.imshow(x_train[i]) plt.axis('off') plt.show()  Scaling Features import numpy as np np.max(x_train), np.min(x_train)  (255, 0)  The range of values in the data is 0-255, - we can scale it to [0,1], dividing it by 255 will do. - or we can standardize it by subtracting the mean and dividing std.\nmean = np.mean(x_train) std = np.std(x_train) x_train = (x_train-mean)/std x_test = (x_test-mean)/std np.max(x_train), np.min(x_train), np.max(x_test), np.min(x_test)  (2.09341038199596, -1.8816433721538972, 2.09341038199596, -1.8816433721538972)  Labels to One-Hot print(y_train[:5])  [[6] [9] [9] [4] [1]]  num_classes = 10 y_train = tf.keras.utils.to_categorical(y_train, num_classes) y_test = tf.keras.utils.to_categorical(y_test, num_classes) print(y_train[:5])  [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]  Colour Channels Let\u0026rsquo;s check the shape of the arrays\nx_train.shape, y_train.shape, x_test.shape, y_test.shape  ((50000, 32, 32, 3), (50000, 10), (10000, 32, 32, 3), (10000, 10))  The shape of each image is (32 x 32 x 3), previously we saw MNIST dataset had a shape of (28 x 28), why is it so?\nMNIST is a gray-scale image, it has a single channel of gray scale. But CIFAR-10 is a colour image, every colour pixel has 3 channels RGB, all these 3 channel contribute to what colour you see.\nAny colour image is made of 3 channels RGB.\nVisualize colour channels import matplotlib.pyplot as plt plt.figure(figsize=(9,3)) plt.subplot(1,3,1) plt.imshow(x_train[1][:,:,0], cmap='Reds') plt.subplot(1,3,2) plt.imshow(x_train[1][:,:,1], cmap='Greens') plt.subplot(1,3,3) plt.imshow(x_train[1][:,:,2], cmap='Blues') plt.show()  So when we flatten the CIFAR-10 image, it will give 32x32x3 = 3072.\nModel import tensorflow as tf from tensorflow import keras tf.keras.backend.clear_session() input_shape = (32,32,3) # 3072 nclasses = 10 model = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape=input_shape), tf.keras.layers.Dense(units=1024), tf.keras.layers.Activation('tanh'), tf.keras.layers.Dropout(0.5), tf.keras.layers.Dense(units=512), tf.keras.layers.Activation('tanh'), tf.keras.layers.Dropout(0.5), tf.keras.layers.Dense(units=nclasses), tf.keras.layers.Activation('softmax') ]) model.summary()  Model: \u0026quot;sequential\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 3072) 0 _________________________________________________________________ dense (Dense) (None, 1024) 3146752 _________________________________________________________________ activation (Activation) (None, 1024) 0 _________________________________________________________________ dropout (Dropout) (None, 1024) 0 _________________________________________________________________ dense_1 (Dense) (None, 512) 524800 _________________________________________________________________ activation_1 (Activation) (None, 512) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 512) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 5130 _________________________________________________________________ activation_2 (Activation) (None, 10) 0 ================================================================= Total params: 3,676,682 Trainable params: 3,676,682 Non-trainable params: 0 _________________________________________________________________  Training optimizer = tf.keras.optimizers.Adam(lr=0.001) model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) tf_history_dp = model.fit(x_train, y_train, batch_size=500, epochs=100, verbose=True, validation_data=(x_test, y_test))  Train on 50000 samples, validate on 10000 samples Epoch 1/100 50000/50000 [==============================] - 3s 57us/sample - loss: 2.2985 - acc: 0.2712 - val_loss: 1.7830 - val_acc: 0.3898 Epoch 2/100 50000/50000 [==============================] - 3s 53us/sample - loss: 1.9610 - acc: 0.3329 - val_loss: 1.7066 - val_acc: 0.4134 . . Epoch 99/100 50000/50000 [==============================] - 3s 51us/sample - loss: 1.1897 - acc: 0.5877 - val_loss: 1.4037 - val_acc: 0.5220 Epoch 100/100 50000/50000 [==============================] - 3s 51us/sample - loss: 1.1839 - acc: 0.5884 - val_loss: 1.3991 - val_acc: 0.5227  import matplotlib.pyplot as plt plt.figure(figsize=(20,7)) plt.subplot(1,2,1) plt.plot(tf_history_dp.history['loss'], label='Training Loss') plt.plot(tf_history_dp.history['val_loss'], label='Validation Loss') plt.legend() plt.subplot(1,2,2) plt.plot(tf_history_dp.history['acc'], label='Training Accuracy') plt.plot(tf_history_dp.history['val_acc'], label='Validation Accuracy') plt.legend() plt.show()  Model is clearly overfitting.\nImage Augmentation We have discussed that, more images/data improves the model performance and avoid overfitting. But it\u0026rsquo;s not always possible to get new data, so we can augment the old data to create new data.\nAugmentation can be: - random crop - rotation - horizontal and vertical flips - x-y shift - colour jitter - etc.\nImage Augmentation in Tensorflow from tensorflow.keras.datasets import cifar10 import tensorflow as tf (x_train, y_train), (x_test, y_test) = cifar10.load_data() x_train = (x_train-mean)/std x_test = (x_test-mean)/std num_classes = 10 y_train = tf.keras.utils.to_categorical(y_train, num_classes) y_test = tf.keras.utils.to_categorical(y_test, num_classes)  import tensorflow as tf from tensorflow import keras tf.keras.backend.clear_session() input_shape = (32,32,3) # 3072 nclasses = 10 model = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape=input_shape), tf.keras.layers.Dense(units=1024), tf.keras.layers.Activation('tanh'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(units=512), tf.keras.layers.Activation('tanh'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(units=nclasses), tf.keras.layers.Activation('softmax') ]) model.summary()  Model: \u0026quot;sequential\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 3072) 0 _________________________________________________________________ dense (Dense) (None, 1024) 3146752 _________________________________________________________________ activation (Activation) (None, 1024) 0 _________________________________________________________________ dropout (Dropout) (None, 1024) 0 _________________________________________________________________ dense_1 (Dense) (None, 512) 524800 _________________________________________________________________ activation_1 (Activation) (None, 512) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 512) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 5130 _________________________________________________________________ activation_2 (Activation) (None, 10) 0 ================================================================= Total params: 3,676,682 Trainable params: 3,676,682 Non-trainable params: 0 _________________________________________________________________  from tensorflow.keras.preprocessing.image import ImageDataGenerator train_datagen = ImageDataGenerator( shear_range=0.1, zoom_range=0.1, horizontal_flip=True, rotation_range=20) test_datagen = ImageDataGenerator() train_generator = train_datagen.flow( x_train, y_train, batch_size=200) validation_generator = test_datagen.flow( x_test, y_test, batch_size=200)  import matplotlib.pyplot as plt i = 1 plt.figure(figsize=(20,2)) for x_batch, y_batch in train_datagen.flow(x_train, y_train, batch_size=1): plt.subplot(1,10,i) plt.imshow(x_batch[0]) i += 1 if i\u0026gt;10:break  You can see some of the images are zoomed, some are rotated\u0026hellip;etc. SO these images are now different that the original image and for the model these are new images.\noptimizer = tf.keras.optimizers.SGD(lr=0.001, momentum=0.9) model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) model.fit_generator( train_generator, steps_per_epoch=100, epochs=200, validation_data=validation_generator)  Epoch 1/200 100/100 [==============================] - 11s 113ms/step - loss: 2.1187 - acc: 0.2506 - val_loss: 1.8658 - val_acc: 0.3438 Epoch 2/200 100/100 [==============================] - 11s 105ms/step - loss: 1.9376 - acc: 0.3195 - val_loss: 1.8019 - val_acc: 0.3725 . . Epoch 199/200 100/100 [==============================] - 10s 102ms/step - loss: 1.4075 - acc: 0.5145 - val_loss: 1.3601 - val_acc: 0.5340 Epoch 200/200 100/100 [==============================] - 10s 104ms/step - loss: 1.4111 - acc: 0.5127 - val_loss: 1.3616 - val_acc: 0.5369 \u0026lt;tensorflow.python.keras.callbacks.History at 0x7f644a09bcc0\u0026gt;  import matplotlib.pyplot as plt tf_history_aug = model.history plt.figure(figsize=(20,7)) plt.subplot(1,2,1) plt.plot(tf_history_aug.history['loss'], label='Training Loss') plt.plot(tf_history_aug.history['val_loss'], label='Validation Loss') plt.legend() plt.subplot(1,2,2) plt.plot(tf_history_aug.history['acc'], label='Training Accuracy') plt.plot(tf_history_aug.history['val_acc'], label='Validation Accuracy') plt.legend() plt.show()  The model is not overfitting and the performance is still increasing, so training for more epoch can give a good performance, but it will take more time, so we will stop here. Try to improve the model.\n Train the model longer. Use different architecture with aug use different activation different optimizer  There are other model architectures which work good for images, we will discuss that in the intermediate track.\nSaving a Trained Model Saving a trained model is very important, hours of training should not be wasted and we need the trained model to be deployed in some other device. It\u0026rsquo;s very simple in tf.keras\nmodel_path = 'cifar10_trained_model.h5' model.save(model_path)  !ls  cifar10_trained_model.h5 sample_data  Loading a saved model from tensorflow.keras.models import load_model model = load_model(model_path) model.summary()  Model: \u0026quot;sequential\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 3072) 0 _________________________________________________________________ dense (Dense) (None, 1024) 3146752 _________________________________________________________________ activation (Activation) (None, 1024) 0 _________________________________________________________________ dropout (Dropout) (None, 1024) 0 _________________________________________________________________ dense_1 (Dense) (None, 512) 524800 _________________________________________________________________ activation_1 (Activation) (None, 512) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 512) 0 _________________________________________________________________ dense_2 (Dense) (None, 10) 5130 _________________________________________________________________ activation_2 (Activation) (None, 10) 0 ================================================================= Total params: 3,676,682 Trainable params: 3,676,682 Non-trainable params: 0 _________________________________________________________________  ","date":1568070000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568070000,"objectID":"43cf94b079c1eb7574ba1c8f45e433ad","permalink":"/courses/deeplearning/2.8/","publishdate":"2019-09-10T00:00:00+01:00","relpermalink":"/courses/deeplearning/2.8/","section":"courses","summary":"Open in GitHub\nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nWe will use ANNs for a basic computer vision application of image classification on CIFAR10 Dataset\nDataset CIFAR-10 The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\nDownload the Dataset Tensorflow has inbuilt dataset which makes it easy to get training and testing data.","tags":null,"title":"ANN - Computer Vision","type":"docs"},{"authors":null,"categories":null,"content":" \nDeep Learning - Beginners Track Instructor: Shangeth Rajaa \nSentiment Analysis We are going to classify a movie review as Positive or Negative review given a text review.\nWe\u0026rsquo;ll use the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.\nIMDB Dataset Download the Dataset import tensorflow as tf imdb = tf.keras.datasets.imdb (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)  Explore the Dataset train_data.shape, train_labels.shape, test_data.shape, test_labels.shape  ((25000,), (25000,), (25000,), (25000,))  len(train_data[0])  218  Each of these numbers correspond to a word.\nword_2_int = tf.keras.datasets.imdb.get_word_index(path='imdb_word_index.json') word_2_int['hello'], word_2_int['world']  (4822, 179)  len(word_2_int)  88584  def sentence_2_int(sentence): sentence_2_int_list = [] for i in sentence.lower().split(' '): sentence_2_int_list.append(word_2_int[i]) return sentence_2_int_list sentence = \u0026quot;Worst movie i've ever seen\u0026quot; sentence_2_int(sentence)  [246, 17, 204, 123, 107]  Preparing the data Padding Every sentence will be off different length, to pass the sentences through the ANN model, we need to have a fixed length data. So we - pad small sentences - cut very long sentences to a fixed length\npad_value = 0 sentence_len = 100 train_data = tf.keras.preprocessing.sequence.pad_sequences(train_data, value=0, padding='post', maxlen=sentence_len) test_data = tf.keras.preprocessing.sequence.pad_sequences(test_data, value=0, padding='post', maxlen=sentence_len) train_data.shape, train_labels.shape, test_data.shape, test_labels.shape  ((25000, 100), (25000,), (25000, 100), (25000,))  ANN Model We will use a new layer into our model called the Embedding layer.\nThe words of each sentence are now represented with numbers, we cannot directly feed those numbers into the model, we need to convert the numbers into vectors for the model to understand what word it is?\nbut how do we decide the vectors? we leave that to the model.\nSo embedding layer, takes in an integer and converts it into a vector.\nvocab_size = 10000 embedding_dim = 16 import tensorflow as tf from tensorflow import keras tf.keras.backend.clear_session() model = tf.keras.Sequential([ tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=sentence_len), # the model will take as input an integer matrix of size (batch, input_length). # now model.output_shape == (None, 100, 16), where None is the batch dimension. tf.keras.layers.Dropout(0.4), tf.keras.layers.Flatten(), tf.keras.layers.Dense(units=sentence_len*embedding_dim), tf.keras.layers.Activation('relu'), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(units=500), tf.keras.layers.Activation('relu'), tf.keras.layers.Dropout(0.4), tf.keras.layers.Dense(units=1), tf.keras.layers.Activation('sigmoid') ]) model.summary()  Model: \u0026quot;sequential\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 100, 16) 160000 _________________________________________________________________ dropout (Dropout) (None, 100, 16) 0 _________________________________________________________________ flatten (Flatten) (None, 1600) 0 _________________________________________________________________ dense (Dense) (None, 1600) 2561600 _________________________________________________________________ activation (Activation) (None, 1600) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 1600) 0 _________________________________________________________________ dense_1 (Dense) (None, 500) 800500 _________________________________________________________________ activation_1 (Activation) (None, 500) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 500) 0 _________________________________________________________________ dense_2 (Dense) (None, 1) 501 _________________________________________________________________ activation_2 (Activation) (None, 1) 0 ================================================================= Total params: 3,522,601 Trainable params: 3,522,601 Non-trainable params: 0 _________________________________________________________________  optimizer = tf.keras.optimizers.Adam(lr=0.001) model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) tf_history = model.fit(train_data, train_labels, batch_size=2000, epochs=10, verbose=True, validation_data=(test_data, test_labels))  Train on 25000 samples, validate on 25000 samples Epoch 1/10 25000/25000 [==============================] - 12s 489us/sample - loss: 0.6917 - acc: 0.5162 - val_loss: 0.6860 - val_acc: 0.5558 Epoch 2/10 25000/25000 [==============================] - 12s 485us/sample - loss: 0.6434 - acc: 0.6450 - val_loss: 0.5741 - val_acc: 0.7036 Epoch 3/10 25000/25000 [==============================] - 12s 482us/sample - loss: 0.4729 - acc: 0.7753 - val_loss: 0.4278 - val_acc: 0.8019 Epoch 4/10 25000/25000 [==============================] - 12s 487us/sample - loss: 0.3406 - acc: 0.8524 - val_loss: 0.3833 - val_acc: 0.8292 Epoch 5/10 25000/25000 [==============================] - 12s 483us/sample - loss: 0.2646 - acc: 0.8873 - val_loss: 0.3691 - val_acc: 0.8388 Epoch 6/10 25000/25000 [==============================] - 12s 482us/sample - loss: 0.2038 - acc: 0.9180 - val_loss: 0.3913 - val_acc: 0.8380 Epoch 7/10 25000/25000 [==============================] - 12s 484us/sample - loss: 0.1641 - acc: 0.9350 - val_loss: 0.4083 - val_acc: 0.8380 Epoch 8/10 25000/25000 [==============================] - 12s 483us/sample - loss: 0.1318 - acc: 0.9490 - val_loss: 0.4307 - val_acc: 0.8374 Epoch 9/10 25000/25000 [==============================] - 12s 484us/sample - loss: 0.1029 - acc: 0.9595 - val_loss: 0.4761 - val_acc: 0.8348 Epoch 10/10 25000/25000 [==============================] - 12s 485us/sample - loss: 0.0871 - acc: 0.9660 - val_loss: 0.4966 - val_acc: 0.8368  model.save('trained_model.h5')  Model Pipeline Load trained Model trained_model = tf.keras.models.load_model('trained_model.h5') trained_model.summary()  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor Model: \u0026quot;sequential\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 100, 16) 160000 _________________________________________________________________ dropout (Dropout) (None, 100, 16) 0 _________________________________________________________________ flatten (Flatten) (None, 1600) 0 _________________________________________________________________ dense (Dense) (None, 1600) 2561600 _________________________________________________________________ activation (Activation) (None, 1600) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 1600) 0 _________________________________________________________________ dense_1 (Dense) (None, 500) 800500 _________________________________________________________________ activation_1 (Activation) (None, 500) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 500) 0 _________________________________________________________________ dense_2 (Dense) (None, 1) 501 _________________________________________________________________ activation_2 (Activation) (None, 1) 0 ================================================================= Total params: 3,522,601 Trainable params: 3,522,601 Non-trainable params: 0 _________________________________________________________________  Sentence to Vectors import numpy as np def sentence_2_int(sentence): sentence_2_int_list = [] for i in sentence.lower().split(' '): sentence_2_int_list.append(word_2_int[i]) arr = np.array(sentence_2_int_list).reshape(1,-1) arr = tf.keras.preprocessing.sequence.pad_sequences(arr, value=0, padding='post', maxlen=100) return arr sentence = \u0026quot;Worst movie i've ever seen\u0026quot; sentence_2_int(sentence)  array([[246, 17, 204, 123, 107, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)  model.predict(sentence_2_int(sentence))  array([[0.22403198]], dtype=float32)  def sentence_2_prediction(sentence): vector = sentence_2_int(sentence) prob = model.predict(vector) prediction = prob \u0026gt; 0.5 if prediction == 1: print('Positive Review :D') else: print('Negative Review :(')  sentence = 'Good Movie i really enjoyed it' sentence_2_prediction(sentence)  Positive Review :D  sentence = 'worst movie' sentence_2_prediction(sentence)  Negative Review :(  If you try many different sentences, you may notice the model actually doesn\u0026rsquo;t perform well. There may be many reasons for it.\n vocabulary size sequence length(no of words in a sentence for padding) model architecture embedding dim  Train the model by changing all the above to improve its performance.\n","date":1568242800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568242800,"objectID":"01eee9253c7ff27f07bc869a2b11830c","permalink":"/courses/deeplearning/2.9/","publishdate":"2019-09-12T00:00:00+01:00","relpermalink":"/courses/deeplearning/2.9/","section":"courses","summary":"Deep Learning - Beginners Track Instructor: Shangeth Rajaa \nSentiment Analysis We are going to classify a movie review as Positive or Negative review given a text review.\nWe\u0026rsquo;ll use the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.","tags":null,"title":"ANN - Natural Language Processing","type":"docs"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Python","Information Theory"],"content":" Colab Notebook\nBefore seeing KL Divergence, let\u0026rsquo;s see a very simple concept called Entropy\nEntropy Entropy is the expected information contained in a Distribution. It measures the uncertainty.\n$H(x) = \\sum{p(x)I(x)}$ where $I(x)$ is called the Information content of $x$.\n\u0026ldquo;If an event is very probable, it is no surprise (and generally uninteresting) when that event happens as expected. However, if an event is unlikely to occur, it has much more information to learn that the event happened or will happen. For instance, the knowledge that some particular number will not be the winning number of a lottery provides very little information, because any particular chosen number will almost certainly not win. However, the knowledge that a particular number will win a lottery has a high value because it communicates the outcome of a very low probability event. \u0026ldquo;\n\nSo the information content is an increasing function of inverse of its probability. More the probability, lesser the information content and vice versa. ${\\bf I(x) = log_2(\\dfrac{1}{p(x)}) = -log_2(p(x))}$\nTherefore, ${\\bf H(x) = \\sum{p(x)I(x)} = -\\sum{p(x)log_2(p(x))}}$ and for continuous case ${\\bf H(x) = -\\int{p(x)log_2(p(x))}}$.\nEntropy in Pytorch import torch def entropy(p, epsilon=1e-8): ''' epsilon to avoid log(0) error ''' logp = torch.log2(p + epsilon) e = torch.sum(-p*logp) return e # probability of choosing [a, b, c, d] p1 = torch.Tensor([0.1, 0.2, 0.4, 0.3]) p2 = torch.Tensor([0.9, 0.0, 0.0, 0.1]) print('Entropy of Distribution {} = {}'.format(p1.numpy(), entropy(p1))) print('Entropy of Distribution {} = {}'.format(p2.numpy(), entropy(p2)))  Entropy of Distribution [0.1 0.2 0.4 0.3] = 1.846439242362976 Entropy of Distribution [0.9 0. 0. 0.1] = 0.4689956307411194  The first distribution has more entropy(uncertainty) as when we sample a data, there is a fair chance of the sampled data is from any of a, b, c, d.\nBut the second distribution is almost deterministic and has less uncertainty.\nKullback-Leibler Divergence KL Divergence measures the similarity of two probability distribution. Let $P$ and $Q$ be two probability distributions. For ex: Let $P$ be a Gaussian Distribution and $Q$ be Uniform.\nThe Kullback-Leibler Divergence is\n${\\bf D_{KL}(p(x)||q(x)) = \\int_{-\\infty}^{+\\infty} p(x)ln{\\dfrac{p(x)}{q(x)}}}$\n\nFor discrete distribution,\n${\\bf D_{KL}(p(x)||q(x)) = \\sum p(x)ln{\\dfrac{p(x)}{q(x)}}}$\nProperties of KL Divergence  KLdivergence is not a distance measure or \u0026ldquo;metric\u0026rdquo; measure. It is not symmetric ie: $D_{KL}(p(x)||q(x)) \\neq D_{KL}(q(x)||p(x))$. It need not satisfy the triangular inequality. It is a non-negative measure ie:$D_{KL}(p(x)||q(x)) \\geq 0$ and $D_{KL}(p(x)||q(x)) = 0$ if and only if $p(x)=q(x)$.  KL Divergence in PyTorch import torch def kl_divergence(p, q, epsilon=1e-8): ''' epsilon to avoid log(0) or divided by 0 error ''' d_kl = (p * torch.log((p / (q+ epsilon)) + epsilon)).sum() return d_kl # comparison of kl of similar distribution p2-p3 # and dissimilar distribution p1-p2 p1 = torch.Tensor([0.1, 0.2, 0.4, 0.3]) p2 = torch.Tensor([0.9, 0.0, 0.0, 0.1]) p3 = torch.Tensor([0.8, 0.0, 0.05, 0.15]) print('KL Divergence of {} and {} = {}'.format(p1.numpy(), p1.numpy(), kl_divergence(p1, p2))) print('KL Divergence of {} and {} = {}'.format(p2.numpy(), p3.numpy(), kl_divergence(p2, p3)))  KL Divergence of [0.1 0.2 0.4 0.3] and [0.1 0.2 0.4 0.3] = 10.47386646270752 KL Divergence of [0.9 0. 0. 0.1] and [0.8 0. 0.05 0.15] = 0.06545820087194443  Mutual Information with KL Divergence Let random variable $x$ and $y$ has distribution of $p(x)$ and $p(y)$. If $x$ and $y$ are independent, then $p(x,y) = p(x)p(y)$.\nIf the distance between $p(x,y)$ and $p(x)p(y)$ becomes larger, $p(x)$ and $p(y)$ becomes dependent. We can use KL Divergence of $p(x,y)$ and $p(x)p(y)$ to measure the dependency of x and y.\n${\\bf I(x, y) = D_{KL}(p(x,y)||p(x)p(y)) = \\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} p(x, y)ln{\\dfrac{p(x, y)}{p(x)p(y)}}}$\nCross Entropy The Kullback-Leibler Divergence of $p(x)$ and $q(x)$ is given by\n$D_{KL}(p(x)||q(x)) = \\int_{-\\infty}^{+\\infty} p(x)ln{\\dfrac{p(x)}{q(x)}}$\n$\\qquad = \\int_{-\\infty}^{+\\infty} p(x)ln(p(x))-\\int_{-\\infty}^{+\\infty}p(x)ln(q(x))$\n$D_{KL}(p(x)||q(x)) = - H(p(x)) -\\int_{-\\infty}^{+\\infty}p(x)ln(q(x))$\n$D_{KL}(p(x)||q(x)) + H(p(x)) = -\\int_{-\\infty}^{+\\infty}p(x)ln(q(x)) = H(p(x), q(x))$\n\n$H(p(x), q(x))$ is called the Cross-Entropy. When $p(x)$ and $q(x)$ are same, then the Cross Entropy is equal to the entropy as $D_{KL}(p(x)||q(x))=0$.\nThe amount by which the cross entropy exceeds the entropy is called relative entropy or KL Divergence.\n${\\bf Cross\\ Entropy = KL\\ Divergence + Entropy }$\nCross Entropy in PyTorch import torch def cross_entropy(p, q, epsilon=1e-8): ''' epsilon to avoid log(0) error ''' logq = torch.log2(q + epsilon) ce = torch.sum(-p*logq) return ce # comparison of cross_entropy of similar distribution p2-p3 # and dissimilar distribution p1-p2 p1 = torch.Tensor([0.1, 0.2, 0.4, 0.3]) p2 = torch.Tensor([0.9, 0.0, 0.0, 0.1]) p3 = torch.Tensor([0.8, 0.0, 0.05, 0.15]) print('Cross Entropy of {} and {} = {}'.format(p1.numpy(), p2.numpy(), cross_entropy(p1, p2))) print('Cross Entropy of {} and {} = {}'.format(p2.numpy(), p3.numpy(), cross_entropy(p2, p3)))  Cross Entropy of [0.1 0.2 0.4 0.3] and [0.9 0. 0. 0.1] = 16.957033157348633 Cross Entropy of [0.9 0. 0. 0.1] and [0.8 0. 0.05 0.15] = 0.5634317994117737  Reference  Elements of Information Theory Thomas M. Cover, Joy A. Thomas https://web.stanford.edu/~montanar/RESEARCH/BOOK/partA.pdf https://en.wikipedia.org/wiki/Cross_entropy  ","date":1580004000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580004000,"objectID":"934b2f10496230107ac40fd9e0b931cd","permalink":"/post/kl-divergence/","publishdate":"2020-01-26T02:00:00Z","relpermalink":"/post/kl-divergence/","section":"post","summary":"Colab Notebook\nBefore seeing KL Divergence, let\u0026rsquo;s see a very simple concept called Entropy\nEntropy Entropy is the expected information contained in a Distribution. It measures the uncertainty.\n$H(x) = \\sum{p(x)I(x)}$ where $I(x)$ is called the Information content of $x$.\n\u0026ldquo;If an event is very probable, it is no surprise (and generally uninteresting) when that event happens as expected. However, if an event is unlikely to occur, it has much more information to learn that the event happened or will happen.","tags":["Information Theory","Python"],"title":"KL Divergence","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Python","OpenAI Gym"],"content":" Off-Policy Monte Carlo with Importance Sampling Off Policy Learning Link to the Notebook By exploration-exploitation trade-off, the agent should take sub-optimal exploratory action by which the agent may receive less reward. One way of exploration is by using an epsilon-greedy policy, where the agent takes a nongreedy action with a small probability.\nIn an on-policy, improvement and evaluation are done on the policy which is used to select actions.\nIn off-policy, improvement and evaluation are done on a different policy from the one used to select actions. the policy learned is off the policy used for action selection while gathering episodes.\n Target Policy $\\pi(a/s)$ : The value function of learning is based on $\\pi(a/s)$. We want the target policy to be the optimal policy $\\pi^{*}(a/s)$. The target policy will be used for action selection after the learning process is complete(deployment). Behavior Policy $b(a/s)$: Behavior policy is used for action selection while gathering episodes to train the agent. This generally follows an exploratory policy.  Importance Sampling We have a random variable $x \\sim b$ sampled from behavior policy distribution $b$. We want to estimate the expected value of $x$ wrt the target distribution $\\pi$ ie: $E_{\\pi}[X]$. The sample average will give the expected value of x under b $E_{b}[X]$.\n$E_{\\pi}[X] = \\sum x \\pi(x)$ where $x \\in X$\n$ \\qquad = \\sum x \\pi(x) \\dfrac{b(x)}{b(x)}$\n$ \\qquad = \\sum x \\dfrac{\\pi(x)}{b(x)} b(x)$\n$ \\qquad = \\sum x \\rho(x) b(x)$\nwhere $\\rho(x) = \\dfrac{\\pi(x)}{b(x)}$ and is called the importance sampling ratio.\n\nLet $x\\rho(x)$ be a new random variable $X_\\rho(X)$. Then, $E_{\\pi}[X] = \\sum x \\rho(x) b(x) = E_{b}[X_{\\rho}(X)]$. Now we have expectation under $b$ instead of $\\pi$.\n\nHow do we estimating expectation from the data? Compute the weighted sample average with importance sampling ratio as the weights. $E_{\\pi}[X] = \\sum x \\rho(x) b(x) = E_{b}[X_{\\rho}(X)] \\approx \\dfrac{1}{n}\\sum_{i=1}^{n}x_i \\rho(x_i)$ and $x_i \\sim b$.\nOff Policy Monte Carlo Prediction with Importance Sampling In Monte Carlo prediction, we estimate the value of each state by computing a sample average over returns starting from that state. $V_{\\pi}(s) = E_{\\pi}[G_t|S_t=s]$\n\nIn off-policy, we are trying to estimate value under the target policy $\\pi(s)$ using returns following the behavior policy $b(s)$. As discussed above, we need to find the value of $\\rho$ for each of the sampled returns. $\\rho$ is the probability of trajectory under $\\pi$ divided by the probability of trajectory under $b$.\n$V_{\\pi}(s) = E_{b}[\\rho G_t|S_t=s]$ ; $\\rho = \\dfrac{P(trajectory\\ under\\ \\pi)}{P(trajectory\\ under\\ b)}$\n\nThe probaility of a trajectory under a policy can be given by $P(trajectory\\ under\\ policy) = P(A_t, S_{t+1}, A_{t+1}, \u0026hellip;, S_T| S_t, A_{t:T})$\nwhere all actions are taken under the policy $b$.\n\nAs this is a Markov process, we can split the probability terms into $P(A_t, S_{t+1}, A_{t+1}, \u0026hellip;, S_T|S_t, A_{t:T}) = b(A_t|S_t)p(S_{t+1}|S_t, A_t)\\ b(A_{t+1}|S_{t+1}) p(S_{t+2}|S_{t+1}, A_{t+1}) \u0026hellip; \\ p(S_T|S_{T-1}, A_{T-1})$\n\n$b(A_t|S_t)p(S_{t+1}|S_t, A_t)$ give the probability of action $A_t$ at state $S_t$ times the probability of the state transition to state $S_{t+1}$\n$P(A_t, S_{t+1}, A_{t+1}, \u0026hellip;, S_T| S_t, A_{t:T}) = \\prod_{k=1}^{T} b(A_k|S_k)p(S_{k+1}|S_k, A_k)$\n$\\rho = \\dfrac{P(trajectory\\ under\\ \\pi)}{P(trajectory\\ under\\ b)}$\n$\\quad = \\prod_{k=t}^{T-1}\\dfrac{\\pi(A_k|S_k)p(S_{k+1}|S_k, A_k)}{b(A_k|S_k)p(S_{k+1}|S_k, A_k)}$\n$\\rho = \\prod_{k=t}^{T-1}\\dfrac{\\pi(A_k|S_k)}{b(A_k|S_k)}$\nIncremental Implementation of Off-policy MC Suppose we have a sequence of returns $G_1, G_2, \u0026hellip;G_{n-1}$ all starting in the same state and with corresponding weights $W_i$, then the weighted average of returns $V_n = \\dfrac{\\sum_{k=1}^{n-1} W_kG_k}{\\sum_{k=1}^{n-1} W_k}$ , $n \\geq 2$ and the cumulative sum of weights $C_{n+1} = C_n + W_{n+1}$ where $C_0 = 0$.\nThe update rule for $V_{n+1} = V_n + \\dfrac{W_n}{C_n}[G_n-V_n]$, $n \\geq 1$ and $V_1$ is arbitrary.\nHere $W_n$ will be the importance sampling weight.\nGreedy $\\pi$ policy When the target policy $\\pi$ is $\\epsilon$ greedy, it is determenistic and $\\pi(A_t/S_t) = 1$ . So $\\rho = \\prod_{k=t}^{T-1}\\dfrac{1}{b(A_k|S_k)}$ and $W$ can be updated with $W \\leftarrow W \\dfrac{1}{b(A_t|S_t)}$ for each time step in the trajectory.\nSo the Off policy Monte Carlo Control algorithm is Off-Policy MC Control with Weighted Importance in Python BlackJack Environment import sys import gym import numpy as np from collections import defaultdict  env = gym.make('Blackjack-v0') print(vars(env), end='\\n\\n') print(dir(env))  {'action_space': Discrete(2), 'observation_space': Tuple(Discrete(32), Discrete(11), Discrete(2)), 'np_random': RandomState(MT19937) at 0x7FE1148D2888, 'natural': False, 'dealer': [10, 1], 'player': [10, 1], 'spec': EnvSpec(Blackjack-v0)} ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_get_obs', 'action_space', 'close', 'dealer', 'metadata', 'natural', 'np_random', 'observation_space', 'player', 'render', 'reset', 'reward_range', 'seed', 'spec', 'step', 'unwrapped']  print(env.observation_space) print(env.action_space)  Tuple(Discrete(32), Discrete(11), Discrete(2)) Discrete(2)  random_state = env.reset() print('Random State', random_state) random_action = env.action_space.sample() print('Random Action', random_action)  Random State (18, 2, False) Random Action 1  Generate Random Eposides num_episodes = 3 for i in range(num_episodes): print('Episode : ', i+1) state = env.reset() step = 0 while True: step +=1 action = env.action_space.sample() print('Step = {}\\t State = {}\\t Action Taken = {}'.format(step, state, action)) state, reward, done, info = env.step(action) if done: print('Game Ended...') if reward \u0026gt; 0: print('Agent Won!\\n') else: print('Agent Lost!\\n') break  Episode : 1 Step = 1 State = (10, 7, False) Action Taken = 1 Step = 2 State = (21, 7, True) Action Taken = 1 Step = 3 State = (15, 7, False) Action Taken = 1 Step = 4 State = (18, 7, False) Action Taken = 0 Game Ended... Agent Lost! Episode : 2 Step = 1 State = (20, 8, False) Action Taken = 0 Game Ended... Agent Won! Episode : 3 Step = 1 State = (12, 7, False) Action Taken = 1 Step = 2 State = (15, 7, False) Action Taken = 0 Game Ended... Agent Won!  Training def random_policy(nA): A = np.ones(nA, dtype=float) / nA def policy_fn(observation): return A return policy_fn def greedy_policy(Q): def policy_fn(state): A = np.zeros_like(Q[state], dtype=float) best_action = np.argmax(Q[state]) A[best_action] = 1.0 return A return policy_fn def mc_off_policy(env, num_episodes, behavior_policy, max_time=100, discount_factor=1.0): Q = defaultdict(lambda:np.zeros(env.action_space.n)) C = defaultdict(lambda:np.zeros(env.action_space.n)) target_policy = greedy_policy(Q) for i_episode in range(1, num_episodes+1): if i_episode % 1000 == 0: print(\u0026quot;\\rEpisode {}/{}.\u0026quot;.format(i_episode, num_episodes), end=\u0026quot;\u0026quot;) sys.stdout.flush() episode = [] state = env.reset() for t in range(max_time): probs = behavior_policy(state) action = np.random.choice(np.arange(len(probs)), p=probs) next_state, reward, done, _ = env.step(action) episode.append((state, action, reward)) if done: break state = next_state G = 0.0 W = 1.0 for t in range(len(episode))[::-1]: state, action, reward = episode[t] G = discount_factor * G + reward C[state][action] += W Q[state][action] += (W / C[state][action]) * (G - Q[state][action]) if action != np.argmax(target_policy(state)): break W = W * 1./behavior_policy(state)[action] return Q, target_policy  random_policy = random_policy(env.action_space.n) Q, policy = mc_off_policy(env, num_episodes=500000, behavior_policy=random_policy)  Episode 500000/500000.  Plot import matplotlib.pyplot as plt from mpl_toolkits.axes_grid1 import make_axes_locatable def plot_policy(policy): def get_Z(x, y, usable_ace): if (x,y,usable_ace) in policy: return policy[x,y,usable_ace] else: return 1 def get_figure(usable_ace, ax): x_range = np.arange(11, 22) y_range = np.arange(10, 0, -1) X, Y = np.meshgrid(x_range, y_range) Z = np.array([[get_Z(x,y,usable_ace) for x in x_range] for y in y_range]) surf = ax.imshow(Z, cmap=plt.get_cmap('Pastel2', 2), vmin=0, vmax=1, extent=[10.5, 21.5, 0.5, 10.5]) plt.xticks(x_range) plt.yticks(y_range) plt.gca().invert_yaxis() ax.set_xlabel('Player\\'s Current Sum') ax.set_ylabel('Dealer\\'s Showing Card') ax.grid(color='w', linestyle='-', linewidth=1) divider = make_axes_locatable(ax) cax = divider.append_axes(\u0026quot;right\u0026quot;, size=\u0026quot;5%\u0026quot;, pad=0.1) cbar = plt.colorbar(surf, ticks=[0,1], cax=cax) cbar.ax.set_yticklabels(['0 (STICK)','1 (HIT)']) fig = plt.figure(figsize=(15, 15)) ax = fig.add_subplot(121) ax.set_title('Usable Ace') get_figure(True, ax) ax = fig.add_subplot(122) ax.set_title('No Usable Ace') get_figure(False, ax) plt.show()  policy = dict((k,np.argmax(v)) for k, v in Q.items()) plot_policy(policy)  ","date":1579399200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579399200,"objectID":"8865bcc143549087f8e3221ab3b84eae","permalink":"/post/off-policy-monte-carlo/","publishdate":"2020-01-19T02:00:00Z","relpermalink":"/post/off-policy-monte-carlo/","section":"post","summary":"Off-Policy Monte Carlo with Importance Sampling Off Policy Learning Link to the Notebook By exploration-exploitation trade-off, the agent should take sub-optimal exploratory action by which the agent may receive less reward. One way of exploration is by using an epsilon-greedy policy, where the agent takes a nongreedy action with a small probability.\nIn an on-policy, improvement and evaluation are done on the policy which is used to select actions.","tags":["Reinforcement Learning","Python"],"title":"Off Policy Monte Carlo Prediction with Importance sampling","type":"post"},{"authors":null,"categories":null,"content":"","date":1573689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573689600,"objectID":"18022b17a396083465776accd9779413","permalink":"/project/drl-navigator/","publishdate":"2019-11-14T00:00:00Z","relpermalink":"/project/drl-navigator/","section":"project","summary":"Deep Reinforcement Learning Agent for Navigator Environment","tags":["Deep Learning","Python","Reinforcement Learning","PyTorch"],"title":"DRL Navigator","type":"project"},{"authors":null,"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"26dca72a5b03dbfd3d8a67818dcd5ded","permalink":"/project/policy-based-rl/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/project/policy-based-rl/","section":"project","summary":"Policy based RL methods for Lunar lander and Cartpole environments.","tags":["Deep Learning","Python","Reinforcement Learning","PyTorch"],"title":"Policy Based Reinforcement Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":1571616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571616000,"objectID":"bb8a25f7044129f16179cf73f1ed8718","permalink":"/project/temporal-difference/","publishdate":"2019-10-21T00:00:00Z","relpermalink":"/project/temporal-difference/","section":"project","summary":"TD methods like SARSA(0), SARSAMax and Expected SARSA.","tags":["Deep Learning","Python","Reinforcement Learning","PyTorch"],"title":"Temporal Difference","type":"project"},{"authors":null,"categories":null,"content":"","date":1571529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571529600,"objectID":"9ad339d3cde09596c32ac4d91630a84b","permalink":"/project/monte-carlo/","publishdate":"2019-10-20T00:00:00Z","relpermalink":"/project/monte-carlo/","section":"project","summary":"MC method for BlackJack environment.","tags":["Deep Learning","Python","Reinforcement Learning","PyTorch"],"title":"Monte Carlo Prediction","type":"project"},{"authors":null,"categories":null,"content":"","date":1562528221,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562528221,"objectID":"835012fefc81d462e93431e390609db4","permalink":"/project/facial-emotion-recognition-pytorch/","publishdate":"2019-07-08T01:07:01+05:30","relpermalink":"/project/facial-emotion-recognition-pytorch/","section":"project","summary":"Recognizing the facial emotions with Deep learning model trained on PyTorch and deployed with TF.js model converted with ONNX.","tags":["Deep Learning","Computer Vision","Python","PyTorch","Tensorflow","JavaScript"],"title":"Facial Emotion Recognition PyTorch ONNX","type":"project"},{"authors":["Zhengying\tLiu","Zhen\tXu","Julio\tJacques Junior","Meysam\tMadadi","Sergio\tEscalera","Shangeth\tRajaa"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1560470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560470400,"objectID":"293b9a3850ef93d4b029d5635dfddd5c","permalink":"/publication/ads/","publishdate":"2019-06-14T00:00:00Z","relpermalink":"/publication/ads/","section":"publication","summary":"Framework and Mathematical formulation of AutoML","tags":["Source Themes"],"title":"Overview and unifying conceptualization of Automated Machine Learning","type":"publication"},{"authors":null,"categories":null,"content":"","date":1560470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560470400,"objectID":"ceba210875c59b1eaa91353e1e8ea687","permalink":"/project/pneumonia-diagnosis-with-deep-learning/","publishdate":"2019-06-14T00:00:00Z","relpermalink":"/project/pneumonia-diagnosis-with-deep-learning/","section":"project","summary":"Web Application for Diagnosis of Pnuemonia with deep learning model trained and backed with PyTorch framework.","tags":["Deep Learning","Python","Computer Vision","PyTorch","Flask"],"title":"Pneumonia Diagnosis with Deep Learning","type":"project"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Python","PyTorch"],"content":" As we have seen in GAN 1, GAN 2, GAN 3, GAN 4 that GANs have 2 network, the Generator G and the Discriminator D. Given a latent vector z, the G generates a new sample from the distribution of the training data. D classifies a data sample as real(from the training data) or fake(generated by G).\nIn the starting, the G generates a random data sample(as it didnt learn the data distribution) and the D is not a good classifier now. As the training process goes, the G starts learning the data distribution and D becomes a good classifier. D tries to classify all sampels generated by D as fake, G tries to generate samples such that D classifies that as real. In the process, both the networks become better and Generator learns the distribution of the data and can now generate realistic samples. D becomes good at classifying real/fake data samples. Conditional GAN Let us consider MNIST GAN, after we trained a MNIST dataset on a GAN model, the generator(G) can now generate some images which look alike of the MNIST numbers.\nBut what if we want the G to generate images of a specific digit?. The G which we trained generated images samples depending on the latent vector z. But we used a random z. So we cannot choose a map from random z - \u0026gt; Specific image.\nSo we introduce a conditional label y, such that for a condition label y the generator have to generate sample. Now the Generator learns the distribution of the dataset and generates samples based on the condition y or c(condition).\nThe representation may vary, but the concept is the same.\nWe can also generate a output for a specific input, G : x -\u0026gt; y. Here we generate an image y given an inpu image x. This is a Pix2Pix GAN.\nCheck this amazing demo of Pix2Pix GAN\nThis kind of image to image (pix2pix) can be done with the help of Encoder-Decoder architecture, where the input image is encoded to a feature representation vector anf this vector is decoded to the target image.\nSo the generator will learn the mapping from G: x-\u0026gt;y with autoencoder architecture, and generate new samples for the given x.\nThe generator G will get a pair of images:\n training x and training y\nG will classify as real training x and generated y(for x)\nG will classify as fake training x and generated y (different x)\nG will classify as fake  This way a conditional GAN(CGAN) or pix2pix GAN is trained, which has massive applications. In the next post we will see how to train a GAN to do a image to image translation(pix2pix) without labelled pair.\n","date":1558759380,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558759380,"objectID":"57bde7a314f54aa0646c1282cb1f5762","permalink":"/post/gan-5/","publishdate":"2019-05-25T04:43:00Z","relpermalink":"/post/gan-5/","section":"post","summary":"Conditional GAN","tags":["Deep Learning","GAN"],"title":"GAN 5","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Python","PyTorch"],"content":" DGGAN is exactly the same as Linear GANs, excpet they use COnvolutional neural networks. As we all know CNNs are the best feature extractor for many kind of data like image, videos, audio, etc.\nWe will use Street View House (SVHN) Dataset and generate new home numbers using DCGAN. The model architecture will be the same, except the CNNs will be used in the Discriminator to classify images as real or fake. Generator will use a transpose convolutional layers to upsample/generate new image samples from a given latent vector z.\nDiscriminator In the original paper, no max-pooling layers are used with the CNN layers, rather a stride of 2 is used. Batch Normalization and Leaku ReLU are also used. Linear layers are connected at the end of flattened cnn layers and sigmoid activation is used to make the output in the range 0 to 1.\nGenerator In generator as we need to upsample a latent vector to an image of size [3, 32, 32], we use transposed convolutional layer with ReLU activation and Batch Normalization. Tanh activation in the output layer. Let\u0026rsquo;s code the DCGAN Data Pytorch have SVHN dataset built-in to the datset library, we will use that for the dataset.\nimport torch from torchvision import datasets from torchvision import transforms transform = transforms.ToTensor() svhn = datasets.SVHN(root='data/', split='train', download=True, transform=transform) train_loader = torch.utils.data.DataLoader(dataset=svhn_train, batch_size=256, shuffle=True)  We want to scale the images to have the value in the range -1 to 1.\ndef scale_img(x, feature_range=(-1, 1)): min, max = feature_range x = x * (max - min) + min return x  Discriminator We will build a model with CNN and batch norm layers, it is a normal CNN classifier.\nimport torch.nn as nn import torch.nn.functional as F # function to return conv and batchnorm together def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True): layers = [] conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False) layers.append(conv_layer) if batch_norm: layers.append(nn.BatchNorm2d(out_channels)) return nn.Sequential(*layers) class Discriminator(nn.Module): def __init__(self, conv_dim=32): super(Discriminator, self).__init__() self.conv_dim = conv_dim self.conv1 = conv(3, conv_dim, 4, batch_norm=False) # self.conv2 = conv(conv_dim, conv_dim*2, 4) self.conv3 = conv(conv_dim*2, conv_dim*4, 4) self.fc = nn.Linear(conv_dim*4*4*4, 1) def forward(self, x): out = F.leaky_relu(self.conv1(x), 0.2) out = F.leaky_relu(self.conv2(out), 0.2) out = F.leaky_relu(self.conv3(out), 0.2) out = out.view(-1, self.conv_dim*4*4*4) out = self.fc(out) return out  Generator Generator need transposed Convolutioanl layer with Batch Norm and relu to upsample the latent vector to a image sample.\ndef deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True): layers = [] transpose_conv_layer = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)r layers.append(transpose_conv_layer) if batch_norm: layers.append(nn.BatchNorm2d(out_channels)) return nn.Sequential(*layers) class Generator(nn.Module): def __init__(self, z_size, conv_dim=32): super(Generator, self).__init__() self.conv_dim = conv_dim self.fc = nn.Linear(z_size, conv_dim*4*4*4) self.t_conv1 = deconv(conv_dim*4, conv_dim*2, 4) self.t_conv2 = deconv(conv_dim*2, conv_dim, 4) self.t_conv3 = deconv(conv_dim, 3, 4, batch_norm=False) def forward(self, x): out = self.fc(x) out = out.view(-1, self.conv_dim*4, 4, 4) # ( out = F.relu(self.t_conv1(out)) out = F.relu(self.t_conv2(out)) out = self.t_conv3(out) out = F.tanh(out) return out  Building the models conv_dim = 32 z_size = 100 D = Discriminator(conv_dim) G = Generator(z_size=z_size, conv_dim=conv_dim) print(D) print() print(G) train_on_gpu = torch.cuda.is_available() if train_on_gpu: G.cuda() D.cuda() print('GPU available for training. Models moved to GPU') else: print('Training on CPU.')  Output :\n Discriminator( (conv1): Sequential( (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) ) (conv2): Sequential( (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (conv3): Sequential( (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (fc): Linear(in_features=2048, out_features=1, bias=True) ) Generator( (fc): Linear(in_features=100, out_features=2048, bias=True) (t_conv1): Sequential( (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (t_conv2): Sequential( (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (t_conv3): Sequential( (0): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) ) )  Loss Loss and training loop is exactly same as Linear GANs. we scale the images in range -1 to 1 inside training loop. Check GAN2 and GAN3 is you dont understand about the loss and training loop.\ndef real_loss(D_out, smooth=False): batch_size = D_out.size(0) if smooth: labels = torch.ones(batch_size)*0.9 else: labels = torch.ones(batch_size) if train_on_gpu: labels = labels.cuda() criterion = nn.BCEWithLogitsLoss() loss = criterion(D_out.squeeze(), labels) return loss def fake_loss(D_out): batch_size = D_out.size(0) labels = torch.zeros(batch_size) if train_on_gpu: labels = labels.cuda() criterion = nn.BCEWithLogitsLoss() loss = criterion(D_out.squeeze(), labels) return loss  Optimizers check this optimizer post by ruder\nimport torch.optim as optim lr = 1e-4 beta1=0.5 beta2=0.999 d_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2]) g_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])  Training loop num_epochs = 50 samples = [] losses = [] print_every = 300 sample_size=16 fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size)) fixed_z = torch.from_numpy(fixed_z).float() for epoch in range(num_epochs): for batch_i, (real_images, _) in enumerate(train_loader): batch_size = real_images.size(0) real_images = scale(real_images) # rescale image d_optimizer.zero_grad() if train_on_gpu: real_images = real_images.cuda() D_real = D(real_images) d_real_loss = real_loss(D_real) z = np.random.uniform(-1, 1, size=(batch_size, z_size)) z = torch.from_numpy(z).float() if train_on_gpu: z = z.cuda() fake_images = G(z) D_fake = D(fake_images) d_fake_loss = fake_loss(D_fake) d_loss = d_real_loss + d_fake_loss d_loss.backward() d_optimizer.step() g_optimizer.zero_grad() z = np.random.uniform(-1, 1, size=(batch_size, z_size)) z = torch.from_numpy(z).float() if train_on_gpu: z = z.cuda() fake_images = G(z) D_fake = D(fake_images) g_loss = real_loss(D_fake) g_loss.backward() g_optimizer.step() print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format( epoch+1, num_epochs, d_loss.item(), g_loss.item()))  Output:\nEpoch [ 1/ 50] | d_loss: 1.3871 | g_loss: 0.7894 Epoch [ 1/ 50] | d_loss: 0.8700 | g_loss: 2.5015 Epoch [ 2/ 50] | d_loss: 1.0024 | g_loss: 1.4002 Epoch [ 2/ 50] | d_loss: 1.2057 | g_loss: 1.1445 Epoch [ 3/ 50] | d_loss: 0.9766 | g_loss: 1.0346 Epoch [ 3/ 50] | d_loss: 0.9508 | g_loss: 0.9849 Epoch [ 4/ 50] | d_loss: 1.0338 | g_loss: 1.2916 Epoch [ 4/ 50] | d_loss: 0.7476 | g_loss: 1.7354 Epoch [ 5/ 50] | d_loss: 0.8847 | g_loss: 1.9047 Epoch [ 5/ 50] | d_loss: 0.9131 | g_loss: 2.6848 Epoch [ 6/ 50] | d_loss: 0.3747 | g_loss: 2.0961 Epoch [ 6/ 50] | d_loss: 0.5761 | g_loss: 1.4796 Epoch [ 7/ 50] | d_loss: 1.0538 | g_loss: 2.5600 Epoch [ 7/ 50] | d_loss: 0.5655 | g_loss: 1.1675  View generated Samples def view_samples(epoch, samples): fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True) for ax, img in zip(axes.flatten(), samples[epoch]): img = img.detach().cpu().numpy() img = np.transpose(img, (1, 2, 0)) img = ((img +1)*255 / (2)).astype(np.uint8) ax.xaxis.set_visible(False) ax.yaxis.set_visible(False) im = ax.imshow(img.reshape((32,32,3)))  Training a DCGAN is same as the Linear/Vanilla GAN, DCGANs can extract more features in an image with the CNN and can help in generating the distributions well.\nIn the next post, we will look at Pix2Pix GAN and its applications.\n","date":1558669380,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558669380,"objectID":"9aa73addb97c4bd545885afb2579256c","permalink":"/post/gan-4/","publishdate":"2019-05-24T03:43:00Z","relpermalink":"/post/gan-4/","section":"post","summary":"Deep Convolutional GAN","tags":["Deep Learning","GAN"],"title":"GAN 4","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Python","PyTorch"],"content":" We saw an Intro to GANs and the Theory of Game between Generator and Discriminator in the previous posts. In this post we are going to implement and learn about how to train GANs in PyTorch. We will start with MNIST dataset and in the future posts we will implement different applications of GANs and also my research paper on one of the application of GANs.\nSo the task is to use the MNIST dataset to generate new MNIST alike data samples with GANs. Let\u0026rsquo;s Code GAN Get the Data Import all the necessary libraries like Numpy, Matplotlib, torch, torchvision.\nimport numpy as np import torch import matplotlib.pyplot as plt from torchvision import datasets import torchvision.transforms as transforms  Now lets get the MNIST data from the torchvision datasets.\ntransform = transforms.ToTensor() data = datasets.MNIST(root='data', train=True, download=True, transform=transform) data_loader = torch.utils.data.DataLoader(data, batch_size=1024)  The Model As we have already seen in Theory of Game between Generator and Discriminator, the GAN models generally have 2 networks Discriminator D and Generator G. We will code both of these network as seperate classes in PyTorch. Discriminator The discriminator is a just a classifier , which takes input images and classifies the images as real or fake generated images. So lets make a classifier network in PyTorch.\nimport torch.nn as nn import torch.nn.functional as F class D(nn.Module): def __init__(self, input_size, hidden_dim, output_size): super(D, self).__init__() self.fc1 = nn.Linear(input_size, hidden_dim*4) self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2) self.fc3 = nn.Linear(hidden_dim*2, hidden_dim) self.fc4 = nn.Linear(hidden_dim, output_size) self.dropout = nn.Dropout(0.3) def forward(self, x): # flatten image x = x.view(-1, 28*28) x = F.leaky_relu(self.fc1(x), 0.2) x = self.dropout(x) x = F.leaky_relu(self.fc2(x), 0.2) x = self.dropout(x) x = F.leaky_relu(self.fc3(x), 0.2) x = self.dropout(x) out = F.log_softmax(self.fc4(x)) return out  The D network has 4 linear layers with leaky relu and dropout layers in between.\nHere the input size will be 28*28*1 (size of MNIST image)\nhidden dim can be anything of your choice.\noutput_size = 2 (real or fake)\nI am also adding a log softmax in the end for computation purpose.\nLets make a Discriminator object\nD_network = D(28*28*1, 50, 2) print(D_network)  output :\nD( (fc1): Linear(in_features=784, out_features=200, bias=True) (fc2): Linear(in_features=200, out_features=100, bias=True) (fc3): Linear(in_features=100, out_features=50, bias=True) (fc4): Linear(in_features=50, out_features=2, bias=True) (dropout): Dropout(p=0.3) )  Generator The Generator takes a random vector(z)(also called latent vector) and generates a sample image with a distribution close to the training data distribution. We want to upsample z to an image of size 1*28*28. Tanh was used as activation in the output layer(as used in the original paper) , but feel free to try other activations and check which gives good result.\nclass G(nn.Module): def __init__(self, input_size, hidden_dim, output_size): super(G, self).__init__() self.fc1 = nn.Linear(input_size, hidden_dim) self.fc2 = nn.Linear(hidden_dim, hidden_dim*2) self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4) self.fc4 = nn.Linear(hidden_dim*4, output_size) self.dropout = nn.Dropout(0.3) def forward(self, x): x = F.leaky_relu(self.fc1(x), 0.2) x = self.dropout(x) x = F.leaky_relu(self.fc2(x), 0.2) x = self.dropout(x) x = F.leaky_relu(self.fc3(x), 0.2) x = self.dropout(x) out = F.tanh(self.fc4(x)) return out  The G network architecture is same as D\u0026rsquo;s architecture except now we upsample the z to 28*28*1 size image.\nG_network = G(100, 50, 1*28*28) print(G_network)  G( (fc1): Linear(in_features=100, out_features=50, bias=True) (fc2): Linear(in_features=50, out_features=100, bias=True) (fc3): Linear(in_features=100, out_features=200, bias=True) (fc4): Linear(in_features=200, out_features=784, bias=True) (dropout): Dropout(p=0.3) )  Loss The discriminator wants the probability of fake images close to 0 and the generator wants the probability of the fake images generated by it to be close to 1.\nSo we define 2 losses\n Real Loss (loss btw p and 1) Fake loss (loss btw p and 0)  p is the probability of image to be real.\n For Generator : minimize real_loss(p) or p to be closer to 1. ie: fool generator by making realistic images.\n For Discriminator : minimize real_loss + fake loss. ie: p of real image close to 1 and p of fake image close to 0.\ndef real_loss(D_out, smooth=False): batch_size = D_out.size(0) # label smoothing if smooth: # smooth, real labels = 0.9 labels = torch.ones(batch_size)*0.9 else: labels = torch.ones(batch_size) # real labels = 1 criterion = nn.NLLLoss() loss = criterion(D_out.squeeze(), labels.long().cuda()) return loss def fake_loss(D_out): batch_size = D_out.size(0) labels = torch.zeros(batch_size) # fake labels = 0 criterion = nn.NLLLoss() loss = criterion(D_out.squeeze(), labels.long().cuda()) return loss   label smoothing is also done for better convergence.\nTraining We will use 2 optimizers\n One for Generator, which optimizes the real_loss of fake images. ie: it tries to make the classification prediction of fake images equal to 1. Next is discriminator, which tries to optimize real+fake loss. ie: it tries to make the prediciton of fake images to 0 and real images to 1.  Adjust the no of epochs, latent vector size, optimizer parameters, dimensions etc.\nnum_epochs = 100 print_every = 400 # train the network D.train() G.train() for epoch in range(num_epochs): for batch_i, (images, _) in enumerate(train_loader): batch_size = images.size(0) ## Important rescaling step ## real_images = images*2 - 1 # rescale input images from [0,1) to [-1, 1) d_optimizer.zero_grad() D_real = D(real_images) d_real_loss = real_loss(D_real, smooth=True) z = np.random.uniform(-1, 1, size=(batch_size, z_size)) z = torch.from_numpy(z).float() fake_images = G(z) D_fake = D(fake_images) d_fake_loss = fake_loss(D_fake) d_loss = d_real_loss + d_fake_loss d_loss.backward() d_optimizer.step() g_optimizer.zero_grad() z = np.random.uniform(-1, 1, size=(batch_size, z_size)) z = torch.from_numpy(z).float() fake_images = G(z) D_fake = D(fake_images) g_loss = real_loss(D_fake) g_optimizer.step() if batch_i % print_every == 0: print('Epoch {:5d}/{:5d}\\td_loss: {:6.4f}\\tg_loss: {:6.4f}'.format( epoch+1, num_epochs, d_loss.item(), g_loss.item()))  Epoch 1/ 100 d_loss: 1.3925 g_loss: 0.6747 Epoch 2/ 100 d_loss: 1.2275 g_loss: 0.6837 Epoch 3/ 100 d_loss: 1.0829 g_loss: 0.6959 Epoch 4/ 100 d_loss: 1.0295 g_loss: 0.7128 Epoch 5/ 100 d_loss: 1.0443 g_loss: 0.7358 Epoch 6/ 100 d_loss: 1.0362 g_loss: 0.7625 Epoch 7/ 100 d_loss: 0.9942 g_loss: 0.8000 Epoch 8/ 100 d_loss: 0.9445 g_loss: 0.8455 Epoch 9/ 100 d_loss: 0.9005 g_loss: 0.9073 Epoch 10/ 100 d_loss: 0.8604 g_loss: 0.9908 ...  Generate new MNIST Samples def view_samples(epoch, samples): fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True) for ax, img in zip(axes.flatten(), samples[epoch]): img = img.detach() ax.xaxis.set_visible(False) ax.yaxis.set_visible(False) im = ax.imshow(img.reshape((28,28)), cmap='Greys_r') sample_size=16 rand_z = np.random.uniform(-1, 1, size=(sample_size, z_size)) rand_z = torch.from_numpy(rand_z).float() G.eval() rand_images = G(rand_z) view_samples(0, [rand_images])  Linear GAN Model does a decent job in generating MNIST images. In next post we will look into DCGAN(Deep Convolutional GAN), to use CNNs for generating new samples.\nCheck this Awesome Repo on comparing Linear GAN and DCGAN for MNIST. Also this notebook for pytorch implementation of vanilla GAN(Linear).\n","date":1558410180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558410180,"objectID":"14121bcbbeb0a6d310e1ce83d1b3402e","permalink":"/post/gan-3/","publishdate":"2019-05-21T03:43:00Z","relpermalink":"/post/gan-3/","section":"post","summary":"MNIST Linear GAN","tags":["Deep Learning","GAN"],"title":"GAN 3","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Python","PyTorch"],"content":" As we saw in Intro and application of Generative Adversarial Network, GANs generate new data from a given dataset by learning the distribution of the dataset through adversarial process.\nWhat is an adversarial process/learning? A google search can tell you that adversarial machine learning is a technique used in Machine Learning which tries to fool the model by giving in false/malicious input.\nComponents of GANs:  Generator The Generator network takes random noise as input and convert it to a data sample(image/music) . The output of generator is a fake but realistic data sample. The choice of the random noise determines the distribution into which the data sample generated falls.\nBut the generator network have to be trained to produce samples for the given random noise. ie: the generator have to learn the distribution of the dataset, so it generates new data samples from the distribution.\nAs this is not a supervised learning, we cannot use labels to learn the parameters of generator. So we use adversarial learning technique to learn the distribution of the dataset.\nThe idea is to maximize the probability that the data sample generated by the generator is from the training dataset. But it is not easy as its un labelled , so we use the help of another network called Discriminator.\n Discriminator Discriminator is a normal Neural Network classifier. The discriminator finds if a data sample is from the training dataset or not.\nDuring the training process, the discriminator is given data from the training dataset 50% of the time and data samples generated by generator other 50% of the time. The discriminator classifies the generated data samples as fake and data from the training dataset as real data.\n  The Game Theory As the disciminator classifies the data sample from the generator as fake, the generator tries to fool the discriminator by generating more realistic data sample(learns the training data distribution well). The generator starts generating samples more close to the distribution of the training dataset.\nAs the generator tries to fool the discriminator, the discriminator learns to classify the more realistic(fake) data generated by the generator as fake. By this process both the networks learn the parameters which gives best results. This creates a competition between Generator(G) and Discriminator(D), this makes this an adversarial learning.\nCheck this\nIn game theory, an equlibrium is reached in a 2 player game when both the players recieve 0 payoff. When a player(P) wins, P gets a positive payoff of 1 and gets a negative payoff of -1 when loses. When a player loses, the player changes the stratergy to win the next round. As this continues the player becomes better but as the other player also gets better , an equilibrium is reached when both players uses random uniform stratergies. At equilibrium, neither of the players can improve further.\nMost of the machine learning models we used so far depends on optimization algorithms. We finds a set of parameters for which the cost function is minimum. But GANs have 2 players G \u0026amp; D. The G is trying to fool D and D is trying to classify G\u0026rsquo;s sample as fake data. As we can see D is trying to minimize the probability of G\u0026rsquo;s output as true data, whereas G is trying to increase the probability.\nCost of D = minimize(P(generated sample data is real)) Cost of G = maximize(P(generated sample data is real))  Theoritically equlibrium occurs when both probabilities are equal.\nP(generated sample data is real) = 0.5  This occurs for a set of parameters for which the G got the maximum probability and D got minimum probability. ie: a saddle point.\nsaddle point - both local maxima and local minima\nGenerator gets a local maxima when the distribution learned by generator is equal to the distribution of the training dataset.\nWe will use 2 seperate optimization algorithms for D and G, so it is not possible for us to find the equilibrium. But if we can use a single optimization algorithm which reduces both D \u0026amp; G costs together, then we may encounter perfect equilibirum.\nIn the next post, we will look into the practical implementation of GANs by coding and training it in PyTorch.\n","date":1558406580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558406580,"objectID":"d9d7cbd874401a4d0fb6f47eb32582a2","permalink":"/post/gan-2/","publishdate":"2019-05-21T02:43:00Z","relpermalink":"/post/gan-2/","section":"post","summary":"Theory of Game between Generator and Discriminator","tags":["Deep Learning","GAN"],"title":"GAN 2","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning"],"content":" GANs are ML models that can imagine new things. GANs can generate new data with a given dataset by learning its distribution. GANs have been mostly used with image data but can also be used on any kind of data. GANs draw a sample from the learned probability distribution of the dataset which is a completely new sample.\nGANs are unsupervised machine learning models , which learns the distribution of the data through adverserial process and generate new sample from the learned distribution.\nSome Recent Research in GAN  Stack GAN Stack GAN can take a description of an image and can generate new images matching that description. GAN picks a sample from a distribution of images which matches the description. slide\n iGAN iGANs can search for realistic possible image as the user draws the rough sketch.\nGithub\n Pix2Pix Images in one domain can be changed to image in another domain with GANS. Rough sketches can be made into a realistic image which are generated by GANs. Blue Prints of a building can be changed to an image of finished building with GANs. Github\n Many other applications like photos to cartoons, daylight image to night scene image, Cycle GAN.\n  Check out all of these Generative models.\nIn the next few posts, we will look deep into how GANs work and code GANs with PyTorch for different applications.\n","date":1558317600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558317600,"objectID":"029dbda1937caf638b86fb2868753ab1","permalink":"/post/gan-1/","publishdate":"2019-05-20T02:00:00Z","relpermalink":"/post/gan-1/","section":"post","summary":"Intro and applications of Generative Adversarial Network","tags":["Deep Learning","GAN"],"title":"GAN 1","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Unsupervised Learning"],"content":" Machine Learning is broadly divided into 3 types:  Supervised learning Unsupervised learning Reinforcement learning  \nSupervised Learning The task in supervised learning is to learn a function to map a data X to a label y. All the classification, regression, object detection/recognition/segmentation generally comes under supervised learning.\nIn supervised learning we have a dataset which contains data X and label y and we need to learn how to find y given X.\nUnsupervised Learning In unsupervised learning, we only have X and not the respective y. The goal is to learn the underlying structure/features of the dataset without any label.\nSome examples of Unsupervised Learning are\n Clustering Clustering is dividing the data into groups through some distance metric, like kmeans clustering.  Feature Learning As the name suggest, learning the features of each of the given data, without its label. This is generally done with a help of a model called Autoencoders.\nAutoencoders take the data X as the label y , it try to recreate the data X given data X and learns some underlying features in that process. We generally take the one of the middle layers of the autoencoder as the encoded feature.  Dimensionality Reduction As we know data can be multi dimensional which can extent even to millions. Computation and visualization of such multi dimensioanl data is difficult and thus we want to reduce the dimension of the data (to pick the dimensions which can represent the data more).\nDimensionality reduction is done by choosing the axis in the data space along which variance of the data is high.  and many other examples like data compression(using auto encoders), Generative models, density estimation, etc.\n  Why Unsupervised Learning?  Unsupervised learning doesn\u0026rsquo;t need labels. Making the training data for supervised learning is not easy. Its expensive, time consuming, labour consuming. The world has a lot of unlabelled data, which can be used directly or with a little pre processing for unsupervised learning.  Unsupervised Learning is still an ameature area of research, which has a lot of potential. Unsupervised learning is less expensive and can accelerate the AI field so much.\n","date":1558310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558310400,"objectID":"27d1b98c303c07483ea9cb766eb656b5","permalink":"/post/unsupervised-learning/","publishdate":"2019-05-20T00:00:00Z","relpermalink":"/post/unsupervised-learning/","section":"post","summary":"Intro to Unsupervised Learning","tags":["Deep Learning","Unsupervised Learning"],"title":"Unsupervised Learning 101","type":"post"},{"authors":["Shangeth Rajaa","JK Sahoo"],"categories":null,"content":"Cite as :\nRajaa S., Sahoo J.K. (2019) Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction. In: Singh M., Gupta P., Tyagi V., Flusser J., Ã–ren T., Kashyap R. (eds) Advances in Computing and Data Sciences. ICACDS 2019. Communications in Computer and Information Science, vol 1045. Springer, Singapore\n","date":1558137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558137600,"objectID":"24ca6bd6ed9bcd6819f9c40eefb45e6c","permalink":"/publication/icacds/","publishdate":"2019-05-18T00:00:00Z","relpermalink":"/publication/icacds/","section":"publication","summary":"Stock Prediction with CNN and Neural Arithmetic Logic Units.","tags":["Deep Learning","PyTorch"],"title":"Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction","type":"publication"},{"authors":[],"categories":null,"content":"","date":1555160400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555160400,"objectID":"d13b1f263203cf0884ea5e3c60769dad","permalink":"/talk/icacds/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/icacds/","section":"talk","summary":"Presentation of my paper \"Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction\" at ICACDS 2019 Conference.","tags":[],"title":"Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction","type":"talk"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":[],"categories":null,"content":"","date":1547236800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547236800,"objectID":"381eff0b30d0969cc6d6ae0655de9911","permalink":"/talk/multi-tasking-learning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/multi-tasking-learning/","section":"talk","summary":"A Talk on Multitasking learning as best project chosen by FacebookAI and Udacity.","tags":[],"title":"Multi Tasking Learning","type":"talk"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"33c96636b53dc0f2a4842dacad9785a8","permalink":"/project/character-generating-rnn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/character-generating-rnn/","section":"project","summary":"Character level language model of RNN(LSTM) in PyTorch.","tags":["Deep Learning","Python","NLP","PyTorch"],"title":"Character Generating RNN","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bf3e111ff5fa7319b68614400b61b81d","permalink":"/project/computer-vision-security-system/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/computer-vision-security-system/","section":"project","summary":"Computer vision security system server build with Python, OpenCV, Flask web server.","tags":["Computer Vision","Python","JavaScript"],"title":"Computer Vision Security System","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"282a173f5eca67ca5c3b6c142509f212","permalink":"/project/emojification/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/emojification/","section":"project","summary":"Emojify a sentence with NLP, flask server to emojify a sentence.","tags":["Deep Learning","NLP","Python","Keras","JavaScript"],"title":"Emojification","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9e55cd99254ae71ee3e62fd187a803e2","permalink":"/project/hand-gesture-recognition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/hand-gesture-recognition/","section":"project","summary":"Recognizing the hand gesture using CNN feature extraction.","tags":["Deep Learning","Computer Vision","Python","Tensorflow"],"title":"Hand Gesture Recognition","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"34b1e8cd4adc5242c33b8c3a394f1aa7","permalink":"/project/lane-detection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/lane-detection/","section":"project","summary":"Lane Detection for self driving cars with Deep Learning(CNN) with the camera image data.","tags":["Deep Learning","Python","Computer Vision","Self Driving Cars","Keras"],"title":"Lane Detection with Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a5a90b199d4d8fa71f69281c9c521b93","permalink":"/project/multitasking-learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/multitasking-learning/","section":"project","summary":"Multitasking learning to use the CNN extracted features for multiple tasks like predicting age, sex, face direction, etc.","tags":["Deep Learning","Python","Computer Vision","Keras"],"title":"Multi Tasking Learning for face characterization","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9b5611b35d0fe55dcb89e7cf0cd27239","permalink":"/project/neural-style-transfer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/neural-style-transfer/","section":"project","summary":"Neural Style transfer of images in PyTorch.","tags":["Deep Learning","Python","Computer Vision","PyTorch"],"title":"Neural Style Transfer","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ec641199efbb6bc51972564333aa628f","permalink":"/project/nytimes-topic-modelling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/nytimes-topic-modelling/","section":"project","summary":"Topic Modelling for New York Times news articles for given dates using NYTimes API.","tags":["Topic Modelling","Python","NLP"],"title":"NYTimes Topic Modelling","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"05559a29ede4e1058072fa375b325d5e","permalink":"/project/pyrevshell/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/pyrevshell/","section":"project","summary":"A server client Reverse shell using python, can use any device's shell using this from another device in the network.","tags":["Python"],"title":"PyRevShell","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"befd31eb9f5cdc14aed99a7aac7c42ce","permalink":"/project/self-driving-cars-steering-angle-prediction/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/self-driving-cars-steering-angle-prediction/","section":"project","summary":"Prediction of which direction the car should change the steering direction in autonomous mode with the camera image as the input using transfer learning and fine tuning.","tags":["Deep Learning","Computer Vision","Self Driving Cars","Python","Tensorflow"],"title":"Self Driving Cars Steering Angle Prediction","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"90fc0166029130a6e9e179b6951e6cca","permalink":"/project/self-driving-cars-vehicle-detection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/self-driving-cars-vehicle-detection/","section":"project","summary":"Detection of other vehicles for self driving cars with YOLO in tensorflow.","tags":["Deep Learning","Computer Vision","Self Driving Cars","Python","Tensorflow"],"title":"Self Driving Cars Vehicle Detection","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8491d5ad45e23efbef2c021b6d1d1f3e","permalink":"/project/seq2seq-machine-translation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/seq2seq-machine-translation/","section":"project","summary":"Machine Translation english to french using Seq2Seq Attention model in PyTorch.","tags":["Deep Learning","Python","NLP","PyTorch"],"title":"Seq2Seq Machine Translation","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f116c6b6180f2fdf083da45996812e52","permalink":"/project/signature-verification/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/signature-verification/","section":"project","summary":"Signature verification with siamese network.","tags":["Deep Learning","Computer Vision"],"title":"Signature Verification with Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d67e120ff9736002127131016086076c","permalink":"/project/sockchat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/sockchat/","section":"project","summary":"Sock Chat is a server-client chat application with database which can be used to any web server or software applications,","tags":["Python"],"title":"SockChat","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"28c0d4e4fc2f501dcb486c837c012cc7","permalink":"/project/tictactoe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/tictactoe/","section":"project","summary":"2 player Tic Tac Toe game programmed in JavaScript.","tags":["JavaScript"],"title":"Tic Tac Toe","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cafd875167c47b3bd805f99a1d48fd56","permalink":"/project/nltk-sentiment-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/nltk-sentiment-analysis/","section":"project","summary":"Sentiment analysis of tweets of any topic fetched with twitter API and sentiment analysis of the tweets with NLTK.","tags":["NLP","Python","JavaScript"],"title":"Twitter Sentiment analysis","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"34c0ac182533d3a863fb69d13e227a5a","permalink":"/project/vehicle-speed-estimation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/vehicle-speed-estimation/","section":"project","summary":"Estimation of a vehicle's speed with its camera frames using deep leanring in PyTorch.","tags":["Deep Learning","Python","Computer Vision","Self Driving Cars","PyTorch"],"title":"Vehicle Speed Estimation","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bb5ee3b1d3ce8470980e92fe2fd3ca24","permalink":"/project/web-builder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/web-builder/","section":"project","summary":"Live HTML, CSS, JavaScript code output.","tags":["JavaScript"],"title":"Web Builder","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5b420fd0a369ab016ae61c3b944068f2","permalink":"/project/weekly-scheduler/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/weekly-scheduler/","section":"project","summary":"This a web weekly scheduler build with JavaScript.","tags":["JavaScript"],"title":"Weekly Scheduler","type":"project"}]