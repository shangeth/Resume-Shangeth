[{"authors":["admin"],"categories":null,"content":"I am a 4rd year Undergrad Dual degree student at BITS Pilani Goa Campus. Actively Looking for research opportunities in Deep Learning to start from May, 2020.\n","date":1558759380,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1558759380,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a 4rd year Undergrad Dual degree student at BITS Pilani Goa Campus. Actively Looking for research opportunities in Deep Learning to start from May, 2020.","tags":null,"title":"Shangeth Rajaa","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"26b574e37d08aa6c19fd68b51660eb1a","permalink":"/google-ml-academy/deeplearning/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/google-ml-academy/deeplearning/","section":"google-ml-academy","summary":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.","tags":null,"title":"Course Overview","type":"docs"},{"authors":null,"categories":null,"content":" Watever\nwatever topic 1 watever topic 2 another topic 3 ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"069cfca4a1416e1ec9d06942b98f0666","permalink":"/google-ml-academy/deeplearning/1.1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/google-ml-academy/deeplearning/1.1/","section":"google-ml-academy","summary":" Watever\nwatever topic 1 watever topic 2 another topic 3 ","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Watever\nwatever topic 1 watever topic 2 another topic 3 ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"dbae80859d0685cb16b6d9812848ba6a","permalink":"/google-ml-academy/deeplearning/1.2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/google-ml-academy/deeplearning/1.2/","section":"google-ml-academy","summary":" Watever\nwatever topic 1 watever topic 2 another topic 3 ","tags":null,"title":"Numpy","type":"docs"},{"authors":null,"categories":null,"content":" Google ML Academy 2019\nInstructor: Shangeth Rajaa \nBefore starting with Neural Networks, we will look into 2 important machine learning models to understand regression and classification tasks\n Linear Regression (Regression) Logistic Regression (Classification)  Linear Regression Most of the Deep learning Courses do not start with linear regression(LinReg), but LinReg gave me a better understanding of machine learning, so i will start with that, hoping that will make the understanding of Neural networks easier.\nYou can think of LinReg model as a curve fitting or function approximation model. Given a dataset $(X, y)$, the task is to find a relation $f$ between $X$ and $y$ such that $y = f(X)$. We are interested in this mapping $f: X \\rightarrow y$, as for any given $X$ in the future we can find $y = f(X)$.\nFor example, given a dataset about housing prices vs area of the house(Toy Dataset)\n   House Price($) y Area(1000 sqft) X     1034 2.4   1145 2.7   1252 3.04   2231 4.67   3423 5.3   \u0026hellip; \u0026hellip;    If we can find any the mapping between $X$ and $y$, $y = f(X)$, then its easy to predict the values of $y$ for any given $X$.\n# example dataset for linear regression %matplotlib inline import numpy as np import matplotlib.pyplot as plt x = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 plt.figure(figsize=(12,7)) plt.scatter(x, y, label='Data $(x,y)$') plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  So the prediction will be a line in case of 2-D data like above.\nThe predicted model will be line $ y = m x + c $ or in ML world popularly $ y = w x + b $.\n$y = f(X) = wX+b$ will be the Linear regression model and the objective is to find the best $w$ and $b$ that will give the nearest values for each $(X,y=wX+b)$ pair in the dataset.\n# example dataset for linear regression import numpy as np import matplotlib.pyplot as plt x = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 plt.figure(figsize=(12,7)) plt.scatter(x, y, label='Data $(x,y)$') plt.plot(x, 2 * x + 1, label='Predicted Line $y = f(X)$', color='r',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  What about Multi Dimensional Data? In real world, the dataset is going to be multi dimensional, ie: the price of a House will not just depend on the area of the house, it may also depend on multiple factors like no of rooms, locality, distance from airport, etc.\n   House Price($) y Area(1000 sqft) x1 No of Rooms x2 Distance form airport x3     1034 2.4 2 5.4   1145 2.7 3 3.1   1252 3.04 3 4.21   2231 4.67 2 2.3   3423 5.3 1 12   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;    In this case the model will be $y = f(X) = w_1x_1 + w_2x_2+w_3x_3+\u0026hellip;+w_nx_n + b$\nThis can also be represented in matrix form as $y = f(X) = X.W + b$\n$$X =\\begin{bmatrix} x_1 \u0026amp; x_2 \u0026amp; x_3 \u0026amp; \\dots \u0026amp; x_n \\ \\end{bmatrix}$$\n$$W = \\begin{bmatrix} x_1\\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} $$\n$$ b = [b]$$\nHow good is the Model? Before learning how to find $W$ and $b$ of a model. Let us assume we have a calculated values for optimal $W$ and $b$, how do we know how good is this model $\\hat{y} = f(X) = X.W + b$.\nNotice it is not $y$, its $\\hat{y}$. Because W and b are not the exact values, they are calculated approximate values to get $f(X)$ close to $y$. So we represent the prediction as $\\hat{y} = X.W + b$ and the true target as $y$.\nOur goal is to make $\\hat{y}$ as close as possible to $y$.\nSo we use a metric function call Mean Squared Error (MSE)\n$MSE(y, \\hat{y}) = \\mathcal{L}(y , \\hat{y}) = \\dfrac{1}{n} \\sum_{i=1}^{i=n}{(y_i - \\hat{y}_i)^2}$\nWhy MSE? Why not just subtraction? - model with 5 is better than -5 in sbtraction which actually is not both models are equally bad, but the predictions are in opposite direction. - MSE takes care of -5 and 5 ; $(-5)^2$ = $5^2$ = 25.\nimport numpy as np def MSE(y, y_hat): num_ex = len(y) mse_loss = np.sum((y - y_hat)**2)/num_ex return mse_loss  y = np.array([1.02, 2.3, 6.7, 3]) y_hat1 = np.array([10.43, 23.4, 12, 11]) y_hat2 = np.array([1, 1.9, 7, 3.1]) MSE(y,y), MSE(y, y_hat1), MSE(y, y_hat2)  (0.0, 156.46202499999998, 0.06509999999999996)  # example dataset for linear regression import numpy as np import matplotlib.pyplot as plt x = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 w = -1 b = 0 y_hat = w * x + b mse_loss = MSE(y, y_hat) plt.figure(figsize=(12,7)) plt.scatter(x, y, label='Data $(x,y)$') plt.plot(x, y_hat, label='Predicted Line $y = f(X)$', color='r',linewidth=4.0) plt.text(0,-40,'MSE = {:.3f}'.format(mse_loss), fontsize=20) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  # example dataset for linear regression import numpy as np import matplotlib.pyplot as plt x = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 w = 1 b = 0.5 y_hat = w * x + b mse_loss = MSE(y, y_hat) plt.figure(figsize=(12,7)) plt.scatter(x, y, label='Data $(x,y)$') plt.plot(x, y_hat, label='Predicted Line $y = f(X)$', color='r',linewidth=4.0) plt.text(0,-40,'MSE = {:.3f}'.format(mse_loss), fontsize=20) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  You can evidently see that a better model have less MSE loss. So the object of finding $W$ and $b$ will be to reduce the MSE Loss.\nIts is not generally possible to get a MSE of 0 in real world data, as the real world data is always noisy and with outliers.\nHow to find the Best Model? (W, b)  randomly initialize W, b in loop for n steps/epochs{  find $\\hat{y} = X.W + b$ find $MSE = \\mathcal{L}(y, \\hat{y})$ find $ \\frac{\\partial \\mathcal{L}}{\\partial w} $ and $ \\frac{\\partial \\mathcal{L}}{\\partial b} $ Update W and b with $w := w - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w}$ and $b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$ }   This is called Gradient Descent.\nLet\u0026rsquo;s try it first and see why it works.\n$\\mathcal{L}= \\dfrac{1}{n} \\sum_{i=1}^{i=n}{(y_i - wx^i-b)^2} $\n$ \\dfrac{\\partial \\mathcal{L}}{\\partial w} = \\dfrac{1}{n}\\sum_{i=1}^{i=n}{2(y_i - wx^i - b)(-x^i)}$\n$ \\dfrac{\\partial \\mathcal{L}}{\\partial b} = \\dfrac{1}{n} \\sum_{i=1}^{i=n}{ 2 (y_i - wx^i-b) (-1)}$\nTherefore\n$w := w - \\alpha \\dfrac{2}{n} (wX+b - y) (X)$\n$b := b - \\alpha \\dfrac{2}{n} \\sum_{i=1}^{i=n}{ (wX+b - y_i)}$\ndef model_forward(x, w, b): y_hat = w * x + b return y_hat def gradient_descent(w, b, X, y, a): w = w - a * 2 / X.shape[0] * np.dot(X.T, model_forward(X, w, b)- y) b = b - a * 2 / X.shape[0] * np.sum(model_forward(X, w, b)- y) return w, b  w, b = np.random.random(1), np.random.random(1) X = np.arange(-25, 25, 0.5) y = 2 * x + 1 + np.random.randn(100)*5 y_hat = model_forward(X, w, b) MSE(y, y_hat) # of randomly initialized model  831.0459153266066  losses = [] alpha = 0.00001 for i in range(1000): y_hat = model_forward(X, w, b) mse = MSE(y_hat, y) losses.append(mse) w, b = gradient_descent(w, b, X, y, alpha) if i%500 == 0: y_hat = model_forward(X, w, b) mse = MSE(y_hat, y) plt.figure(figsize=(12,7)) plt.scatter(X, y, label='Data $(X, y)$') plt.plot(X, y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,-30,'Epoch = {}'.format(i+1), fontsize=20) plt.text(0,-40,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show() plt.plot(losses) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show()  You can see how the loss decreases and the model prediction becomes better with the number of epochs.\nTry to run the code with different alpha, different no of epochs and different initialization of w and b.\nLinear Regression in TensorFlow import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np def MSE(y, y_hat): num_ex = len(y) mse_loss = np.sum((y - y_hat)**2)/num_ex return mse_loss X = np.arange(-25, 25, 0.5).astype('float32') y = (2 * X + 1 + np.random.randn(100)*5).astype('float32') model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])]) model.compile(optimizer='adam', loss='mean_squared_error') tf_history = model.fit(X, y, epochs=1000, verbose=False) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X, y, label='Data $(X, y)$') plt.plot(X, y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,-40,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Lets look into each line - model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n Sequential is like a container or wrapper which holds all the operations to be performed on your input.   model.compile(optimizer='adam', loss='mean_squared_error')\nCompile configures the training process. It defines the metrics, optimizers,..etc. You will understand more about this as we go on with more examples.  tf_history = model.fit(x, y, epochs=1000, verbose=False)\nmodel.fit actually trains the model for given number of epochs. Note: i've used Verbose=False(as the output will be long), set it to true to check the loss and metrics for each epoch.   Just 3-4 line of Tensorflow code can train a ml model, this is the advantage of using packages like Tensorflow/PyTorch. These packages are best optimized for speed and performance.\nmodel.summary()  Model: \u0026quot;sequential\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 1) 2 ================================================================= Total params: 2 Trainable params: 2 Non-trainable params: 0 _________________________________________________________________  tf.keras.Sequential? # if you are not sure of any library, its better to look into the docs.  ","date":1562367600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562367600,"objectID":"9303cd99e713074ad379d093cebcca5c","permalink":"/google-ml-academy/deeplearning/2.1/","publishdate":"2019-07-06T00:00:00+01:00","relpermalink":"/google-ml-academy/deeplearning/2.1/","section":"google-ml-academy","summary":"Google ML Academy 2019\nInstructor: Shangeth Rajaa \nBefore starting with Neural Networks, we will look into 2 important machine learning models to understand regression and classification tasks\n Linear Regression (Regression) Logistic Regression (Classification)  Linear Regression Most of the Deep learning Courses do not start with linear regression(LinReg), but LinReg gave me a better understanding of machine learning, so i will start with that, hoping that will make the understanding of Neural networks easier.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" \nGoogle ML Academy 2019\nInstructor: Shangeth Rajaa \nTask - 1  Get X and y from dataset() function Train a Linear Regression model for this dataset. Visualize the model prediction  Dataset Call dataset() function to get X, y\nimport numpy as np import matplotlib.pyplot as plt def dataset(show=True): X = np.arange(-25, 25, 0.1) y = X**2 + 20 + np.random.randn(500)*50 if show: plt.scatter(X, y) plt.show() return X, y X, y = dataset()  \u0026lt;Figure size 640x480 with 1 Axes\u0026gt;  Scaling Dataset The maximum value of y in the dataset goes upto 700 and the minimum values is less than 0. The range of y is very large which makes the convergence/loss reduction slower. So will we scale the data, scaling the data will help the model converge faster. If all the features and target are in same range, there will be symmetry in the curve of Loss vs weights/bias, which makes the convergence faster.\nWe will do a very simple type of scaling, we will divide all the values of the data with the maximum values for X and y respectively.\nX, y = dataset() print(max(X), max(y), min(X), min(y)) X = X/max(X) y = y/max(y) print(max(X), max(y), min(X), min(y))  24.90000000000071 699.9355092361043 -25.0 -156.58348535333417 1.0 1.0 -1.0040160642569995 -0.22371130380886986  This is not a great scaling method, but good to start. We will see many more scaling/normalizing methods later.\nTry training the model with and without scaling and see the difference yourself.\nLinear Regression in TensorFlow import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np X, y = dataset(show=False) X_scaled = X/max(X) y_scaled = y/max(y) model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])]) optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_scaled, y_scaled, epochs=500, verbose=False) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_scaled) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_scaled, y_scaled, label='Data $(X, y)$') plt.plot(X_scaled, y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  WARNING: Logging before flag parsing goes to stderr. W0827 01:02:10.229988 140184887179072 deprecation.py:506] From /home/shangeth/anaconda3/envs/dlenv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor  Looks the model Prediction for this dataset is very bad, but that is expected as the model is a straight line, it cannot predict non linear regression data. Is there a way to train a regression model for this task?\nPolynomial Regression So when the dataset is not linear, linear regression cannot learn the dataset and make good predictions.\nSo we need a polynomial model which consideres the polynomial terms as well. So we need terms like $x^2$, $x^3$, \u0026hellip;, $x^n$ for the model to learn a polynomial of $n^{th}$ degree.\n$\\hat{y} = w_0 + w_1x + w_2x^2 + \u0026hellip; + w_nx^n$\nOne down side of this model is that, We will have to decide the value of n. But this is better than a linear regression model. We can get an idea of the value of n by visualizing a dataset, but for multi variable dataset, we will have to try different values of n and check which is better.\nPolynomial Features you can calculate the polynomial features for each feature by programming it or you can try sklearn.preprocessing.PolynomialFeatures which allows us to make polynomial terms of our data.\nWe will try degree 2, 3 and 4\nX, y = dataset(show=False) X_scaled = X/max(X) y_scaled = y/max(y)  from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=2) X_2 = poly.fit_transform(X_scaled.reshape(-1,1)) print(X_2.shape) print(X_2[0])  (500, 3) [ 1. -1.00401606 1.00804826]  from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=3) X_3 = poly.fit_transform(X_scaled.reshape(-1,1)) print(X_3.shape) print(X_3[0])  (500, 4) [ 1. -1.00401606 1.00804826 -1.01209664]  from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=4) X_4 = poly.fit_transform(X_scaled.reshape(-1,1)) print(X_4.shape) print(X_4[0])  (500, 5) [ 1. -1.00401606 1.00804826 -1.01209664 1.01616129]  The PolynomialFeatures returns $[1, x, x^2, x^3,\u0026hellip;]$.\nTask - 2  Train a model with polynomial terms in the dataset. Visualize the prediction of the model  The code remains the same except, the no of input features will be 3, 4, 5 respectively.\nTensorflow Model with 2nd Degree import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[3])]) optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_2, y_scaled, epochs=500, verbose=False) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_2) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_2[:, 1], y_scaled, label='Data $(X, y)$') plt.plot(X_2[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Tensorflow Model with 3rd Degree import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[4])]) optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_3, y_scaled, epochs=500, verbose=False) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_3) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_3[:, 1], y_scaled, label='Data $(X, y)$') plt.plot(X_3[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  Tensorflow Model with 4th Degree import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt import numpy as np model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[5])]) optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) model.compile(optimizer=optimizer, loss='mean_squared_error') tf_history = model.fit(X_4, y_scaled, epochs=500, verbose=False) plt.plot(tf_history.history['loss']) plt.xlabel('Epochs') plt.ylabel('MSE Loss') plt.show() mse = tf_history.history['loss'][-1] y_hat = model.predict(X_4) plt.figure(figsize=(12,7)) plt.title('TensorFlow Model') plt.scatter(X_4[:, 1], y_scaled, label='Data $(X, y)$') plt.plot(X_4[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0) plt.xlabel('$X$', fontsize=20) plt.ylabel('$y$', fontsize=20) plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20) plt.grid(True) plt.legend(fontsize=20) plt.show()  The Dataset is a 2 degree dataset, so 2nd degree polynomial regression will do a good job, but 3rd, 4th,\u0026hellip;. all also make a good prediction as thay also contain 2nd degree polynomial terms.\nThis is polynomial regression. Yes, its easy. But one issue, as this was a toy dataset we know its a 2nd degree data, so we tried 2,3,4. But when the data is multi dimensional we cannot visualize the dataset, so its difficult to decide the degree. This is why you will see Neural Networks are awesome. They are End-End, they do not need several feature extraction from our side, they can extract necessary features of their own.\nMake a 3rd degree or 4th degree data and try polynomial regression on it. Also try different functions like exponents, trignometric..etc.\n","date":1562367600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562367600,"objectID":"1293963fd919d991e644882d452522da","permalink":"/google-ml-academy/deeplearning/2.2/","publishdate":"2019-07-06T00:00:00+01:00","relpermalink":"/google-ml-academy/deeplearning/2.2/","section":"google-ml-academy","summary":"Google ML Academy 2019\nInstructor: Shangeth Rajaa \nTask - 1  Get X and y from dataset() function Train a Linear Regression model for this dataset. Visualize the model prediction  Dataset Call dataset() function to get X, y\nimport numpy as np import matplotlib.pyplot as plt def dataset(show=True): X = np.arange(-25, 25, 0.1) y = X**2 + 20 + np.random.randn(500)*50 if show: plt.scatter(X, y) plt.show() return X, y X, y = dataset()  \u0026lt;Figure size 640x480 with 1 Axes\u0026gt;  Scaling Dataset The maximum value of y in the dataset goes upto 700 and the minimum values is less than 0.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":"","date":1562528221,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562528221,"objectID":"835012fefc81d462e93431e390609db4","permalink":"/project/facial-emotion-recognition-pytorch/","publishdate":"2019-07-08T01:07:01+05:30","relpermalink":"/project/facial-emotion-recognition-pytorch/","section":"project","summary":"Recognizing the facial emotions with Deep learning model trained on PyTorch and deployed with TF.js model converted with ONNX.","tags":["Deep Learning","Computer Vision","Python","PyTorch","Tensorflow","JavaScript"],"title":"Facial Emotion Recognition PyTorch ONNX","type":"project"},{"authors":["Zhengying\tLiu","Zhen\tXu","Julio\tJacques Junior","Meysam\tMadadi","Sergio\tEscalera","Shangeth\tRajaa"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1560470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560470400,"objectID":"293b9a3850ef93d4b029d5635dfddd5c","permalink":"/publication/ads/","publishdate":"2019-06-14T00:00:00Z","relpermalink":"/publication/ads/","section":"publication","summary":"Framework and Mathematical formulation of AutoML","tags":["Source Themes"],"title":"Overview and unifying conceptualization of Automated Machine Learning","type":"publication"},{"authors":null,"categories":null,"content":"","date":1560470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560470400,"objectID":"ceba210875c59b1eaa91353e1e8ea687","permalink":"/project/pneumonia-diagnosis-with-deep-learning/","publishdate":"2019-06-14T00:00:00Z","relpermalink":"/project/pneumonia-diagnosis-with-deep-learning/","section":"project","summary":"Web Application for Diagnosis of Pnuemonia with deep learning model trained and backed with PyTorch framework.","tags":["Deep Learning","Python","Computer Vision","PyTorch","Flask"],"title":"Pneumonia Diagnosis with Deep Learning","type":"project"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Python","PyTorch"],"content":" As we have seen in GAN 1, GAN 2, GAN 3, GAN 4 that GANs have 2 network, the Generator G and the Discriminator D. Given a latent vector z, the G generates a new sample from the distribution of the training data. D classifies a data sample as real(from the training data) or fake(generated by G).\nIn the starting, the G generates a random data sample(as it didnt learn the data distribution) and the D is not a good classifier now. As the training process goes, the G starts learning the data distribution and D becomes a good classifier. D tries to classify all sampels generated by D as fake, G tries to generate samples such that D classifies that as real. In the process, both the networks become better and Generator learns the distribution of the data and can now generate realistic samples. D becomes good at classifying real/fake data samples. Conditional GAN Let us consider MNIST GAN, after we trained a MNIST dataset on a GAN model, the generator(G) can now generate some images which look alike of the MNIST numbers.\nBut what if we want the G to generate images of a specific digit?. The G which we trained generated images samples depending on the latent vector z. But we used a random z. So we cannot choose a map from random z - \u0026gt; Specific image.\nSo we introduce a conditional label y, such that for a condition label y the generator have to generate sample. Now the Generator learns the distribution of the dataset and generates samples based on the condition y or c(condition).\nThe representation may vary, but the concept is the same.\nWe can also generate a output for a specific input, G : x -\u0026gt; y. Here we generate an image y given an inpu image x. This is a Pix2Pix GAN.\nCheck this amazing demo of Pix2Pix GAN\nThis kind of image to image (pix2pix) can be done with the help of Encoder-Decoder architecture, where the input image is encoded to a feature representation vector anf this vector is decoded to the target image.\nSo the generator will learn the mapping from G: x-\u0026gt;y with autoencoder architecture, and generate new samples for the given x.\nThe generator G will get a pair of images:\n training x and training y\nG will classify as real training x and generated y(for x)\nG will classify as fake training x and generated y (different x)\nG will classify as fake  This way a conditional GAN(CGAN) or pix2pix GAN is trained, which has massive applications. In the next post we will see how to train a GAN to do a image to image translation(pix2pix) without labelled pair.\n","date":1558759380,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558759380,"objectID":"57bde7a314f54aa0646c1282cb1f5762","permalink":"/post/gan-5/","publishdate":"2019-05-25T04:43:00Z","relpermalink":"/post/gan-5/","section":"post","summary":"Conditional GAN","tags":["Deep Learning post","GAN post"],"title":"GAN 5","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Python","PyTorch"],"content":" DGGAN is exactly the same as Linear GANs, excpet they use COnvolutional neural networks. As we all know CNNs are the best feature extractor for many kind of data like image, videos, audio, etc.\nWe will use Street View House (SVHN) Dataset and generate new home numbers using DCGAN. The model architecture will be the same, except the CNNs will be used in the Discriminator to classify images as real or fake. Generator will use a transpose convolutional layers to upsample/generate new image samples from a given latent vector z.\nDiscriminator In the original paper, no max-pooling layers are used with the CNN layers, rather a stride of 2 is used. Batch Normalization and Leaku ReLU are also used. Linear layers are connected at the end of flattened cnn layers and sigmoid activation is used to make the output in the range 0 to 1.\nGenerator In generator as we need to upsample a latent vector to an image of size [3, 32, 32], we use transposed convolutional layer with ReLU activation and Batch Normalization. Tanh activation in the output layer. Let\u0026rsquo;s code the DCGAN Data Pytorch have SVHN dataset built-in to the datset library, we will use that for the dataset.\nimport torch from torchvision import datasets from torchvision import transforms transform = transforms.ToTensor() svhn = datasets.SVHN(root='data/', split='train', download=True, transform=transform) train_loader = torch.utils.data.DataLoader(dataset=svhn_train, batch_size=256, shuffle=True)  We want to scale the images to have the value in the range -1 to 1.\ndef scale_img(x, feature_range=(-1, 1)): min, max = feature_range x = x * (max - min) + min return x  Discriminator We will build a model with CNN and batch norm layers, it is a normal CNN classifier.\nimport torch.nn as nn import torch.nn.functional as F # function to return conv and batchnorm together def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True): layers = [] conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False) layers.append(conv_layer) if batch_norm: layers.append(nn.BatchNorm2d(out_channels)) return nn.Sequential(*layers) class Discriminator(nn.Module): def __init__(self, conv_dim=32): super(Discriminator, self).__init__() self.conv_dim = conv_dim self.conv1 = conv(3, conv_dim, 4, batch_norm=False) # self.conv2 = conv(conv_dim, conv_dim*2, 4) self.conv3 = conv(conv_dim*2, conv_dim*4, 4) self.fc = nn.Linear(conv_dim*4*4*4, 1) def forward(self, x): out = F.leaky_relu(self.conv1(x), 0.2) out = F.leaky_relu(self.conv2(out), 0.2) out = F.leaky_relu(self.conv3(out), 0.2) out = out.view(-1, self.conv_dim*4*4*4) out = self.fc(out) return out  Generator Generator need transposed Convolutioanl layer with Batch Norm and relu to upsample the latent vector to a image sample.\ndef deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True): layers = [] transpose_conv_layer = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)r layers.append(transpose_conv_layer) if batch_norm: layers.append(nn.BatchNorm2d(out_channels)) return nn.Sequential(*layers) class Generator(nn.Module): def __init__(self, z_size, conv_dim=32): super(Generator, self).__init__() self.conv_dim = conv_dim self.fc = nn.Linear(z_size, conv_dim*4*4*4) self.t_conv1 = deconv(conv_dim*4, conv_dim*2, 4) self.t_conv2 = deconv(conv_dim*2, conv_dim, 4) self.t_conv3 = deconv(conv_dim, 3, 4, batch_norm=False) def forward(self, x): out = self.fc(x) out = out.view(-1, self.conv_dim*4, 4, 4) # ( out = F.relu(self.t_conv1(out)) out = F.relu(self.t_conv2(out)) out = self.t_conv3(out) out = F.tanh(out) return out  Building the models conv_dim = 32 z_size = 100 D = Discriminator(conv_dim) G = Generator(z_size=z_size, conv_dim=conv_dim) print(D) print() print(G) train_on_gpu = torch.cuda.is_available() if train_on_gpu: G.cuda() D.cuda() print('GPU available for training. Models moved to GPU') else: print('Training on CPU.')  Output :\n Discriminator( (conv1): Sequential( (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) ) (conv2): Sequential( (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (conv3): Sequential( (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (fc): Linear(in_features=2048, out_features=1, bias=True) ) Generator( (fc): Linear(in_features=100, out_features=2048, bias=True) (t_conv1): Sequential( (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (t_conv2): Sequential( (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (t_conv3): Sequential( (0): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) ) )  Loss Loss and training loop is exactly same as Linear GANs. we scale the images in range -1 to 1 inside training loop. Check GAN2 and GAN3 is you dont understand about the loss and training loop.\ndef real_loss(D_out, smooth=False): batch_size = D_out.size(0) if smooth: labels = torch.ones(batch_size)*0.9 else: labels = torch.ones(batch_size) if train_on_gpu: labels = labels.cuda() criterion = nn.BCEWithLogitsLoss() loss = criterion(D_out.squeeze(), labels) return loss def fake_loss(D_out): batch_size = D_out.size(0) labels = torch.zeros(batch_size) if train_on_gpu: labels = labels.cuda() criterion = nn.BCEWithLogitsLoss() loss = criterion(D_out.squeeze(), labels) return loss  Optimizers check this optimizer post by ruder\nimport torch.optim as optim lr = 1e-4 beta1=0.5 beta2=0.999 d_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2]) g_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])  Training loop num_epochs = 50 samples = [] losses = [] print_every = 300 sample_size=16 fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size)) fixed_z = torch.from_numpy(fixed_z).float() for epoch in range(num_epochs): for batch_i, (real_images, _) in enumerate(train_loader): batch_size = real_images.size(0) real_images = scale(real_images) # rescale image d_optimizer.zero_grad() if train_on_gpu: real_images = real_images.cuda() D_real = D(real_images) d_real_loss = real_loss(D_real) z = np.random.uniform(-1, 1, size=(batch_size, z_size)) z = torch.from_numpy(z).float() if train_on_gpu: z = z.cuda() fake_images = G(z) D_fake = D(fake_images) d_fake_loss = fake_loss(D_fake) d_loss = d_real_loss + d_fake_loss d_loss.backward() d_optimizer.step() g_optimizer.zero_grad() z = np.random.uniform(-1, 1, size=(batch_size, z_size)) z = torch.from_numpy(z).float() if train_on_gpu: z = z.cuda() fake_images = G(z) D_fake = D(fake_images) g_loss = real_loss(D_fake) g_loss.backward() g_optimizer.step() print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format( epoch+1, num_epochs, d_loss.item(), g_loss.item()))  Output:\nEpoch [ 1/ 50] | d_loss: 1.3871 | g_loss: 0.7894 Epoch [ 1/ 50] | d_loss: 0.8700 | g_loss: 2.5015 Epoch [ 2/ 50] | d_loss: 1.0024 | g_loss: 1.4002 Epoch [ 2/ 50] | d_loss: 1.2057 | g_loss: 1.1445 Epoch [ 3/ 50] | d_loss: 0.9766 | g_loss: 1.0346 Epoch [ 3/ 50] | d_loss: 0.9508 | g_loss: 0.9849 Epoch [ 4/ 50] | d_loss: 1.0338 | g_loss: 1.2916 Epoch [ 4/ 50] | d_loss: 0.7476 | g_loss: 1.7354 Epoch [ 5/ 50] | d_loss: 0.8847 | g_loss: 1.9047 Epoch [ 5/ 50] | d_loss: 0.9131 | g_loss: 2.6848 Epoch [ 6/ 50] | d_loss: 0.3747 | g_loss: 2.0961 Epoch [ 6/ 50] | d_loss: 0.5761 | g_loss: 1.4796 Epoch [ 7/ 50] | d_loss: 1.0538 | g_loss: 2.5600 Epoch [ 7/ 50] | d_loss: 0.5655 | g_loss: 1.1675  View generated Samples def view_samples(epoch, samples): fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True) for ax, img in zip(axes.flatten(), samples[epoch]): img = img.detach().cpu().numpy() img = np.transpose(img, (1, 2, 0)) img = ((img +1)*255 / (2)).astype(np.uint8) ax.xaxis.set_visible(False) ax.yaxis.set_visible(False) im = ax.imshow(img.reshape((32,32,3)))  Training a DCGAN is same as the Linear/Vanilla GAN, DCGANs can extract more features in an image with the CNN and can help in generating the distributions well.\nIn the next post, we will look at Pix2Pix GAN and its applications.\n","date":1558669380,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558669380,"objectID":"9aa73addb97c4bd545885afb2579256c","permalink":"/post/gan-4/","publishdate":"2019-05-24T03:43:00Z","relpermalink":"/post/gan-4/","section":"post","summary":"Deep Convolutional GAN","tags":["Deep Learning post","GAN post"],"title":"GAN 4","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Python","PyTorch"],"content":" We saw an Intro to GANs and the Theory of Game between Generator and Discriminator in the previous posts. In this post we are going to implement and learn about how to train GANs in PyTorch. We will start with MNIST dataset and in the future posts we will implement different applications of GANs and also my research paper on one of the application of GANs.\nSo the task is to use the MNIST dataset to generate new MNIST alike data samples with GANs. Let\u0026rsquo;s Code GAN Get the Data Import all the necessary libraries like Numpy, Matplotlib, torch, torchvision.\nimport numpy as np import torch import matplotlib.pyplot as plt from torchvision import datasets import torchvision.transforms as transforms  Now lets get the MNIST data from the torchvision datasets.\ntransform = transforms.ToTensor() data = datasets.MNIST(root='data', train=True, download=True, transform=transform) data_loader = torch.utils.data.DataLoader(data, batch_size=1024)  The Model As we have already seen in Theory of Game between Generator and Discriminator, the GAN models generally have 2 networks Discriminator D and Generator G. We will code both of these network as seperate classes in PyTorch. Discriminator The discriminator is a just a classifier , which takes input images and classifies the images as real or fake generated images. So lets make a classifier network in PyTorch.\nimport torch.nn as nn import torch.nn.functional as F class D(nn.Module): def __init__(self, input_size, hidden_dim, output_size): super(D, self).__init__() self.fc1 = nn.Linear(input_size, hidden_dim*4) self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2) self.fc3 = nn.Linear(hidden_dim*2, hidden_dim) self.fc4 = nn.Linear(hidden_dim, output_size) self.dropout = nn.Dropout(0.3) def forward(self, x): # flatten image x = x.view(-1, 28*28) x = F.leaky_relu(self.fc1(x), 0.2) x = self.dropout(x) x = F.leaky_relu(self.fc2(x), 0.2) x = self.dropout(x) x = F.leaky_relu(self.fc3(x), 0.2) x = self.dropout(x) out = F.log_softmax(self.fc4(x)) return out  The D network has 4 linear layers with leaky relu and dropout layers in between.\nHere the input size will be 28*28*1 (size of MNIST image)\nhidden dim can be anything of your choice.\noutput_size = 2 (real or fake)\nI am also adding a log softmax in the end for computation purpose.\nLets make a Discriminator object\nD_network = D(28*28*1, 50, 2) print(D_network)  output :\nD( (fc1): Linear(in_features=784, out_features=200, bias=True) (fc2): Linear(in_features=200, out_features=100, bias=True) (fc3): Linear(in_features=100, out_features=50, bias=True) (fc4): Linear(in_features=50, out_features=2, bias=True) (dropout): Dropout(p=0.3) )  Generator The Generator takes a random vector(z)(also called latent vector) and generates a sample image with a distribution close to the training data distribution. We want to upsample z to an image of size 1*28*28. Tanh was used as activation in the output layer(as used in the original paper) , but feel free to try other activations and check which gives good result.\nclass G(nn.Module): def __init__(self, input_size, hidden_dim, output_size): super(G, self).__init__() self.fc1 = nn.Linear(input_size, hidden_dim) self.fc2 = nn.Linear(hidden_dim, hidden_dim*2) self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4) self.fc4 = nn.Linear(hidden_dim*4, output_size) self.dropout = nn.Dropout(0.3) def forward(self, x): x = F.leaky_relu(self.fc1(x), 0.2) x = self.dropout(x) x = F.leaky_relu(self.fc2(x), 0.2) x = self.dropout(x) x = F.leaky_relu(self.fc3(x), 0.2) x = self.dropout(x) out = F.tanh(self.fc4(x)) return out  The G network architecture is same as D\u0026rsquo;s architecture except now we upsample the z to 28*28*1 size image.\nG_network = G(100, 50, 1*28*28) print(G_network)  G( (fc1): Linear(in_features=100, out_features=50, bias=True) (fc2): Linear(in_features=50, out_features=100, bias=True) (fc3): Linear(in_features=100, out_features=200, bias=True) (fc4): Linear(in_features=200, out_features=784, bias=True) (dropout): Dropout(p=0.3) )  Loss The discriminator wants the probability of fake images close to 0 and the generator wants the probability of the fake images generated by it to be close to 1.\nSo we define 2 losses\n Real Loss (loss btw p and 1) Fake loss (loss btw p and 0)  p is the probability of image to be real.\n For Generator : minimize real_loss(p) or p to be closer to 1. ie: fool generator by making realistic images.\n For Discriminator : minimize real_loss + fake loss. ie: p of real image close to 1 and p of fake image close to 0.\n  def real_loss(D_out, smooth=False): batch_size = D_out.size(0) # label smoothing if smooth: # smooth, real labels = 0.9 labels = torch.ones(batch_size)*0.9 else: labels = torch.ones(batch_size) # real labels = 1 criterion = nn.NLLLoss() loss = criterion(D_out.squeeze(), labels.long().cuda()) return loss def fake_loss(D_out): batch_size = D_out.size(0) labels = torch.zeros(batch_size) # fake labels = 0 criterion = nn.NLLLoss() loss = criterion(D_out.squeeze(), labels.long().cuda()) return loss  label smoothing is also done for better convergence.\nTraining We will use 2 optimizers\n One for Generator, which optimizes the real_loss of fake images. ie: it tries to make the classification prediction of fake images equal to 1. Next is discriminator, which tries to optimize real+fake loss. ie: it tries to make the prediciton of fake images to 0 and real images to 1.  Adjust the no of epochs, latent vector size, optimizer parameters, dimensions etc.\nnum_epochs = 100 print_every = 400 # train the network D.train() G.train() for epoch in range(num_epochs): for batch_i, (images, _) in enumerate(train_loader): batch_size = images.size(0) ## Important rescaling step ## real_images = images*2 - 1 # rescale input images from [0,1) to [-1, 1) d_optimizer.zero_grad() D_real = D(real_images) d_real_loss = real_loss(D_real, smooth=True) z = np.random.uniform(-1, 1, size=(batch_size, z_size)) z = torch.from_numpy(z).float() fake_images = G(z) D_fake = D(fake_images) d_fake_loss = fake_loss(D_fake) d_loss = d_real_loss + d_fake_loss d_loss.backward() d_optimizer.step() g_optimizer.zero_grad() z = np.random.uniform(-1, 1, size=(batch_size, z_size)) z = torch.from_numpy(z).float() fake_images = G(z) D_fake = D(fake_images) g_loss = real_loss(D_fake) g_optimizer.step() if batch_i % print_every == 0: print('Epoch {:5d}/{:5d}\\td_loss: {:6.4f}\\tg_loss: {:6.4f}'.format( epoch+1, num_epochs, d_loss.item(), g_loss.item()))  Epoch 1/ 100 d_loss: 1.3925 g_loss: 0.6747 Epoch 2/ 100 d_loss: 1.2275 g_loss: 0.6837 Epoch 3/ 100 d_loss: 1.0829 g_loss: 0.6959 Epoch 4/ 100 d_loss: 1.0295 g_loss: 0.7128 Epoch 5/ 100 d_loss: 1.0443 g_loss: 0.7358 Epoch 6/ 100 d_loss: 1.0362 g_loss: 0.7625 Epoch 7/ 100 d_loss: 0.9942 g_loss: 0.8000 Epoch 8/ 100 d_loss: 0.9445 g_loss: 0.8455 Epoch 9/ 100 d_loss: 0.9005 g_loss: 0.9073 Epoch 10/ 100 d_loss: 0.8604 g_loss: 0.9908 ...  Generate new MNIST Samples def view_samples(epoch, samples): fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True) for ax, img in zip(axes.flatten(), samples[epoch]): img = img.detach() ax.xaxis.set_visible(False) ax.yaxis.set_visible(False) im = ax.imshow(img.reshape((28,28)), cmap='Greys_r') sample_size=16 rand_z = np.random.uniform(-1, 1, size=(sample_size, z_size)) rand_z = torch.from_numpy(rand_z).float() G.eval() rand_images = G(rand_z) view_samples(0, [rand_images])  Linear GAN Model does a decent job in generating MNIST images. In next post we will look into DCGAN(Deep Convolutional GAN), to use CNNs for generating new samples.\nCheck this Awesome Repo on comparing Linear GAN and DCGAN for MNIST. Also this notebook for pytorch implementation of vanilla GAN(Linear).\n","date":1558410180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558410180,"objectID":"14121bcbbeb0a6d310e1ce83d1b3402e","permalink":"/post/gan-3/","publishdate":"2019-05-21T03:43:00Z","relpermalink":"/post/gan-3/","section":"post","summary":"MNIST Linear GAN","tags":["Deep Learning post","GAN post## Supervised learning"],"title":"GAN 3","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Python","PyTorch"],"content":" As we saw in Intro and application of Generative Adversarial Network, GANs generate new data from a given dataset by learning the distribution of the dataset through adversarial process.\nWhat is an adversarial process/learning? A google search can tell you that adversarial machine learning is a technique used in Machine Learning which tries to fool the model by giving in false/malicious input.\nComponents of GANs:  Generator The Generator network takes random noise as input and convert it to a data sample(image/music) . The output of generator is a fake but realistic data sample. The choice of the random noise determines the distribution into which the data sample generated falls.\nBut the generator network have to be trained to produce samples for the given random noise. ie: the generator have to learn the distribution of the dataset, so it generates new data samples from the distribution.\nAs this is not a supervised learning, we cannot use labels to learn the parameters of generator. So we use adversarial learning technique to learn the distribution of the dataset.\nThe idea is to maximize the probability that the data sample generated by the generator is from the training dataset. But it is not easy as its un labelled , so we use the help of another network called Discriminator.\n Discriminator Discriminator is a normal Neural Network classifier. The discriminator finds if a data sample is from the training dataset or not.\nDuring the training process, the discriminator is given data from the training dataset 50% of the time and data samples generated by generator other 50% of the time. The discriminator classifies the generated data samples as fake and data from the training dataset as real data.\n  The Game Theory As the disciminator classifies the data sample from the generator as fake, the generator tries to fool the discriminator by generating more realistic data sample(learns the training data distribution well). The generator starts generating samples more close to the distribution of the training dataset.\nAs the generator tries to fool the discriminator, the discriminator learns to classify the more realistic(fake) data generated by the generator as fake. By this process both the networks learn the parameters which gives best results. This creates a competition between Generator(G) and Discriminator(D), this makes this an adversarial learning.\nCheck this\nIn game theory, an equlibrium is reached in a 2 player game when both the players recieve 0 payoff. When a player(P) wins, P gets a positive payoff of 1 and gets a negative payoff of -1 when loses. When a player loses, the player changes the stratergy to win the next round. As this continues the player becomes better but as the other player also gets better , an equilibrium is reached when both players uses random uniform stratergies. At equilibrium, neither of the players can improve further.\nMost of the machine learning models we used so far depends on optimization algorithms. We finds a set of parameters for which the cost function is minimum. But GANs have 2 players G \u0026amp; D. The G is trying to fool D and D is trying to classify G\u0026rsquo;s sample as fake data. As we can see D is trying to minimize the probability of G\u0026rsquo;s output as true data, whereas G is trying to increase the probability.\nCost of D = minimize(P(generated sample data is real)) Cost of G = maximize(P(generated sample data is real))  Theoritically equlibrium occurs when both probabilities are equal.\nP(generated sample data is real) = 0.5  This occurs for a set of parameters for which the G got the maximum probability and D got minimum probability. ie: a saddle point.\nsaddle point - both local maxima and local minima\nGenerator gets a local maxima when the distribution learned by generator is equal to the distribution of the training dataset.\nWe will use 2 seperate optimization algorithms for D and G, so it is not possible for us to find the equilibrium. But if we can use a single optimization algorithm which reduces both D \u0026amp; G costs together, then we may encounter perfect equilibirum.\nIn the next post, we will look into the practical implementation of GANs by coding and training it in PyTorch.\n","date":1558406580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558406580,"objectID":"d9d7cbd874401a4d0fb6f47eb32582a2","permalink":"/post/gan-2/","publishdate":"2019-05-21T02:43:00Z","relpermalink":"/post/gan-2/","section":"post","summary":"Theory of Game between Generator and Discriminator","tags":["Deep Learning post","GAN post"],"title":"GAN 2","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning"],"content":" GANs are ML models that can imagine new things. GANs can generate new data with a given dataset by learning its distribution. GANs have been mostly used with image data but can also be used on any kind of data. GANs draw a sample from the learned probability distribution of the dataset which is a completely new sample.\nGANs are unsupervised machine learning models , which learns the distribution of the data through adverserial process and generate new sample from the learned distribution.\nSome Recent Research in GAN  Stack GAN Stack GAN can take a description of an image and can generate new images matching that description. GAN picks a sample from a distribution of images which matches the description. slide\n iGAN iGANs can search for realistic possible image as the user draws the rough sketch.\nGithub\n Pix2Pix Images in one domain can be changed to image in another domain with GANS. Rough sketches can be made into a realistic image which are generated by GANs. Blue Prints of a building can be changed to an image of finished building with GANs. Github\n Many other applications like photos to cartoons, daylight image to night scene image, Cycle GAN.\n  Check out all of these Generative models.\nIn the next few posts, we will look deep into how GANs work and code GANs with PyTorch for different applications.\n","date":1558317600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558317600,"objectID":"029dbda1937caf638b86fb2868753ab1","permalink":"/post/gan-1/","publishdate":"2019-05-20T02:00:00Z","relpermalink":"/post/gan-1/","section":"post","summary":"Intro and applications of Generative Adversarial Network","tags":["Deep Learning post","GAN post## Supervised learning"],"title":"GAN 1","type":"post"},{"authors":["Shangeth Rajaa"],"categories":["Deep Learning","Unsupervised Learning"],"content":" Machine Learning is broadly divided into 3 types:  Supervised learning Unsupervised learning Reinforcement learning  \nSupervised Learning The task in supervised learning is to learn a function to map a data X to a label y. All the classification, regression, object detection/recognition/segmentation generally comes under supervised learning.\nIn supervised learning we have a dataset which contains data X and label y and we need to learn how to find y given X.\nUnsupervised Learning In unsupervised learning, we only have X and not the respective y. The goal is to learn the underlying structure/features of the dataset without any label.\nSome examples of Unsupervised Learning are\n Clustering Clustering is dividing the data into groups through some distance metric, like kmeans clustering.  Feature Learning As the name suggest, learning the features of each of the given data, without its label. This is generally done with a help of a model called Autoencoders.\nAutoencoders take the data X as the label y , it try to recreate the data X given data X and learns some underlying features in that process. We generally take the one of the middle layers of the autoencoder as the encoded feature.  Dimensionality Reduction As we know data can be multi dimensional which can extent even to millions. Computation and visualization of such multi dimensioanl data is difficult and thus we want to reduce the dimension of the data (to pick the dimensions which can represent the data more).\nDimensionality reduction is done by choosing the axis in the data space along which variance of the data is high.  and many other examples like data compression(using auto encoders), Generative models, density estimation, etc.\n  Why Unsupervised Learning?  Unsupervised learning doesn\u0026rsquo;t need labels. Making the training data for supervised learning is not easy. Its expensive, time consuming, labour consuming. The world has a lot of unlabelled data, which can be used directly or with a little pre processing for unsupervised learning.  Unsupervised Learning is still an ameature area of research, which has a lot of potential. Unsupervised learning is less expensive and can accelerate the AI field so much.\n","date":1558310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558310400,"objectID":"27d1b98c303c07483ea9cb766eb656b5","permalink":"/post/unsupervised-learning/","publishdate":"2019-05-20T00:00:00Z","relpermalink":"/post/unsupervised-learning/","section":"post","summary":"Intro to Unsupervised Learning","tags":["Deep Learning post","Unsupervised Learning post"],"title":"Unsupervised Learning 101","type":"post"},{"authors":["Shangeth Rajaa","JK Sahoo"],"categories":null,"content":"Cite as :\nRajaa S., Sahoo J.K. (2019) Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction. In: Singh M., Gupta P., Tyagi V., Flusser J., ren T., Kashyap R. (eds) Advances in Computing and Data Sciences. ICACDS 2019. Communications in Computer and Information Science, vol 1045. Springer, Singapore\n","date":1558137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558137600,"objectID":"24ca6bd6ed9bcd6819f9c40eefb45e6c","permalink":"/publication/icacds/","publishdate":"2019-05-18T00:00:00Z","relpermalink":"/publication/icacds/","section":"publication","summary":"Stock Prediction with CNN and Neural Arithmetic Logic Units.","tags":["Deep Learning","PyTorch"],"title":"Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction","type":"publication"},{"authors":[],"categories":null,"content":"","date":1555160400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555160400,"objectID":"d13b1f263203cf0884ea5e3c60769dad","permalink":"/talk/icacds/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/icacds/","section":"talk","summary":"Presentation of my paper \"Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction\" at ICACDS 2019 Conference.","tags":[],"title":"Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction","type":"talk"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":[],"categories":null,"content":"","date":1547236800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547236800,"objectID":"381eff0b30d0969cc6d6ae0655de9911","permalink":"/talk/multi-tasking-learning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/multi-tasking-learning/","section":"talk","summary":"A Talk on Multitasking learning as best project chosen by FacebookAI and Udacity.","tags":[],"title":"Multi Tasking Learning","type":"talk"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"33c96636b53dc0f2a4842dacad9785a8","permalink":"/project/character-generating-rnn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/character-generating-rnn/","section":"project","summary":"Character level language model of RNN(LSTM) in PyTorch.","tags":["Deep Learning","Python","NLP","PyTorch"],"title":"Character Generating RNN","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bf3e111ff5fa7319b68614400b61b81d","permalink":"/project/computer-vision-security-system/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/computer-vision-security-system/","section":"project","summary":"Computer vision security system server build with Python, OpenCV, Flask web server.","tags":["Computer Vision","Python","JavaScript"],"title":"Computer Vision Security System","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"282a173f5eca67ca5c3b6c142509f212","permalink":"/project/emojification/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/emojification/","section":"project","summary":"Emojify a sentence with NLP, flask server to emojify a sentence.","tags":["Deep Learning","NLP","Python","Keras","JavaScript"],"title":"Emojification","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9e55cd99254ae71ee3e62fd187a803e2","permalink":"/project/hand-gesture-recognition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/hand-gesture-recognition/","section":"project","summary":"Recognizing the hand gesture using CNN feature extraction.","tags":["Deep Learning","Computer Vision","Python","Tensorflow"],"title":"Hand Gesture Recognition","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"34b1e8cd4adc5242c33b8c3a394f1aa7","permalink":"/project/lane-detection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/lane-detection/","section":"project","summary":"Lane Detection for self driving cars with Deep Learning(CNN) with the camera image data.","tags":["Deep Learning","Python","Computer Vision","Self Driving Cars","Keras"],"title":"Lane Detection with Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a5a90b199d4d8fa71f69281c9c521b93","permalink":"/project/multitasking-learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/multitasking-learning/","section":"project","summary":"Multitasking learning to use the CNN extracted features for multiple tasks like predicting age, sex, face direction, etc.","tags":["Deep Learning","Python","Computer Vision","Keras"],"title":"Multi Tasking Learning for face characterization","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ec641199efbb6bc51972564333aa628f","permalink":"/project/nytimes-topic-modelling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/nytimes-topic-modelling/","section":"project","summary":"Topic Modelling for New York Times news articles for given dates using NYTimes API.","tags":["Topic Modelling","Python","NLP"],"title":"NYTimes Topic Modelling","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9b5611b35d0fe55dcb89e7cf0cd27239","permalink":"/project/neural-style-transfer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/neural-style-transfer/","section":"project","summary":"Neural Style transfer of images in PyTorch.","tags":["Deep Learning","Python","Computer Vision","PyTorch"],"title":"Neural Style Transfer","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"05559a29ede4e1058072fa375b325d5e","permalink":"/project/pyrevshell/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/pyrevshell/","section":"project","summary":"A server client Reverse shell using python, can use any device's shell using this from another device in the network.","tags":["Python"],"title":"PyRevShell","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"befd31eb9f5cdc14aed99a7aac7c42ce","permalink":"/project/self-driving-cars-steering-angle-prediction/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/self-driving-cars-steering-angle-prediction/","section":"project","summary":"Prediction of which direction the car should change the steering direction in autonomous mode with the camera image as the input using transfer learning and fine tuning.","tags":["Deep Learning","Computer Vision","Self Driving Cars","Python","Tensorflow"],"title":"Self Driving Cars Steering Angle Prediction","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"90fc0166029130a6e9e179b6951e6cca","permalink":"/project/self-driving-cars-vehicle-detection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/self-driving-cars-vehicle-detection/","section":"project","summary":"Detection of other vehicles for self driving cars with YOLO in tensorflow.","tags":["Deep Learning","Computer Vision","Self Driving Cars","Python","Tensorflow"],"title":"Self Driving Cars Vehicle Detection","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8491d5ad45e23efbef2c021b6d1d1f3e","permalink":"/project/seq2seq-machine-translation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/seq2seq-machine-translation/","section":"project","summary":"Machine Translation english to french using Seq2Seq Attention model in PyTorch.","tags":["Deep Learning","Python","NLP","PyTorch"],"title":"Seq2Seq Machine Translation","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f116c6b6180f2fdf083da45996812e52","permalink":"/project/signature-verification/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/signature-verification/","section":"project","summary":"Signature verification with siamese network.","tags":["Deep Learning","Computer Vision"],"title":"Signature Verification with Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d67e120ff9736002127131016086076c","permalink":"/project/sockchat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/sockchat/","section":"project","summary":"Sock Chat is a server-client chat application with database which can be used to any web server or software applications,","tags":["Python"],"title":"SockChat","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"28c0d4e4fc2f501dcb486c837c012cc7","permalink":"/project/tictactoe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/tictactoe/","section":"project","summary":"2 player Tic Tac Toe game programmed in JavaScript.","tags":["JavaScript"],"title":"Tic Tac Toe","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cafd875167c47b3bd805f99a1d48fd56","permalink":"/project/nltk-sentiment-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/nltk-sentiment-analysis/","section":"project","summary":"Sentiment analysis of tweets of any topic fetched with twitter API and sentiment analysis of the tweets with NLTK.","tags":["NLP","Python","JavaScript"],"title":"Twitter Sentiment analysis","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"34c0ac182533d3a863fb69d13e227a5a","permalink":"/project/vehicle-speed-estimation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/vehicle-speed-estimation/","section":"project","summary":"Estimation of a vehicle's speed with its camera frames using deep leanring in PyTorch.","tags":["Deep Learning","Python","Computer Vision","Self Driving Cars","PyTorch"],"title":"Vehicle Speed Estimation","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bb5ee3b1d3ce8470980e92fe2fd3ca24","permalink":"/project/web-builder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/web-builder/","section":"project","summary":"Live HTML, CSS, JavaScript code output.","tags":["JavaScript"],"title":"Web Builder","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5b420fd0a369ab016ae61c3b944068f2","permalink":"/project/weekly-scheduler/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/weekly-scheduler/","section":"project","summary":"This a web weekly scheduler build with JavaScript.","tags":["JavaScript"],"title":"Weekly Scheduler","type":"project"}]