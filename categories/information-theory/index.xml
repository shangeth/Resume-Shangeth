<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Information Theory on Shangeth</title>
    <link>/categories/information-theory/</link>
    <description>Recent content in Information Theory on Shangeth</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year} Shangeth Rajaa</copyright>
    <lastBuildDate>Sun, 26 Jan 2020 02:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/information-theory/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>KL Divergence</title>
      <link>/post/kl-divergence/</link>
      <pubDate>Sun, 26 Jan 2020 02:00:00 +0000</pubDate>
      
      <guid>/post/kl-divergence/</guid>
      <description>Colab Notebook
Before seeing KL Divergence, let&amp;rsquo;s see a very simple concept called Entropy
Entropy Entropy is the expected information contained in a Distribution. It measures the uncertainty.
$H(x) = \sum{p(x)I(x)}$ where $I(x)$ is called the Information content of $x$.
&amp;ldquo;If an event is very probable, it is no surprise (and generally uninteresting) when that event happens as expected. However, if an event is unlikely to occur, it has much more information to learn that the event happened or will happen.</description>
    </item>
    
  </channel>
</rss>