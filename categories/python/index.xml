<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Shangeth</title>
    <link>/categories/python/</link>
    <description>Recent content in Python on Shangeth</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year} Shangeth Rajaa</copyright>
    <lastBuildDate>Sun, 26 Jan 2020 02:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>KL Divergence</title>
      <link>/post/kl-divergence/</link>
      <pubDate>Sun, 26 Jan 2020 02:00:00 +0000</pubDate>
      
      <guid>/post/kl-divergence/</guid>
      <description>Colab Notebook
Before seeing KL Divergence, let&amp;rsquo;s see a very simple concept called Entropy
Entropy Entropy is the expected information contained in a Distribution. It measures the uncertainty.
$H(x) = \sum{p(x)I(x)}$ where $I(x)$ is called the Information content of $x$.
&amp;ldquo;If an event is very probable, it is no surprise (and generally uninteresting) when that event happens as expected. However, if an event is unlikely to occur, it has much more information to learn that the event happened or will happen.</description>
    </item>
    
    <item>
      <title>Off Policy Monte Carlo Prediction with Importance sampling</title>
      <link>/post/off-policy-monte-carlo/</link>
      <pubDate>Sun, 19 Jan 2020 02:00:00 +0000</pubDate>
      
      <guid>/post/off-policy-monte-carlo/</guid>
      <description>Off-Policy Monte Carlo with Importance Sampling Off Policy Learning Link to the Notebook By exploration-exploitation trade-off, the agent should take sub-optimal exploratory action by which the agent may receive less reward. One way of exploration is by using an epsilon-greedy policy, where the agent takes a nongreedy action with a small probability.
In an on-policy, improvement and evaluation are done on the policy which is used to select actions.
In off-policy, improvement and evaluation are done on a different policy from the one used to select actions.</description>
    </item>
    
    <item>
      <title>GAN 5</title>
      <link>/post/gan-5/</link>
      <pubDate>Sat, 25 May 2019 04:43:00 +0000</pubDate>
      
      <guid>/post/gan-5/</guid>
      <description>Conditional GAN</description>
    </item>
    
    <item>
      <title>GAN 4</title>
      <link>/post/gan-4/</link>
      <pubDate>Fri, 24 May 2019 03:43:00 +0000</pubDate>
      
      <guid>/post/gan-4/</guid>
      <description>Deep Convolutional GAN</description>
    </item>
    
    <item>
      <title>GAN 3</title>
      <link>/post/gan-3/</link>
      <pubDate>Tue, 21 May 2019 03:43:00 +0000</pubDate>
      
      <guid>/post/gan-3/</guid>
      <description>MNIST Linear GAN</description>
    </item>
    
    <item>
      <title>GAN 2</title>
      <link>/post/gan-2/</link>
      <pubDate>Tue, 21 May 2019 02:43:00 +0000</pubDate>
      
      <guid>/post/gan-2/</guid>
      <description>Theory of Game between Generator and Discriminator</description>
    </item>
    
  </channel>
</rss>