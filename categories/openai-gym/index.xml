<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>OpenAI Gym on Shangeth</title>
    <link>/categories/openai-gym/</link>
    <description>Recent content in OpenAI Gym on Shangeth</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year} Shangeth Rajaa</copyright>
    <lastBuildDate>Sun, 19 Jan 2020 02:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/openai-gym/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Off Policy Monte Carlo Prediction with Importance sampling</title>
      <link>/post/off-policy-monte-carlo/</link>
      <pubDate>Sun, 19 Jan 2020 02:00:00 +0000</pubDate>
      
      <guid>/post/off-policy-monte-carlo/</guid>
      <description>Off-Policy Monte Carlo with Importance Sampling Off Policy Learning Link to the Notebook By exploration-exploitation trade-off, the agent should take sub-optimal exploratory action by which the agent may receive less reward. One way of exploration is by using an epsilon-greedy policy, where the agent takes a nongreedy action with a small probability.
In an on-policy, improvement and evaluation are done on the policy which is used to select actions.</description>
    </item>
    
  </channel>
</rss>