<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Course Overview on Shangeth</title>
    <link>/courses/deeplearning/</link>
    <description>Recent content in Course Overview on Shangeth</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year} Shangeth Rajaa</copyright>
    <lastBuildDate>Thu, 29 Aug 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/courses/deeplearning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Data and Learning Problem</title>
      <link>/courses/deeplearning/0.1/</link>
      <pubDate>Wed, 18 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/0.1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>/courses/deeplearning/1.1/</link>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/1.1/</guid>
      <description>Before starting with Neural Networks, we will look into 2 important machine learning models to understand regression and classification tasks
 Linear Regression (Regression) Logistic Regression (Classification)  You can think of Linear Regression model as a curve fitting or function approximation model. Given a dataset $(X, y)$, the task is to find a relation $f$ between $X$ and $y$ such that $y = f(X)$. We are interested in this mapping $f: X \rightarrow y$, as for any given $X$ in the future we can find $y = f(X)$.</description>
    </item>
    
    <item>
      <title>Polynomial Regression</title>
      <link>/courses/deeplearning/1.2/</link>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/1.2/</guid>
      <description>Solutions Here:
It&amp;rsquo;s a very simple assignment, you can finish it in less than 10 minutes. If you are stuck somewhere refer this for solutions.
Task-1 : Linear Regression on Non-Linear Data  Get X and y from dataset() function Train a Linear Regression model for this dataset. Visualize the model prediction  Dataset Call dataset() function to get X, y
import numpy as np import matplotlib.pyplot as plt def dataset(show=True): X = np.</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>/courses/deeplearning/1.3/</link>
      <pubDate>Fri, 30 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/1.3/</guid>
      <description>Logistic Regression is one of the most commonly used classification models. Unlike Linear Regression which predicts a real unbounded value $\hat{y} = f(X) = WX+b$, Logistic Regression predicts the probability of a data belonging to a particular class.
For example for a given data (X,y) where X is a received email and y is 0 if email is spam and 1 if email is not spam. Logistic regression predicts the probability of the email X to be not spam.</description>
    </item>
    
    <item>
      <title>Multi Class Classification</title>
      <link>/courses/deeplearning/1.4/</link>
      <pubDate>Fri, 30 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/1.4/</guid>
      <description>Solutions Here: In the previous notebook we used logistic regression for Binary Classification, now we will see how to train a classifier model for Multi-Class Classification.
What is Multi-Class Classification? If the target values have n discrete classification classes ie: y can take discrete value from 0 to n-1. If $y \in {0, 1, 2, 3, &amp;hellip;, n-1}$, then the classification task is n-Multi-Class.
Task - 1 Visualizing Data Create a 3-Multi-Class dataset with sklearn.</description>
    </item>
    
    <item>
      <title>Motivation for Multi Layer Perceptron</title>
      <link>/courses/deeplearning/1.5/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/1.5/</guid>
      <description>Non-Linearity Note: We will look mostly at classification examples, but the same concepts apply to regression problems as well with a little change in using activation function(Sigmoid, Softmax which we learned in previous sections).
So far the datasets we have used are linearly separable, which means they can be separated by line(2-d), plane(3-d) and linear multi dimensional classifiers.
But in the real world, not all datasets are 2-d(visualizable) and linearly separable.</description>
    </item>
    
    <item>
      <title>Neural Network Architectures</title>
      <link>/courses/deeplearning/2.1/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.1/</guid>
      <description>We trained our first neural network in the previous notebook which had 3 layers
 Input Layer Hidden Layer Output Layer  Multiple Nodes The network had 2 linear layers($C_1$,0 $C_2$) in the hidden layer each of which gave us a linear classifier, then we used another linear layer($C_3$) in the output layer to combine $C_1$ and $C_2$ to give us a non-linear classifier($C$). That was amazing, now we can get non-linear classifiers.</description>
    </item>
    
    <item>
      <title>Batch Training</title>
      <link>/courses/deeplearning/2.2/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.2/</guid>
      <description>Batch Training Batch Training is something very important but we didn&amp;rsquo;t use before as our dataset was smaller and we were just learning how to train models.
So far,
 we took each example $(X^i, y^i)$ made prediction with $\hat{y}^i = g(X^i.W+b)$ calculated the loss $\mathcal{L}(y^i , \hat{y}^i)$ used back propagation to update $W$ and $b$ (all parameters in the model) with $w := w - \alpha \frac{\partial \mathcal{L}}{\partial w}$ and $b := b - \alpha \frac{\partial \mathcal{L}}{\partial b}$ and repeated this process.</description>
    </item>
    
    <item>
      <title>Optimizers</title>
      <link>/courses/deeplearning/2.3/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.3/</guid>
      <description>Gradient Descent We have seen what are the steps followed in Gradient Descent to minimize the loss function. Now let&amp;rsquo;s get into more detail.
The objective of gradient descent is to find $W, b$ for which $\mathcal{L}(W, b) $ is minimum for given pairs of data $(X, y)$.
Example Let&amp;rsquo;s forget about Machine learning for sometime and use gradient descent algorithm for optimization of simple functions.
Given a function $y = f(x) = 0.</description>
    </item>
    
    <item>
      <title>Learning Rate</title>
      <link>/courses/deeplearning/2.4/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.4/</guid>
      <description>Learning Rate In Gradient Descent we update the parameters of the model with
$w := w - \alpha \frac{\partial \mathcal{L}}{\partial w}$ and $b := b - \alpha \frac{\partial \mathcal{L}}{\partial b}$.
The learning rate $\alpha$ actually affects the learning process a lot.
 small $\alpha$ is slow, but more accurate, as it does not miss the minimum, but it also get stuck in a local minimum. larger $\alpha$ make huge steps, sometimes it may converge faster but it may miss the minima.</description>
    </item>
    
    <item>
      <title>Bias &amp; Variance</title>
      <link>/courses/deeplearning/2.5/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.5/</guid>
      <description>Bias &amp;amp; Variance Let us train a DNN model for a simple regression problem.
import numpy as np import matplotlib.pyplot as plt def dataset(show=True): X = np.arange(-5, 5, 0.01) y = 8 * np.sin(X) + np.random.randn(1000) if show: yy = 8 * np.sin(X) plt.figure(figsize=(15,9)) plt.scatter(X, y) plt.plot(X, yy, color=&amp;#39;red&amp;#39;, linewidth=7) plt.show() return X, y X, y = dataset(show=True) Lets train 2 models for this dataset
 a very simple linear model a very complex DNN model  Simple Linear Model We are going to split the dataset into 5 groups(random shuffle) and use each of the 5 groups to train 5 different linear models.</description>
    </item>
    
    <item>
      <title>Overfitting &amp; Regularization</title>
      <link>/courses/deeplearning/2.6/</link>
      <pubDate>Fri, 06 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.6/</guid>
      <description>MNIST Dataset The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.
Let&amp;rsquo;s get the dataset using tf.keras.datasets
Download MNIST import tensorflow as tf (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path=&amp;#39;mnist.npz&amp;#39;) Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step  Visualize MNIST Let&amp;rsquo;s visualize what is in the dataset</description>
    </item>
    
    <item>
      <title>ANN - Medical Diagnosis</title>
      <link>/courses/deeplearning/2.7/</link>
      <pubDate>Fri, 06 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.7/</guid>
      <description>We will use ANNs to diagnose Breast Cancer with some characteristics of the cell nuclei.
Dataset Download the Dataset We will use a breast cancer diagnosis dataset from Opeml.org
%%capture !wget https://www.openml.org/data/get_csv/5600/BNG_breast-w.arff Explore the Dataset import pandas as pd df = pd.read_csv(&amp;#39;/content/BNG_breast-w.arff&amp;#39;) df.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }   You can see all the features are real numbers, with different range, so they need to be scaled.</description>
    </item>
    
    <item>
      <title>ANN - Computer Vision</title>
      <link>/courses/deeplearning/2.8/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.8/</guid>
      <description>We will use ANNs for a basic computer vision application of image classification on CIFAR10 Dataset
Dataset CIFAR-10 The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.
Download the Dataset Tensorflow has inbuilt dataset which makes it easy to get training and testing data.
from tensorflow.keras.datasets import cifar10 import tensorflow as tf (x_train, y_train), (x_test, y_test) = cifar10.</description>
    </item>
    
    <item>
      <title>ANN - Natural Language Processing</title>
      <link>/courses/deeplearning/2.9/</link>
      <pubDate>Thu, 12 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.9/</guid>
      <description>Sentiment Analysis We are going to classify a movie review as Positive or Negative review given a text review.
We&amp;rsquo;ll use the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.
IMDB Dataset Download the Dataset import tensorflow as tf imdb = tf.</description>
    </item>
    
  </channel>
</rss>