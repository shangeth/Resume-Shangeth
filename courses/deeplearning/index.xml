<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Course Overview on Shangeth</title>
    <link>/courses/deeplearning/</link>
    <description>Recent content in Course Overview on Shangeth</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year} Shangeth Rajaa</copyright>
    <lastBuildDate>Thu, 29 Aug 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/courses/deeplearning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Linear Regression</title>
      <link>/courses/deeplearning/1.1/</link>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/1.1/</guid>
      <description>Open in GitHub
Google Explore ML Academy Instructor: Shangeth Rajaa 
Before starting with Neural Networks, we will look into 2 important machine learning models to understand regression and classification tasks - Linear Regression (Regression) - Logistic Regression (Classification)

Most of the Deep learning Courses do not start with linear regression(LinReg), but LinReg gave me a better understanding of machine learning, so i will start with that, hoping that will make the understanding of Neural networks easier.</description>
    </item>
    
    <item>
      <title>Polynomial Regression</title>
      <link>/courses/deeplearning/1.2/</link>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/1.2/</guid>
      <description>Open in GitHub
Google Explore ML Academy - BPGC Instructor: Shangeth Rajaa 
Solutions Here:

 Open in GitHub
Its a very simple assignment, you can finish it in less than 10 minutes. If you are stuck somewhere refer this for solutions.
Task-1 : Linear Regression on Non-Linear Data  Get X and y from dataset() function Train a Linear Regression model for this dataset. Visualize the model prediction  Dataset Call dataset() function to get X, y</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>/courses/deeplearning/1.3/</link>
      <pubDate>Fri, 30 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/1.3/</guid>
      <description>Open in GitHub
Google Explore ML Academy Instructor: Shangeth Rajaa 
Logistic Regression is one of the most commonly used classification model. Unlike Linear Regression which predicts a real unbounded value $\hat{y} = f(X) = WX+b$, Logistic Regression predicts the probability of a data belonging to a particular class.

For example for a given data (X,y) where X is a recieved email and y is 0 if email is spam and 1 if email is not spam.</description>
    </item>
    
    <item>
      <title>Multi Class Classification</title>
      <link>/courses/deeplearning/1.4/</link>
      <pubDate>Fri, 30 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/1.4/</guid>
      <description>Open in GitHub
Google Explore ML Academy Instructor: Shangeth Rajaa 
Solutions Here: 
 Open in GitHub
In the previous notebeook we used logistic regression for Binary Classification, now we will see how to train a classifier model for Multi-Class Classification.
What is Multi-Class Classification? If the target values have n discrete classification classes ie: y can take discrete value from 0 to n-1. If $y \in {0, 1, 2, 3, &amp;hellip;, n-1}$, then the classification task is n-Multi-Class.</description>
    </item>
    
    <item>
      <title>Motivation for Multi Layer Perceptron</title>
      <link>/courses/deeplearning/1.5/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/1.5/</guid>
      <description>Open in GitHub
Google Explore ML Academy Instructor: Shangeth Rajaa 
Non-Linearity Note: We will look mostly at classification examples, but the same concepts apply to regression problems as well with a little change in using activation function(Sigmoid, Softmax which we learned in previous sections).
So far the datasets we have used are linearly seperable, which means they can be seperated by line(2-d), plane(3-d) and linear multi dimensional classifiers.</description>
    </item>
    
    <item>
      <title>Neural Network Architectures</title>
      <link>/courses/deeplearning/2.1/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.1/</guid>
      <description>Open in GitHub
Google Explore ML Academy Instructor: Shangeth Rajaa 
We trained our first neural network in the previous notebook which had 3 layers
 Input Layer Hidden Layer Output Layer  Multiple Nodes The network had 2 linear layers($C_1$,0 $C_2$) in the hidden layer each of which gave us a linear classifier, then we used another linear layer($C_3$) in the output layer to combine $C_1$ and $C_2$ to give us a non-linear classifier($C$).</description>
    </item>
    
    <item>
      <title>Batch Training</title>
      <link>/courses/deeplearning/2.2/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.2/</guid>
      <description>Open in GitHub
Google Explore ML Academy Instructor: Shangeth Rajaa 
Batch Training Batch Training is something very important but we didn&amp;rsquo;t use before as our dataset was smaller and we were just learning how to train models.
So far,
 we took each example $(X^i, y^i)$ made prediction with $\hat{y}^i = g(X^i.W+b)$ calculated the loss $\mathcal{L}(y^i , \hat{y}^i)$ used back propagation to update $W$ and $b$ (all parameters in the model) with $w := w - \alpha \frac{\partial \mathcal{L}}{\partial w}$ and $b := b - \alpha \frac{\partial \mathcal{L}}{\partial b}$ and repeated this process.</description>
    </item>
    
    <item>
      <title>Optimizers</title>
      <link>/courses/deeplearning/2.3/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.3/</guid>
      <description>Open in GitHub
Google Explore ML Academy Instructor: Shangeth Rajaa 
Gradient Descent We have seen what are the steps followed in Gradient Descent to minimize the loss function. Now let&amp;rsquo;s get into more detail.
The objective of gradient descent is to find $W, b$ for which $\mathcal{L}(W, b) $ is minimum for given pairs of data $(X, y)$.
Example Let&amp;rsquo;s forget about Machine learning for sometime and use gradient descent algorithm for optimization of simple functions.</description>
    </item>
    
    <item>
      <title>Learning Rate</title>
      <link>/courses/deeplearning/2.4/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.4/</guid>
      <description>Open in GitHub
Google Explore ML Academy Instructor: Shangeth Rajaa 
Learning Rate In Gradient Descent we update the parameters of the model with
$w := w - \alpha \frac{\partial \mathcal{L}}{\partial w}$ and $b := b - \alpha \frac{\partial \mathcal{L}}{\partial b}$.
The learning rate $\alpha$ actually affects the learning process a lot.
 small $\alpha$ is slow, but more accuracte, as it does not miss the minimum, but it also get stuck in the local minimum.</description>
    </item>
    
    <item>
      <title>Bias &amp; Variance</title>
      <link>/courses/deeplearning/2.5/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.5/</guid>
      <description>Open in GitHub
Google Explore ML Academy Instructor: Shangeth Rajaa 
Bias &amp;amp; Variance Let us train a DNN model for a simple regression problem.
import numpy as np import matplotlib.pyplot as plt def dataset(show=True): X = np.arange(-5, 5, 0.01) y = 8 * np.sin(X) + np.random.randn(1000) if show: yy = 8 * np.sin(X) plt.figure(figsize=(15,9)) plt.scatter(X, y) plt.plot(X, yy, color=&#39;red&#39;, linewidth=7) plt.show() return X, y X, y = dataset(show=True)  Lets train 2 models for this dataset</description>
    </item>
    
    <item>
      <title>Overfitting &amp; Regularization</title>
      <link>/courses/deeplearning/2.6/</link>
      <pubDate>Fri, 06 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.6/</guid>
      <description>Open in GitHub
Google Explore ML Academy Instructor: Shangeth Rajaa 
MNIST Dataset The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.
Let&amp;rsquo;s get the dataset using tf.keras.datasets
Download MNIST import tensorflow as tf (x_train, y_train), (x_test, y_test) = tf.</description>
    </item>
    
    <item>
      <title>ANN - Medical Diagnosis</title>
      <link>/courses/deeplearning/2.7/</link>
      <pubDate>Fri, 06 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.7/</guid>
      <description>Open in GitHub
Google Explore ML Academy Instructor: Shangeth Rajaa 
We will use ANNs to diagnose Brease Cancer with some characteristics of the cell nuclei.
Dataset Download the Dataset We will use a breast cancer diagnosis dataset from Opeml.org
%%capture !wget https://www.openml.org/data/get_csv/5600/BNG_breast-w.arff  Explore the Dataset import pandas as pd df = pd.read_csv(&#39;/content/BNG_breast-w.arff&#39;) df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .</description>
    </item>
    
    <item>
      <title>ANN - Computer Vision</title>
      <link>/courses/deeplearning/2.8/</link>
      <pubDate>Fri, 06 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.8/</guid>
      <description>Open in GitHub
Google Explore ML Academy Instructor: Shangeth Rajaa 
We will use ANNs for a basic computer vision application of image classification on CIFAR10 Dataset
Dataset CIFAR-10 The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.
Download the Dataset Tensorflow has inbuilt dataset which makes it easy to get training and testing data.</description>
    </item>
    
    <item>
      <title>ANN - Natural Language Processing</title>
      <link>/courses/deeplearning/2.9/</link>
      <pubDate>Fri, 06 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/courses/deeplearning/2.9/</guid>
      <description>Google Explore ML Academy Instructor: Shangeth Rajaa 
Sentiment Analysis We are going to classify a movie review as Positive or Negative review given a text review.
We&amp;rsquo;ll use the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.</description>
    </item>
    
  </channel>
</rss>