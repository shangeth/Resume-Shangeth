<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shangeth</title>
    <link>/</link>
    <description>Recent content on Shangeth</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year} Shangeth Rajaa</copyright>
    <lastBuildDate>Fri, 30 Aug 2019 00:00:00 +0100</lastBuildDate>
    
	    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>/google-ml-academy/deeplearning/1.1/</link>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/google-ml-academy/deeplearning/1.1/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_1_Linear_Regression.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;a href=&#34;https://github.com/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_1_Linear_Regression.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;svg class=&#34;octicon octicon-mark-github v-align-middle&#34; height=&#34;30&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;30&#34; aria-hidden=&#34;true&#34;&gt;&lt;path fill-rule=&#34;evenodd&#34; d=&#34;M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z&#34;&gt;&lt;/path&gt;&lt;/svg&gt; Open in GitHub&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;&lt;a href=&#39;https://shangeth.com/google-ml-academy/&#39;&gt;Google ML Academy 2019&lt;/a&gt;&lt;/h1&gt;
&lt;h3&gt;Instructor: &lt;a href=&#39;https://shangeth.com/&#39;&gt;Shangeth Rajaa&lt;/a&gt;&lt;/h3&gt;
&lt;hr&gt;&lt;/p&gt;

&lt;p&gt;Before starting with Neural Networks, we will look into 2 important machine learning models to understand regression and classification tasks
- Linear Regression (Regression)
- Logistic Regression (Classification)&lt;/p&gt;

&lt;h1 id=&#34;linear-regression&#34;&gt;Linear Regression&lt;/h1&gt;

&lt;p&gt;Most of the Deep learning Courses do not start with linear regression(LinReg), but LinReg gave me a better understanding of machine learning, so i will start with that, hoping that will make the understanding of Neural networks easier.&lt;/p&gt;

&lt;p&gt;You can think of LinReg model as a curve fitting or function approximation model. Given a dataset $(X, y)$, the task is to find a relation $f$ between $X$ and $y$ such that $y = f(X)$. We are interested in this mapping $f: X \rightarrow y$, as for any given $X$ in the future we can find $y = f(X)$.&lt;/p&gt;

&lt;p&gt;For example, given a dataset about housing prices vs area of the house(Toy Dataset)&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;House Price($) y&lt;/th&gt;
&lt;th&gt;Area(1000 sqft) X&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1034&lt;/td&gt;
&lt;td&gt;2.4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1145&lt;/td&gt;
&lt;td&gt;2.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1252&lt;/td&gt;
&lt;td&gt;3.04&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2231&lt;/td&gt;
&lt;td&gt;4.67&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3423&lt;/td&gt;
&lt;td&gt;5.3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If we can find any the mapping between $X$ and $y$, $y = f(X)$, then its easy to predict the values of $y$ for any given $X$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# example dataset for linear regression
%matplotlib inline

import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-25, 25, 0.5)
y = 2 * x + 1 + np.random.randn(100)*5

plt.figure(figsize=(12,7))
plt.scatter(x, y, label=&#39;Data $(x,y)$&#39;)
plt.xlabel(&#39;$X$&#39;, fontsize=20)
plt.ylabel(&#39;$y$&#39;, fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_1/output_5_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So the prediction will be a line in case of 2-D data like above.&lt;/p&gt;

&lt;p&gt;The predicted model will be line $ y = m x + c $ or in ML world popularly $ y = w x + b $.&lt;/p&gt;

&lt;p&gt;$y = f(X) = wX+b$ will be the Linear regression model and the objective is to find the best $w$ and $b$ that will give the nearest values for each $(X,y=wX+b)$ pair in the dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# example dataset for linear regression

import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-25, 25, 0.5)
y = 2 * x + 1 + np.random.randn(100)*5

plt.figure(figsize=(12,7))
plt.scatter(x, y, label=&#39;Data $(x,y)$&#39;)
plt.plot(x, 2 * x + 1, label=&#39;Predicted Line $y = f(X)$&#39;, color=&#39;r&#39;,linewidth=4.0)
plt.xlabel(&#39;$X$&#39;, fontsize=20)
plt.ylabel(&#39;$y$&#39;, fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_1/output_7_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;what-about-multi-dimensional-data&#34;&gt;What about Multi Dimensional Data?&lt;/h1&gt;

&lt;p&gt;In real world, the dataset is going to be multi dimensional, ie: the price of a House will not just depend on the area of the house, it may also depend on multiple factors like no of rooms, locality, distance from airport, etc.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;House Price($) y&lt;/th&gt;
&lt;th&gt;Area(1000 sqft) x1&lt;/th&gt;
&lt;th&gt;No of Rooms x2&lt;/th&gt;
&lt;th&gt;Distance form airport x3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1034&lt;/td&gt;
&lt;td&gt;2.4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5.4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1145&lt;/td&gt;
&lt;td&gt;2.7&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3.1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1252&lt;/td&gt;
&lt;td&gt;3.04&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;4.21&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2231&lt;/td&gt;
&lt;td&gt;4.67&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2.3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3423&lt;/td&gt;
&lt;td&gt;5.3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In this case the model will be $y = f(X) = w_1x_1 + w_2x_2+w_3x_3+&amp;hellip;+w_nx_n + b$&lt;/p&gt;

&lt;p&gt;This can also be represented in matrix form as $y = f(X) =  X.W + b$&lt;/p&gt;

&lt;p&gt;$$X = \begin{bmatrix}
    x_1  &amp;amp; x_2 &amp;amp; x_3 &amp;amp; \dots &amp;amp; x_n &lt;br /&gt;
\end{bmatrix}$$&lt;/p&gt;

&lt;p&gt;$$W = \begin{bmatrix}x_1\\x_2\\ \vdots\\x_n\end{bmatrix}$$&lt;/p&gt;

&lt;p&gt;$$b = [b]$$&lt;/p&gt;

&lt;h1 id=&#34;how-good-is-the-model&#34;&gt;How good is the Model?&lt;/h1&gt;

&lt;p&gt;Before learning how to find $W$ and $b$ of a model. Let us assume we have a calculated values for optimal $W$ and $b$, how do we know how good is this model $\hat{y} = f(X) = X.W + b$.&lt;/p&gt;

&lt;p&gt;Notice it is not $y$, its $\hat{y}$.
Because W and b are not the exact values, they are calculated approximate values to get $f(X)$ close to $y$. So we represent the prediction as $\hat{y} = X.W + b$ and the true target as $y$.&lt;/p&gt;

&lt;p&gt;Our goal is to make $\hat{y}$ as close as possible to $y$.&lt;/p&gt;

&lt;p&gt;So we use a metric function call Mean Squared Error (MSE)&lt;/p&gt;

&lt;p&gt;$MSE(y, \hat{y}) = \mathcal{L}(y , \hat{y}) = \dfrac{1}{n} \sum_{i=1}^{i=n}{(y_i - \hat{y}_i)^2}$&lt;/p&gt;

&lt;p&gt;Why MSE? Why not just subtraction?
- model with 5 is better than -5 in sbtraction which actually is not both models are equally bad, but the predictions are in opposite direction.
- MSE takes care of -5 and 5 ; $(-5)^2$ = $5^2$ = 25.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

def MSE(y, y_hat):
  num_ex = len(y)
  mse_loss = np.sum((y - y_hat)**2)/num_ex
  return mse_loss
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = np.array([1.02, 2.3, 6.7, 3])
y_hat1 = np.array([10.43, 23.4, 12, 11])
y_hat2 = np.array([1, 1.9, 7, 3.1])

MSE(y,y), MSE(y, y_hat1), MSE(y, y_hat2)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(0.0, 156.46202499999998, 0.06509999999999996)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# example dataset for linear regression

import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-25, 25, 0.5)
y = 2 * x + 1 + np.random.randn(100)*5

w = -1
b = 0
y_hat = w * x + b
mse_loss = MSE(y, y_hat)

plt.figure(figsize=(12,7))
plt.scatter(x, y, label=&#39;Data $(x,y)$&#39;)
plt.plot(x, y_hat, label=&#39;Predicted Line $y = f(X)$&#39;, color=&#39;r&#39;,linewidth=4.0)
plt.text(0,-40,&#39;MSE = {:.3f}&#39;.format(mse_loss), fontsize=20)
plt.xlabel(&#39;$X$&#39;, fontsize=20)
plt.ylabel(&#39;$y$&#39;, fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_1/output_12_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# example dataset for linear regression

import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-25, 25, 0.5)
y = 2 * x + 1 + np.random.randn(100)*5

w = 1
b = 0.5
y_hat = w * x + b
mse_loss = MSE(y, y_hat)

plt.figure(figsize=(12,7))
plt.scatter(x, y, label=&#39;Data $(x,y)$&#39;)
plt.plot(x, y_hat, label=&#39;Predicted Line $y = f(X)$&#39;, color=&#39;r&#39;,linewidth=4.0)
plt.text(0,-40,&#39;MSE = {:.3f}&#39;.format(mse_loss), fontsize=20)
plt.xlabel(&#39;$X$&#39;, fontsize=20)
plt.ylabel(&#39;$y$&#39;, fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_1/output_13_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can evidently see that a better model have less MSE loss. So the object of finding $W$ and $b$ will be to reduce the MSE Loss.&lt;/p&gt;

&lt;p&gt;Its is not generally possible to get a MSE of 0 in real world data, as the real world data is always noisy and with outliers.&lt;/p&gt;

&lt;h1 id=&#34;how-to-find-the-best-model-w-b&#34;&gt;How to find the Best Model? (W, b)&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;randomly initialize W, b&lt;/li&gt;
&lt;li&gt;in loop for n steps/epochs{

&lt;ul&gt;
&lt;li&gt;find $\hat{y} = X.W + b$&lt;/li&gt;
&lt;li&gt;find $MSE = \mathcal{L}(y, \hat{y})$&lt;/li&gt;
&lt;li&gt;find $ \frac{\partial \mathcal{L}}{\partial w} $ and $ \frac{\partial \mathcal{L}}{\partial b} $&lt;/li&gt;
&lt;li&gt;Update W and b with $w := w - \alpha \frac{\partial \mathcal{L}}{\partial w}$ and $b := b - \alpha \frac{\partial \mathcal{L}}{\partial b}$
}&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is called Gradient Descent.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s try it first and see why it works.&lt;/p&gt;

&lt;p&gt;$\mathcal{L}= \dfrac{1}{n} \sum_{i=1}^{i=n}{(y_i - wx^i-b)^2} $&lt;/p&gt;

&lt;p&gt;$ \dfrac{\partial \mathcal{L}}{\partial w}  =  \dfrac{1}{n} \sum_{i=1}^{i=n}{ 2 (y_i - wx^i-b) (-x^i)} $&lt;/p&gt;

&lt;p&gt;$= \dfrac{2}{n} \sum_{i=1}^{i=n}{ (wx^i+b - y_i) (x^i)}$&lt;/p&gt;

&lt;p&gt;$ \dfrac{\partial \mathcal{L}}{\partial b}  =  \dfrac{1}{n} \sum_{i=1}^{i=n}{ 2 (y_i - wx^i-b) (-1)}$&lt;/p&gt;

&lt;p&gt;$= \dfrac{2}{n} \sum_{i=1}^{i=n}{ (wx^i+b - y_i)}$&lt;/p&gt;

&lt;p&gt;Therefore&lt;/p&gt;

&lt;p&gt;$w := w - \alpha  \dfrac{2}{n} (wX+b - y) (X)$&lt;/p&gt;

&lt;p&gt;$b := b - \alpha \dfrac{2}{n}   \sum_{i=1}^{i=n}{ (wX+b - y_i)}$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def model_forward(x, w, b):
  y_hat = w * x + b
  return y_hat
  
  
def gradient_descent(w, b, X, y, a):
  w = w - a * 2 / X.shape[0] * np.dot(X.T, model_forward(X, w, b)- y)
  b = b - a * 2 / X.shape[0] * np.sum(model_forward(X, w, b)- y)
  return w, b
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;w, b = np.random.random(1), np.random.random(1)

X = np.arange(-25, 25, 0.5)
y = 2 * x + 1 + np.random.randn(100)*5

y_hat = model_forward(X, w, b)
MSE(y, y_hat) # of randomly initialized model
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;831.0459153266066
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;losses = []

alpha = 0.00001

for i in range(1000):
  y_hat = model_forward(X, w, b)
  mse = MSE(y_hat, y)
  losses.append(mse)
  w, b = gradient_descent(w, b, X, y, alpha)
  
  if i%500 == 0:
    y_hat = model_forward(X, w, b)
    mse = MSE(y_hat, y)
    
    plt.figure(figsize=(12,7))
    plt.scatter(X, y, label=&#39;Data $(X, y)$&#39;)
    plt.plot(X, y_hat, color=&#39;red&#39;, label=&#39;Predicted Line $y = f(X)$&#39;,linewidth=4.0)
    plt.xlabel(&#39;$X$&#39;, fontsize=20)
    plt.ylabel(&#39;$y$&#39;, fontsize=20)
    plt.text(0,-30,&#39;Epoch = {}&#39;.format(i+1), fontsize=20)
    plt.text(0,-40,&#39;MSE = {:.3f}&#39;.format(mse), fontsize=20)
    plt.grid(True)
    plt.legend(fontsize=20)
    plt.show()

plt.plot(losses)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;MSE Loss&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_1/output_18_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../1_1/output_18_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../1_1/output_18_2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can see how the loss decreases and the model prediction becomes better with the number of epochs.&lt;/p&gt;

&lt;p&gt;Try to run the code with different alpha, different no of epochs and different initialization of w and b.&lt;/p&gt;

&lt;h1 id=&#34;linear-regression-in-tensorflow&#34;&gt;Linear Regression in TensorFlow&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np

def MSE(y, y_hat):
  num_ex = len(y)
  mse_loss = np.sum((y - y_hat)**2)/num_ex
  return mse_loss


X = np.arange(-25, 25, 0.5).astype(&#39;float32&#39;)
y = (2 * X + 1 + np.random.randn(100)*5).astype(&#39;float32&#39;)

model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)
tf_history = model.fit(X, y, epochs=1000, verbose=False)

plt.plot(tf_history.history[&#39;loss&#39;])
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;MSE Loss&#39;)
plt.show()

mse = tf_history.history[&#39;loss&#39;][-1]
y_hat = model.predict(X)

plt.figure(figsize=(12,7))
plt.title(&#39;TensorFlow Model&#39;)
plt.scatter(X, y, label=&#39;Data $(X, y)$&#39;)
plt.plot(X, y_hat, color=&#39;red&#39;, label=&#39;Predicted Line $y = f(X)$&#39;,linewidth=4.0)
plt.xlabel(&#39;$X$&#39;, fontsize=20)
plt.ylabel(&#39;$y$&#39;, fontsize=20)
plt.text(0,-40,&#39;MSE = {:.3f}&#39;.format(mse), fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_1/output_21_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../1_1/output_21_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Lets look into each line
- &lt;code&gt;model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    Sequential is like a container or wrapper which holds all the operations to be performed on your input.
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Compile configures the training process. It defines the metrics, optimizers,..etc. You will understand more about this as we go on with more examples.
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;tf_history = model.fit(x, y, epochs=1000, verbose=False)&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model.fit actually trains the model for given number of epochs. Note: i&#39;ve used Verbose=False(as the output will be long), set it to true to check the loss and metrics for each epoch.
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Just 3-4 line of Tensorflow code can train a ml model, this is the advantage of using packages like Tensorflow/PyTorch. These packages are best optimized for speed and performance.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.summary()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Model: &amp;quot;sequential&amp;quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 1)                 2         
=================================================================
Total params: 2
Trainable params: 2
Non-trainable params: 0
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tf.keras.Sequential? # if you are not sure of any library, its better to look into the docs.
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/google-ml-academy/deeplearning/1.2/</link>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/google-ml-academy/deeplearning/1.2/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_2_1_Assignment_Polynomial_Regression.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;&lt;/a&gt;
&lt;center&gt;&lt;a href=&#34;https://github.com/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_2_1_Assignment_Polynomial_Regression.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;svg class=&#34;octicon octicon-mark-github v-align-middle&#34; height=&#34;30&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;30&#34; aria-hidden=&#34;true&#34;&gt;&lt;path fill-rule=&#34;evenodd&#34; d=&#34;M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z&#34;&gt;&lt;/path&gt;&lt;/svg&gt; Open in GitHub&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;&lt;a href=&#39;https://shangeth.com/google-ml-academy/&#39;&gt;Google ML Academy 2019&lt;/a&gt;&lt;/h1&gt;
&lt;h3&gt;Instructor: &lt;a href=&#39;https://shangeth.com/&#39;&gt;Shangeth Rajaa&lt;/a&gt;&lt;/h3&gt;
&lt;hr&gt;&lt;/p&gt;

&lt;p&gt;Solutions Here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_2_2_Assignment_Polynomial_Regression_Solution.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;a href=&#34;https://github.com/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_2_2_Assignment_Polynomial_Regression_Solution.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;svg class=&#34;octicon octicon-mark-github v-align-middle&#34; height=&#34;30&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;30&#34; aria-hidden=&#34;true&#34;&gt;&lt;path fill-rule=&#34;evenodd&#34; d=&#34;M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z&#34;&gt;&lt;/path&gt;&lt;/svg&gt; Open in GitHub&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Its a very simple assignment, you can finish it in less than 10 minutes. If you are stuck somewhere refer this for solutions.&lt;/p&gt;

&lt;h1 id=&#34;task-1-linear-regression-on-non-linear-data&#34;&gt;Task-1 : Linear Regression on Non-Linear Data&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Get X and y from dataset() function&lt;/li&gt;
&lt;li&gt;Train a Linear Regression model for this dataset.&lt;/li&gt;
&lt;li&gt;Visualize the model prediction&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;

&lt;p&gt;Call &lt;code&gt;dataset()&lt;/code&gt; function to get X, y&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import matplotlib.pyplot as plt

def dataset(show=True):
    X = np.arange(-25, 25, 0.1)
    # Try changing y to a different function
    y = X**3 + 20 + np.random.randn(500)*1000
    if show:
        plt.scatter(X, y)
        plt.show()
    return X, y

X, y = dataset()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_2/output_4_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;scaling-dataset&#34;&gt;Scaling Dataset&lt;/h2&gt;

&lt;p&gt;The maximum value of y in the dataset goes upto 15000 and the minimum values is less than -15000. The range of y is very large which makes the convergence/loss reduction slower. So will we scale the data, scaling the data will help the model converge faster. If all the features and target are in same range, there will be symmetry in the curve of Loss vs weights/bias, which makes the convergence faster.&lt;/p&gt;

&lt;p&gt;We will do a very simple type of scaling, we will divide all the values of the data with the maximum values for X and y respectively.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X, y = dataset()

print(max(X), max(y), min(X), min(y))

X = X/max(X)
y = y/max(y)

print(max(X), max(y), min(X), min(y))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_2/output_6_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;24.90000000000071 16694.307606867886 -25.0 -16126.103960535462
1.0 1.0 -1.0040160642569995 -0.9659642280642613
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is not a great scaling method, but good to start. We will see many more scaling/normalizing methods later.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Try training the model with and without scaling and see the difference yourself.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;linear-regression-in-tensorflow&#34;&gt;Linear Regression in TensorFlow&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np

X, y = dataset(show=False)
X_scaled = X/max(X)
y_scaled = y/max(y)

model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])

# you can also define optimizers in this way, so you can change parameters like lr.
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(optimizer=optimizer, loss=&#39;mean_squared_error&#39;)
tf_history = model.fit(X_scaled, y_scaled, epochs=500, verbose=True)

plt.plot(tf_history.history[&#39;loss&#39;])
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;MSE Loss&#39;)
plt.show()

mse = tf_history.history[&#39;loss&#39;][-1]
y_hat = model.predict(X_scaled)

plt.figure(figsize=(12,7))
plt.title(&#39;TensorFlow Model&#39;)
plt.scatter(X_scaled, y_scaled, label=&#39;Data $(X, y)$&#39;)
plt.plot(X_scaled, y_hat, color=&#39;red&#39;, label=&#39;Predicted Line $y = f(X)$&#39;,linewidth=4.0)
plt.xlabel(&#39;$X$&#39;, fontsize=20)
plt.ylabel(&#39;$y$&#39;, fontsize=20)
plt.text(0,0.70,&#39;MSE = {:.3f}&#39;.format(mse), fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;WARNING: Logging before flag parsing goes to stderr.
W0829 03:45:55.755687 139671975163776 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor


Epoch 1/500
500/500 [==============================] - 0s 951us/sample - loss: 0.2951
Epoch 2/500
500/500 [==============================] - 0s 41us/sample - loss: 0.2856
.
.
Epoch 499/500
500/500 [==============================] - 0s 41us/sample - loss: 0.0264
Epoch 500/500
500/500 [==============================] - 0s 39us/sample - loss: 0.0263
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_2/output_9_2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../1_2/output_9_3.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Looks the model Prediction for this dataset is not very great, but that is expected as the model is a straight line, it cannot predict non linear regression data.  Is there a way to train a regression model for this task?&lt;/p&gt;

&lt;h1 id=&#34;polynomial-regression&#34;&gt;Polynomial Regression&lt;/h1&gt;

&lt;p&gt;So when the dataset is not linear, linear regression cannot learn the dataset and make good predictions.&lt;/p&gt;

&lt;p&gt;So we need a polynomial model which consideres the polynomial terms as well. So we need terms like $x^2$, $x^3$, &amp;hellip;, $x^n$ for the model to learn a polynomial of $n^{th}$ degree.&lt;/p&gt;

&lt;p&gt;$\hat{y} = w_0 + w_1x + w_2x^2 + &amp;hellip; + w_nx^n$&lt;/p&gt;

&lt;p&gt;One down side of this model is that, We will have to decide the value of n. But this is better than a linear regression model. We can get an idea of the value of n by visualizing a dataset, but for multi variable dataset, we will have to try different values of n and check which is better.&lt;/p&gt;

&lt;h2 id=&#34;polynomial-features&#34;&gt;Polynomial Features&lt;/h2&gt;

&lt;p&gt;you can calculate the polynomial features for each feature by programming it or you can try &lt;code&gt;sklearn.preprocessing.PolynomialFeatures&lt;/code&gt; which allows us to make polynomial terms of our data.&lt;/p&gt;

&lt;p&gt;We will try degree 2, 3 and 4&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X, y = dataset(show=False)
X_scaled = X/max(X)
y_scaled = y/max(y)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2) 
                                        
X_2 = poly.fit_transform(X_scaled.reshape(-1,1))
print(X_2.shape)
print(X_2[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(500, 3)
[ 1.         -1.00401606  1.00804826]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=3) 
                                        
X_3 = poly.fit_transform(X_scaled.reshape(-1,1))
print(X_3.shape)
print(X_3[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(500, 4)
[ 1.         -1.00401606  1.00804826 -1.01209664]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=4) 
                                        
X_4 = poly.fit_transform(X_scaled.reshape(-1,1))
print(X_4.shape)
print(X_4[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(500, 5)
[ 1.         -1.00401606  1.00804826 -1.01209664  1.01616129]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The PolynomialFeatures returns $[1, x, x^2, x^3,&amp;hellip;]$.&lt;/p&gt;

&lt;h1 id=&#34;task-2&#34;&gt;Task - 2&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Train a model with polynomial terms in the dataset.&lt;/li&gt;
&lt;li&gt;Visualize the prediction of the model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The code remains the same except, the no of input features will be 3, 4, 5 respectively.&lt;/p&gt;

&lt;h2 id=&#34;tensorflow-model-with-2nd-degree&#34;&gt;Tensorflow Model with 2nd Degree&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[3])])

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

model.compile(optimizer=optimizer, loss=&#39;mean_squared_error&#39;)
tf_history = model.fit(X_2, y_scaled, epochs=500, verbose=True)

plt.plot(tf_history.history[&#39;loss&#39;])
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;MSE Loss&#39;)
plt.show()

mse = tf_history.history[&#39;loss&#39;][-1]
y_hat = model.predict(X_2)

plt.figure(figsize=(12,7))
plt.title(&#39;TensorFlow Model&#39;)
plt.scatter(X_2[:, 1], y_scaled, label=&#39;Data $(X, y)$&#39;)
plt.plot(X_2[:, 1], y_hat, color=&#39;red&#39;, label=&#39;Predicted Line $y = f(X)$&#39;,linewidth=4.0)
plt.xlabel(&#39;$X$&#39;, fontsize=20)
plt.ylabel(&#39;$y$&#39;, fontsize=20)
plt.text(0,0.70,&#39;MSE = {:.3f}&#39;.format(mse), fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1/500
500/500 [==============================] - 0s 332us/sample - loss: 0.5278
Epoch 2/500
500/500 [==============================] - 0s 38us/sample - loss: 0.4773
.
.
Epoch 499/500
500/500 [==============================] - 0s 44us/sample - loss: 0.0242
Epoch 500/500
500/500 [==============================] - 0s 43us/sample - loss: 0.0242
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_2/output_20_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../1_2/output_20_2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Why is the polynomial regression with 2-d features look like a straight line?&lt;/p&gt;

&lt;p&gt;Well because the model thinks that a straight line(look like, we can&amp;rsquo;t be sure if its a straight like, it can a parabola as well) better fits the dataset than a parabola. If you train the model for less epochs you can notice the model tries to fit the data with a parabola(2-d) but it improves as it moves to a line.&lt;/p&gt;

&lt;p&gt;Train the same model for may be 50 epochs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[3])])

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

model.compile(optimizer=optimizer, loss=&#39;mean_squared_error&#39;)
tf_history = model.fit(X_2, y_scaled, epochs=50, verbose=True)

plt.plot(tf_history.history[&#39;loss&#39;])
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;MSE Loss&#39;)
plt.show()

mse = tf_history.history[&#39;loss&#39;][-1]
y_hat = model.predict(X_2)

plt.figure(figsize=(12,7))
plt.title(&#39;TensorFlow Model&#39;)
plt.scatter(X_2[:, 1], y_scaled, label=&#39;Data $(X, y)$&#39;)
plt.plot(X_2[:, 1], y_hat, color=&#39;red&#39;, label=&#39;Predicted Line $y = f(X)$&#39;,linewidth=4.0)
plt.xlabel(&#39;$X$&#39;, fontsize=20)
plt.ylabel(&#39;$y$&#39;, fontsize=20)
plt.text(0,0.70,&#39;MSE = {:.3f}&#39;.format(mse), fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1/50
500/500 [==============================] - 0s 370us/sample - loss: 0.8566
Epoch 2/50
500/500 [==============================] - 0s 38us/sample - loss: 0.7970
.
.
Epoch 49/50
500/500 [==============================] - 0s 38us/sample - loss: 0.1027
Epoch 50/50
500/500 [==============================] - 0s 35us/sample - loss: 0.1000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_2/output_22_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../1_2/output_22_2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can clearly see that the model tries to fit the data with a parabola which doen&amp;rsquo;t seem to fit well, so it changes the parameters to fit a line.&lt;/p&gt;

&lt;h2 id=&#34;tensorflow-model-with-3rd-degree&#34;&gt;Tensorflow Model with 3rd Degree&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[4])])

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

model.compile(optimizer=optimizer, loss=&#39;mean_squared_error&#39;)
tf_history = model.fit(X_3, y_scaled, epochs=500, verbose=True)

plt.plot(tf_history.history[&#39;loss&#39;])
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;MSE Loss&#39;)
plt.show()

mse = tf_history.history[&#39;loss&#39;][-1]
y_hat = model.predict(X_3)

plt.figure(figsize=(12,7))
plt.title(&#39;TensorFlow Model&#39;)
plt.scatter(X_3[:, 1], y_scaled, label=&#39;Data $(X, y)$&#39;)
plt.plot(X_3[:, 1], y_hat, color=&#39;red&#39;, label=&#39;Predicted Line $y = f(X)$&#39;,linewidth=4.0)
plt.xlabel(&#39;$X$&#39;, fontsize=20)
plt.ylabel(&#39;$y$&#39;, fontsize=20)
plt.text(0,0.70,&#39;MSE = {:.3f}&#39;.format(mse), fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1/500
500/500 [==============================] - 0s 456us/sample - loss: 0.4445
Epoch 2/500
500/500 [==============================] - 0s 40us/sample - loss: 0.3993
.
.
Epoch 499/500
500/500 [==============================] - 0s 38us/sample - loss: 0.0040
Epoch 500/500
500/500 [==============================] - 0s 37us/sample - loss: 0.0039
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_2/output_25_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../1_2/output_25_2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;3-D features perfectly fit the data with a 3rd degree polynomial as expected.&lt;/p&gt;

&lt;h2 id=&#34;tensorflow-model-with-4th-degree&#34;&gt;Tensorflow Model with 4th Degree&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[5])])

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

model.compile(optimizer=optimizer, loss=&#39;mean_squared_error&#39;)
tf_history = model.fit(X_4, y_scaled, epochs=500, verbose=True)

plt.plot(tf_history.history[&#39;loss&#39;])
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;MSE Loss&#39;)
plt.show()

mse = tf_history.history[&#39;loss&#39;][-1]
y_hat = model.predict(X_4)

plt.figure(figsize=(12,7))
plt.title(&#39;TensorFlow Model&#39;)
plt.scatter(X_4[:, 1], y_scaled, label=&#39;Data $(X, y)$&#39;)
plt.plot(X_4[:, 1], y_hat, color=&#39;red&#39;, label=&#39;Predicted Line $y = f(X)$&#39;,linewidth=4.0)
plt.xlabel(&#39;$X$&#39;, fontsize=20)
plt.ylabel(&#39;$y$&#39;, fontsize=20)
plt.text(0,0.70,&#39;MSE = {:.3f}&#39;.format(mse), fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1/500
500/500 [==============================] - 0s 240us/sample - loss: 0.5839
Epoch 2/500
500/500 [==============================] - 0s 37us/sample - loss: 0.5453
.
.
Epoch 499/500
500/500 [==============================] - 0s 37us/sample - loss: 0.0040
Epoch 500/500
500/500 [==============================] - 0s 39us/sample - loss: 0.0040
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_2/output_28_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../1_2/output_28_2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4th degree poly-regression also did a good job in fitting the data as it also have the 3rd degree terms.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[5])])

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

model.compile(optimizer=optimizer, loss=&#39;mean_squared_error&#39;)
tf_history = model.fit(X_4, y_scaled, epochs=50, verbose=True)

plt.plot(tf_history.history[&#39;loss&#39;])
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;MSE Loss&#39;)
plt.show()

mse = tf_history.history[&#39;loss&#39;][-1]
y_hat = model.predict(X_4)

plt.figure(figsize=(12,7))
plt.title(&#39;TensorFlow Model&#39;)
plt.scatter(X_4[:, 1], y_scaled, label=&#39;Data $(X, y)$&#39;)
plt.plot(X_4[:, 1], y_hat, color=&#39;red&#39;, label=&#39;Predicted Line $y = f(X)$&#39;,linewidth=4.0)
plt.xlabel(&#39;$X$&#39;, fontsize=20)
plt.ylabel(&#39;$y$&#39;, fontsize=20)
plt.text(0,0.70,&#39;MSE = {:.3f}&#39;.format(mse), fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1/50
500/500 [==============================] - 0s 181us/sample - loss: 0.6724
Epoch 2/50
500/500 [==============================] - 0s 35us/sample - loss: 0.6104
.
.
Epoch 48/50
500/500 [==============================] - 0s 34us/sample - loss: 0.0661
Epoch 49/50
500/500 [==============================] - 0s 35us/sample - loss: 0.0655
Epoch 50/50
500/500 [==============================] - 0s 38us/sample - loss: 0.0648
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_2/output_30_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../1_2/output_30_2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you run the 4th degree poly-regression for fewer epochs, you can notice, the model tries to fit a 4th(or higher than 3rd) degree polynomial but as the loss is high, the model changes it parameters to set the 4th degree terms to almost 0 and thus giving a 3rd degree polynomial as you train for more epochs.&lt;/p&gt;

&lt;p&gt;This is polynomial regression. Yes, its easy. But one issue, as this was a toy dataset we know its a 3rd degree data, so we tried 2,3,4. But when the data is multi dimensional we cannot visualize the dataset, so its difficult to decide the degree. This is why you will see Neural Networks are awesome. They are End-End, they do not need several feature extraction from our side, they can extract necessary features of their own.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Make a Higher degree (4th/5th degree) data and try polynomial regression on it. Also try different functions like exponents, trignometric..etc.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/google-ml-academy/deeplearning/1.3/</link>
      <pubDate>Fri, 30 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/google-ml-academy/deeplearning/1.3/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_3_Logistic_Regression.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;a href=&#34;https://github.com/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_3_Logistic_Regression.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;svg class=&#34;octicon octicon-mark-github v-align-middle&#34; height=&#34;30&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;30&#34; aria-hidden=&#34;true&#34;&gt;&lt;path fill-rule=&#34;evenodd&#34; d=&#34;M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z&#34;&gt;&lt;/path&gt;&lt;/svg&gt; Open in GitHub&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;&lt;a href=&#39;https://shangeth.com/google-ml-academy/&#39;&gt;Google ML Academy 2019&lt;/a&gt;&lt;/h1&gt;
&lt;h3&gt;Instructor: &lt;a href=&#39;https://shangeth.com/&#39;&gt;Shangeth Rajaa&lt;/a&gt;&lt;/h3&gt;
&lt;hr&gt;&lt;/p&gt;

&lt;h1 id=&#34;logistic-regression&#34;&gt;Logistic Regression&lt;/h1&gt;

&lt;p&gt;Logistic Regression is one of the most commonly used classification model. Unlike Linear Regression which predicts a real unbounded value $\hat{y} = f(X) = WX+b$, Logistic Regression predicts the probability of a data belonging to a particular class.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;For example for a given data (X,y) where X is a recieved email and y is 0 if email is spam and 1 if email is not spam. Logistic regression predicts the probability of the email X to be not spam.
$\hat{y} = f(X) = P(y=1|X)$&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;So if $\hat{y} = P(y=1|X) &amp;gt; 0.5$ then the probability of the email to be spam ih high, so $X$ is a spam email.&lt;/p&gt;

&lt;p&gt;Other examples of classification can be&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Classification of image as cat, dog, parrot $ y = {0, 1, 2}$. Here y can be 0 or 1 or 2 depending on the probability of model prediction. (Multi class Classification)&lt;/li&gt;
&lt;li&gt;Classification of Cancer report as Malignant/Benign $y = {0, 1}$. (Binary Classification)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;why-not-use-linear-regression-model-for-classification&#34;&gt;Why not use Linear Regression Model for classification?&lt;/h2&gt;

&lt;p&gt;$\hat{y}_{linreg} = WX+b$ where $\hat{y} \in \mathbf{R} $, so the prediction can take value from $-\infty$ to $\infty$.&lt;/p&gt;

&lt;p&gt;$\hat{y}_{classification} \in { 0, 1, 2, &amp;hellip;, n }$, Classification prediction takes discrete values depening on number of class.&lt;/p&gt;

&lt;p&gt;So we need a model which limits the prediction in the range ${0,1}$ for binary classification and ${0,1, 2, &amp;hellip;, n}$ for multi-class classification.&lt;/p&gt;

&lt;h2 id=&#34;classification-data&#34;&gt;Classification Data&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s use &lt;code&gt;sklearn.datasets.make_blobs&lt;/code&gt; to make a random classification dataset in 2D space so we can visualize it.&lt;/p&gt;

&lt;p&gt;We are generating less data for visualization, for training we will use more data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.datasets import make_blobs

# 10 examples, (X)2 feature, (y)2 classes
X, y = make_blobs(n_samples=10, n_features=2, centers=2, random_state=0)

X.shape, y.shape
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;((10, 2), (10,))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s seperate the 0 and 1 class to visualize it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

class_0 = np.where(y == 0)
class_1 = np.where(y == 1)

X_0 = X[class_0]
X_1 = X[class_1]

X_0.shape, X_1.shape
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;((5, 2), (5, 2))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt


plt.figure(figsize=(12,9))
plt.scatter(X_0[:, 0], X_0[:, 1], marker=&#39;o&#39;, s=150, color=&#39;blue&#39;, label=&#39;1&#39;)
plt.scatter(X_1[:, 0], X_1[:, 1], marker=&#39;x&#39;, s=150, color=&#39;red&#39;, label=&#39;0&#39;)
plt.xlabel(&#39;$x1$&#39;, fontsize=20)
plt.ylabel(&#39;$x2$&#39;, fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_3/output_8_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;One possible linear classifier for this dataset can be&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt

x = np.arange(-1, 5, 0.5)
y = 3 - 0.5 * x


plt.figure(figsize=(12,9))
plt.scatter(X_0[:, 0], X_0[:, 1], marker=&#39;o&#39;, s=150, color=&#39;blue&#39;, label=&#39;1&#39;)
plt.scatter(X_1[:, 0], X_1[:, 1], marker=&#39;x&#39;, s=150, color=&#39;red&#39;, label=&#39;0&#39;)
plt.plot(x, y, label=&#39;Classifier&#39;, color=&#39;black&#39;)
plt.xlabel(&#39;$x1$&#39;, fontsize=20)
plt.ylabel(&#39;$x2$&#39;, fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_3/output_10_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let us consider the line $y = 3-x/2$ is the best classifier which seperates the data.&lt;/p&gt;

&lt;p&gt;Let $C(x,y) = y + x/2-3 $.&lt;/p&gt;

&lt;p&gt;By basic school geometry, we know points in opposite side of a line $C(x,y)$ will give opposite values on $C(x,y)$.&lt;/p&gt;

&lt;p&gt;ie: $C(x_1,y_1).C(x_2,y_2) &amp;lt; 0$, then $(x_1, y_1)$ and $(x_2, y_2)$ lies in the opposite side of $C(x,y)$.&lt;/p&gt;

&lt;p&gt;Why are we even talking about this property?
Well, this can tell something about how good a classifier is.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Let us take 2 classifiers

&lt;ul&gt;
&lt;li&gt;one which classifies all points correctly&lt;/li&gt;
&lt;li&gt;one which misclassifies few points&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;calculate the $C(x,y)$ for every point and check how its different for both classifiers.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt

x = np.arange(-1, 5, 0.5)
y = 3 - 0.5 * x


plt.figure(figsize=(12,9))
plt.scatter(X_0[:, 0], X_0[:, 1], marker=&#39;o&#39;, s=150, color=&#39;blue&#39;, label=&#39;1&#39;)
plt.scatter(X_1[:, 0], X_1[:, 1], marker=&#39;x&#39;, s=150, color=&#39;red&#39;, label=&#39;0&#39;)
plt.plot(x, y, label=&#39;Classifier&#39;, color=&#39;black&#39;)

for i in range(len(X_0)):
  plt.text(X_0[i, 0]+0.1, X_0[i, 1], &#39;$C = {:.3f}$&#39;.format(X_0[i, 0]/2 + X_0[i, 1]- 3), fontsize=20)

for i in range(len(X_1)):
  plt.text(X_1[i, 0]+0.1, X_1[i, 1], &#39;$C = {:.3f}$&#39;.format(X_1[i, 0]/2 + X_1[i, 1]- 3), fontsize=20)


plt.xlabel(&#39;$x1$&#39;, fontsize=20)
plt.ylabel(&#39;$x2$&#39;, fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_3/output_12_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s try with a bad classifier&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt

x = np.arange(-1, 5, 0.5)
y = 3 -   x


plt.figure(figsize=(12,9))
plt.scatter(X_0[:, 0], X_0[:, 1], marker=&#39;o&#39;, s=150, color=&#39;blue&#39;, label=&#39;1&#39;)
plt.scatter(X_1[:, 0], X_1[:, 1], marker=&#39;x&#39;, s=150, color=&#39;red&#39;, label=&#39;0&#39;)
plt.plot(x, y, label=&#39;Classifier&#39;, color=&#39;black&#39;)

for i in range(len(X_0)):
  plt.text(X_0[i, 0]+0.1, X_0[i, 1], &#39;$C = {:.3f}$&#39;.format(X_0[i, 0] + X_0[i, 1]- 3), fontsize=20)

for i in range(len(X_1)):
  plt.text(X_1[i, 0]+0.1, X_1[i, 1], &#39;$C = {:.3f}$&#39;.format(X_1[i, 0] + X_1[i, 1]- 3), fontsize=20)


plt.xlabel(&#39;$x1$&#39;, fontsize=20)
plt.ylabel(&#39;$x2$&#39;, fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_3/output_14_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can see, the same class points have same sign when the classifier is good. So this metric C is actually a good way to find how good a classifier is.&lt;/p&gt;

&lt;p&gt;We would like to know the probability of a point to be class 1. $P(y=1|X)$&lt;/p&gt;

&lt;p&gt;We can do this by converting $C(x,y)$ into a range $[0,1]$ using a function called Sigmoid/Logistic.&lt;/p&gt;

&lt;h2 id=&#34;sigmoid-logistic-function&#34;&gt;Sigmoid/Logistic Function&lt;/h2&gt;

&lt;h2 id=&#34;g-x-dfrac-1-1-e-x&#34;&gt;$g(x) = \dfrac{1}{1+e^{-x}}$&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#39;https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png&#39; height=&#34;300&#34; width=&#34;400&#34;&gt;&lt;/p&gt;

&lt;p&gt;Sigmoid can convert a number in Real range to $[0, 1]$ which is what we need to convert the score $C(x,y)$ to probability $P(y=1|X)$.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s code sigmoid in Numpy.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

def sigmoid(x):
  return 1/(1+np.exp(-x))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = np.array([-200,980, 0.1, -23, 1e-3])

sigmoid(a)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;array([1.38389653e-87, 1.00000000e+00, 5.24979187e-01, 1.02618796e-10,
       5.00250000e-01])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sigmoid converted all the number from range 980 to -200 into a range of [0,1].&lt;/p&gt;

&lt;h2 id=&#34;probabilities-with-sigmoid&#34;&gt;Probabilities with Sigmoid&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt

x = np.arange(-1, 5, 0.5)
y = 3 - 0.5 * x


plt.figure(figsize=(12,9))
plt.scatter(X_0[:, 0], X_0[:, 1], marker=&#39;o&#39;, s=150, color=&#39;blue&#39;, label=&#39;1&#39;)
plt.scatter(X_1[:, 0], X_1[:, 1], marker=&#39;x&#39;, s=150, color=&#39;red&#39;, label=&#39;0&#39;)
plt.plot(x, y, label=&#39;Classifier&#39;, color=&#39;black&#39;)

for i in range(len(X_0)):
  plt.text(X_0[i, 0]+0.1, X_0[i, 1], &#39;$\hat{{y}} = {:.3f}$&#39;.format(sigmoid(X_0[i, 0]/2 + X_0[i, 1]- 3)), fontsize=20)

for i in range(len(X_1)):
  plt.text(X_1[i, 0]+0.1, X_1[i, 1], &#39;$\hat{{y}} = {:.3f}$&#39;.format(sigmoid(X_1[i, 0]/2 + X_1[i, 1]- 3)), fontsize=20)


plt.xlabel(&#39;$x1$&#39;, fontsize=20)
plt.ylabel(&#39;$x2$&#39;, fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_3/output_21_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So sigmoid gives the probability of the point to be class &amp;lsquo;1&amp;rsquo; ie: $P(y=1|X)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note: this can also be $\hat{y} = P(y=0|X)$, it depends on how you define 1 and 0 class. you can define it either way, but usually we use $\hat{y} = P(y=1|X)$.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When $\hat{y} &amp;gt; 0.5$ we classify it as class &amp;ldquo;1&amp;rdquo; and when $\hat{y} &amp;lt;= 0.5$ we classify it as class &amp;ldquo;0&amp;rdquo;.&lt;/p&gt;

&lt;h2 id=&#34;how-to-compare-the-models&#34;&gt;How to compare the models?&lt;/h2&gt;

&lt;p&gt;We still didn&amp;rsquo;t learn about how to find the best model. But let&amp;rsquo;s say we have 2 models, how do we compare which one is the best?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;By comparing the $\hat{y}$(prediction of the model) and $y$(true label) of each data.&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt

x = np.arange(-1, 5, 0.5)
y = 3 - 0.5 * x


plt.figure(figsize=(12,9))
plt.scatter(X_0[:, 0], X_0[:, 1], marker=&#39;o&#39;, s=150, color=&#39;blue&#39;, label=&#39;1&#39;)
plt.scatter(X_1[:, 0], X_1[:, 1], marker=&#39;x&#39;, s=150, color=&#39;red&#39;, label=&#39;0&#39;)
plt.plot(x, y, label=&#39;Classifier&#39;, color=&#39;black&#39;)

for i in range(len(X_0)):
  plt.text(X_0[i, 0]+0.1, X_0[i, 1]-0.2, &#39;$\hat{{y}} = {:.3f}$\n$y=1$&#39;.format(sigmoid(X_0[i, 0]/2 + X_0[i, 1]- 3)), fontsize=20)

for i in range(len(X_1)):
  plt.text(X_1[i, 0]+0.1, X_1[i, 1]-0.2, &#39;$\hat{{y}} = {:.3f}$\n$y=0$&#39;.format(sigmoid(X_1[i, 0]/2 + X_1[i, 1]- 3)), fontsize=20)


plt.xlabel(&#39;$x1$&#39;, fontsize=20)
plt.ylabel(&#39;$x2$&#39;, fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_3/output_24_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;maximum-likelihood&#34;&gt;Maximum Likelihood&lt;/h2&gt;

&lt;p&gt;$\hat{y} = g(W.X+b)$ gives the probability of $X$ belonging to class ${1}$. We want every point to have maximum predicted probability of that point to have its true label.ie:
- if true label $y=1$, then we want to maximize $\hat{y}_1 = \hat{y} = g(W.X+b)$.
- if true label $y=0$, then we want to maximize $\hat{y}_0 = 1 - \hat{y} = 1 - g(W.X+b)$.&lt;/p&gt;

&lt;p&gt;As a point can have only 2 option either ${0, 1}$. so $\hat{y}_0 + \hat{y}_1 = 1$.&lt;/p&gt;

&lt;h3 id=&#34;likelihood&#34;&gt;Likelihood&lt;/h3&gt;

&lt;p&gt;Product of predicted probabilities of every point to have its true label.&lt;/p&gt;

&lt;p&gt;$L = \prod_{i=1}^{m}{P(\hat{y}^i|X^i)}$&lt;/p&gt;

&lt;p&gt;where $\hat{y}^i$ means $i^{th}$ data prediction and $X^i$ means $i^{th}$ data.&lt;/p&gt;

&lt;p&gt;So the objective of any classification model in Machine Learning is to maximize this Likelihood $L$ thus its called Maximum Likelihood.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take an example to calculate the Maximum Likelihood.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

y_hat = np.array([0.94, 0.0271, 0.43, 0.56, 0.012])

MaxL = 1
for i in range(len(y_hat)):
    MaxL *= y_hat[i]

MaxL
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;7.360967039999999e-05
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The maximum Likelihood of just 5 numbers goes to an order of $10^{-5}$, in real dataset we will have thousands, sometimes millions of data which will give a MaximumLikelihood beyond the range of computation.&lt;/p&gt;

&lt;p&gt;So we use the property of logarithm $log(a.b) = log(a) + log(b)$ to make this multiplication to addition, so it remains in the range of computation.&lt;/p&gt;

&lt;p&gt;$Log(MaxL) = log(\prod_{i=1}^{m}{P(\hat{y}^i|X^i))}$&lt;/p&gt;

&lt;p&gt;$LogLikelihood = \sum_{i=1}^{m}{log(P(\hat{y}^i|X^i))}$&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s try this Log likelihood with Numpy&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

y_hat = np.array([0.94, 0.0271, 0.43, 0.56, 0.012])

LogL = 1
for i in range(len(y_hat)):
    LogL += np.log(y_hat[i])

LogL
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;-8.516734149556177
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This number is can be used in computation easily and you can observe for any dataset Log likelihood will be in a good range of computation and it will be a negative number as log of any number less than one is negative.&lt;/p&gt;

&lt;p&gt;So we introduce a negative sign to make it positive. Why? We would like to make this problem to find minimum loss, optimization is relatively easier for convex functions than concave.&lt;/p&gt;

&lt;p&gt;$NegLogLikelihood = -\sum_{i=1}^{m}{log(P(\hat{y}^i|X^i))}$&lt;/p&gt;

&lt;p&gt;This is called as **Negative Log Likelihood Loss **or also as &lt;strong&gt;Cross Entropy Loss&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Now there are 2 cases
- $y = 0$, then we want $P(\hat{y}^i|X^i) =  \hat{y}_0 = 1 - \hat{y} = 1 - g(W.X+b)$
- $y = 1$, then $P(\hat{y}^i|X^i) =  \hat{y}_1 = \hat{y} =  g(W.X+b)$&lt;/p&gt;

&lt;p&gt;So we generalize this 2 cases with&lt;/p&gt;

&lt;p&gt;$ NLL(y, \hat{y}) = -\dfrac{1}{m} \sum_{i=1}^{m}{y^i log(\hat{y}^i) + (1-y^i) log(1 - \hat{y}^i)}$&lt;/p&gt;

&lt;p&gt;We divide the loss with m, to get the average, so the number of example may not affect the loss.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s code this loss with Numpy.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def CrossEntropy(y_hat, y):
    return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y = np.array([0, 1, 1, 0, 0, 1, 1, 1 ])
y_hat_1 = np.array([0.11, 0.76, 0.56, 0.21, 0.04, 0.7, 0.64, 0.95])
y_hat_2 = np.array([0.71, 0.36, 0.16, 0.61, 0.34, 0.5, 0.14, 0.8])

CrossEntropy(y_hat_1, y),CrossEntropy(y_hat_2, y)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(0.26269860327583516, 1.041454329918626)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See how a bad prediction gives more CrossEntropy loss than a better prediction.&lt;/p&gt;

&lt;h2 id=&#34;finding-the-best-model-gradient-descent&#34;&gt;Finding the best Model : Gradient Descent&lt;/h2&gt;

&lt;p&gt;We are going to use the same optimization algorithm which we used for Linear Regression. In almost every deep learning problem, we will use gradient descent or a variation or better version of Gradient Descent. Adam and SGD are better versions of Gradient descent which also uses something called momentum. We will learn more about it later.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;randomly initialize W, b&lt;/li&gt;
&lt;li&gt;in loop for n steps/epochs{

&lt;ul&gt;
&lt;li&gt;find $\hat{y} = g(X.W + b)$&lt;/li&gt;
&lt;li&gt;find $ \mathcal{L}(y, \hat{y}) = NLL(y, \hat{y})$&lt;/li&gt;
&lt;li&gt;find $ \frac{\partial \mathcal{L}}{\partial w} $ and $ \frac{\partial \mathcal{L}}{\partial b} $&lt;/li&gt;
&lt;li&gt;Update W and b with $w := w - \alpha \frac{\partial \mathcal{L}}{\partial w}$ and $b := b - \alpha \frac{\partial \mathcal{L}}{\partial b}$
}&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Every thing remains same as Linear Regression except now the Loss function $\mathcal{L}(y, \hat{y})$ is different, so  $ \frac{\partial \mathcal{L}}{\partial w} $ and $ \frac{\partial \mathcal{L}}{\partial b} $ may be different. But it can easily be alculated with chain rule.&lt;/p&gt;

&lt;p&gt;If you didn&amp;rsquo;t understand how we calculated  $ \frac{\partial \mathcal{L}}{\partial w} $ and $ \frac{\partial \mathcal{L}}{\partial b} $ in Linear Regression , I strongly recommend you to learn multi variable calculus. Its very easy and interesting.&lt;/p&gt;

&lt;p&gt;One good thing about frameworks like Tensorflow, PyTorch is that they have soemthing called Automatic Gradient. So you don&amp;rsquo;t need to perform these gradient calculation by hand and code it, you give the loss function and parameters, the framework will calculate the gradients of Loss wrt every parameter and update the parameter.&lt;/p&gt;

&lt;p&gt;So after chain rule,&lt;/p&gt;

&lt;p&gt;$\dfrac{\partial \mathcal{L}}{\partial w_i}  = \dfrac{1}{m} \sum_{j=1}^{m}{x_i^j(\hat{y}^j - y^j)}$&lt;/p&gt;

&lt;p&gt;$\dfrac{\partial \mathcal{L}}{\partial b}  = \dfrac{1}{m} \sum_{j=1}^{m}{(\hat{y}^j - y^j)}$&lt;/p&gt;

&lt;p&gt;We will update the parameters with&lt;/p&gt;

&lt;p&gt;$w_i := w_i - \alpha \dfrac{1}{m} \sum_{j=1}^{m}{x_i^j(\hat{y}^j - y^j)}$&lt;/p&gt;

&lt;p&gt;$b := b - \alpha \dfrac{1}{m} \sum_{j=1}^{m}{((\hat{y}^j - y^j)}$&lt;/p&gt;

&lt;p&gt;where $\alpha$ is called learning rate, if learning rate is very high the model will learn faster, but may not converge well. if the learning rate is less, the model may take more time but will converge well ie: will get to less loss.&lt;/p&gt;

&lt;p&gt;Now Let&amp;rsquo;s code Logistic Regression model in Numpy and train it. Then use Tensorflow&amp;rsquo;s Keras API to train Logistic Regression Model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

def gradient_descent(w, b, X, y, a):
  w = w - a / X.shape[0] * np.dot(X.T, Log_Reg_model(X, w, b)- y)
  b = b - a / X.shape[0] * np.sum(Log_Reg_model(X, w, b)- y)
  return w, b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So far we have seen the math which is used in Logistic Regression model. But when you code, you will have to take care of the dimensions as well. we have seen $\hat{y} = g(X.W+b)$ here $X.W$ need to be in correct dimension for matrix multiplication.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sigmoid(x):
  return 1/(1+np.exp(-x))
  
def Log_Reg_model(x, w, b):
  y_hat = sigmoid(np.matmul(x, w) + b)
  return y_hat
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# here i am initializing w as (2,1) to match X(1000,2)
# you can also initialize w as (1,2) and use np.matmul(X, w.T) + b
w, b = np.random.random((2, 1)), np.random.random((1, 1))

from sklearn.datasets import make_blobs

X, y = make_blobs(n_samples=1000, n_features=2, centers=2, random_state=9)
y = y.reshape(-1,1)
print(X.shape, y.shape, w.shape, b.shape)
print(sigmoid(np.matmul(X, w) + b).shape) # to check the dimension
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(1000, 2) (1000, 1) (2, 1) (1, 1)
(1000, 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# shape of prediction and label should match
Log_Reg_model(X, w, b).shape, y.shape
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;((1000, 1), (1000, 1))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Test the Cross entropy loss

def CrossEntropy(y_hat, y):
    return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean()

y_hat = Log_Reg_model(X, w, b)

CrossEntropy(y_hat, y)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.5361011603546345
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets code a function to visualize&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt

def visualize_classification(X, y, w, b, e=None, loss=None):
    class_0 = np.where(y == 0)
    class_1 = np.where(y == 1)

    X_0 = X[class_0]
    X_1 = X[class_1]

    # change this according to the plot scale
    x0 = np.arange(-15, 5, 0.01)
    y0 = ((-b - x0 * w[1])/w[0]).reshape(-1)

    plt.figure(figsize=(12,9))
    plt.scatter(X_0[:, 0], X_0[:, 1], marker=&#39;o&#39;, s=150, color=&#39;blue&#39;, label=&#39;1&#39;)
    plt.scatter(X_1[:, 0], X_1[:, 1], marker=&#39;x&#39;, s=150, color=&#39;red&#39;, label=&#39;0&#39;)
    plt.plot(x0, y0, label=&#39;Classifier&#39;, color=&#39;black&#39;)
    if e is not None:
        plt.text(-4,3,&#39;Epoch = {}&#39;.format(e), fontsize=20)
    if loss is not None:
        plt.text(-4,2,&#39;CE = {:.3f}&#39;.format(loss), fontsize=20)
    
    plt.xlabel(&#39;$x1$&#39;, fontsize=20)
    plt.ylabel(&#39;$x2$&#39;, fontsize=20)
    plt.grid(True)
    plt.legend(fontsize=20)
    plt.show()

visualize_classification(X, y.reshape(-1), w, b, e=1, loss=1.1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_3/output_42_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;losses = []

alpha = 0.001
w, b = np.random.random((2, 1)), np.random.random(1)

for i in range(10000):
  y_hat = Log_Reg_model(X, w, b)
  ce = CrossEntropy(y_hat, y)
  losses.append(ce)
  w, b = gradient_descent(w, b, X, y, alpha)
  
  if i%2000 == 0:
    y_hat = Log_Reg_model(X, w, b)
    ce = CrossEntropy(y_hat, y)
    
    visualize_classification(X, y.reshape(-1), w, b, e=i+1, loss=ce)

plt.plot(losses)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;MSE Loss&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_3/output_43_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../1_3/output_43_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../1_3/output_43_2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../1_3/output_43_3.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../1_3/output_43_4.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../1_3/output_43_5.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Our classifier made a good classification on this toy dataset. Now Let&amp;rsquo;s use Tensorflow to make a Logistic Regression model and train on this dataset.
&lt;hr&gt;&lt;/p&gt;

&lt;h1 id=&#34;logistic-regression-in-tensorflow&#34;&gt;Logistic Regression in TensorFlow&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np

def CrossEntropy(y_hat, y):
    return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean()

from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=10000, n_features=2, centers=2, random_state=9)


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[2]), keras.layers.Activation(&#39;sigmoid&#39;)])
model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
tf_history = model.fit(X, y, epochs=10, verbose=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1/10
10000/10000 [==============================] - 0s 42us/sample - loss: 1.8787 - acc: 0.5026
Epoch 2/10
10000/10000 [==============================] - 0s 34us/sample - loss: 0.8342 - acc: 0.5618
.
.
Epoch 9/10
10000/10000 [==============================] - 0s 35us/sample - loss: 0.0172 - acc: 1.0000
Epoch 10/10
10000/10000 [==============================] - 0s 34us/sample - loss: 0.0135 - acc: 1.0000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We used a new metric called accuray_score.&lt;/p&gt;

&lt;p&gt;$Accuracy = \dfrac{No\ of\ Correct\ Predictions}{Total\ no\ of\ Predictions}$&lt;/p&gt;

&lt;p&gt;so if out of 100 data, we made 64 correct predictions and 36 incorrect then the accuracy score is $\dfrac{64}{100} = 0.64$.&lt;/p&gt;

&lt;p&gt;This dataset was very easy, so it got accuracy score of 1.0 easily.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s Try another dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np

def CrossEntropy(y_hat, y):
    return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean()

from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=10000, n_features=2, centers=2, random_state=27)

class_0 = np.where(y == 0)
class_1 = np.where(y == 1)

X_0 = X[class_0]
X_1 = X[class_1]

plt.figure(figsize=(12,9))
plt.scatter(X_0[:, 0], X_0[:, 1], marker=&#39;o&#39;, s=150, color=&#39;blue&#39;, label=&#39;1&#39;)
plt.scatter(X_1[:, 0], X_1[:, 1], marker=&#39;x&#39;, s=150, color=&#39;red&#39;, label=&#39;0&#39;)
plt.xlabel(&#39;$x1$&#39;, fontsize=20)
plt.ylabel(&#39;$x2$&#39;, fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[2]), keras.layers.Activation(&#39;sigmoid&#39;)])
model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
tf_history = model.fit(X, y, epochs=50, verbose=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_3/output_48_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1/50
10000/10000 [==============================] - 0s 44us/sample - loss: 5.3699 - acc: 0.5000
Epoch 2/50
10000/10000 [==============================] - 0s 37us/sample - loss: 3.3782 - acc: 0.5000
.
.
Epoch 49/50
10000/10000 [==============================] - 0s 38us/sample - loss: 0.0044 - acc: 0.9992
Epoch 50/50
10000/10000 [==============================] - 0s 37us/sample - loss: 0.0043 - acc: 0.9993
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see from the plot, that is not possible to get a accuracy of 1.0 with a linear classifier like Logistic Regression for this dataset as few of the class points are overlapping. But it still gives a very good result with just few lines of code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/google-ml-academy/deeplearning/1.4/</link>
      <pubDate>Fri, 30 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/google-ml-academy/deeplearning/1.4/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_4_1_MultiClass_Classification.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;a href=&#34;https://github.com/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_4_1_MultiClass_Classification.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;svg class=&#34;octicon octicon-mark-github v-align-middle&#34; height=&#34;30&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;30&#34; aria-hidden=&#34;true&#34;&gt;&lt;path fill-rule=&#34;evenodd&#34; d=&#34;M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z&#34;&gt;&lt;/path&gt;&lt;/svg&gt; Open in GitHub&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;h1&gt;&lt;a href=&#39;https://shangeth.com/google-ml-academy/&#39;&gt;Google ML Academy 2019&lt;/a&gt;&lt;/h1&gt;&lt;/center&gt;
&lt;center&gt;&lt;h3&gt;Instructor: &lt;a href=&#39;https://shangeth.com/&#39;&gt;Shangeth Rajaa&lt;/a&gt;&lt;/h3&gt;&lt;/center&gt;
&lt;hr&gt;&lt;/p&gt;

&lt;p&gt;Solutions Here:
&lt;a href=&#34;https://colab.research.google.com/github/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_4_2_MultiClass_Classification_Solution.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;a href=&#34;https://github.com/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_4_2_MultiClass_Classification_Solution.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;svg class=&#34;octicon octicon-mark-github v-align-middle&#34; height=&#34;30&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;30&#34; aria-hidden=&#34;true&#34;&gt;&lt;path fill-rule=&#34;evenodd&#34; d=&#34;M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z&#34;&gt;&lt;/path&gt;&lt;/svg&gt; Open in GitHub&lt;/a&gt;&lt;/center&gt;&lt;/p&gt;

&lt;h1 id=&#34;multi-class-classification&#34;&gt;Multi Class Classification&lt;/h1&gt;

&lt;p&gt;In the previous notebeook we used logistic regression for Binary Classification, now we will see how to train a classifier model for Multi-Class Classification.&lt;/p&gt;

&lt;h2 id=&#34;what-is-multi-class-classification&#34;&gt;What is Multi-Class Classification?&lt;/h2&gt;

&lt;p&gt;If the target values have n discrete classification classes ie: y can take discrete value from 0 to n-1. If $y \in {0, 1, 2, 3, &amp;hellip;, n-1}$, then the classification task is n-Multi-Class.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/972/1*SwXHlCzh-d9UqHOglp3vcA.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;task-1&#34;&gt;Task - 1&lt;/h1&gt;

&lt;h2 id=&#34;visualizing-data&#34;&gt;Visualizing Data&lt;/h2&gt;

&lt;p&gt;Create a 3-Multi-Class dataset with sklearn.datasets and visualize it.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s very easy, use the same code form previous notebook and make changes for 3 classes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.datasets import make_blobs

X, y = make_blobs(n_samples=300, n_features=2, centers=3, random_state=42)

X.shape, y.shape, set(y)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;((300, 2), (300,), {0, 1, 2})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you made 3 centers, you can see &lt;code&gt;set(y)&lt;/code&gt; will return &lt;code&gt;{0, 1, 2}&lt;/code&gt;. where 0 represent the first class, 1 represent second and 2 represents the third class.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

# getting the index of each class
class_0 = np.where(y == 0)
class_1 = np.where(y == 1)
class_2 = np.where(y == 2)

X_0 = X[class_0]
X_1 = X[class_1]
X_2 = X[class_2]

X_0.shape, X_1.shape, X_2.shape
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;((100, 2), (100, 2), (100, 2))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt


plt.figure(figsize=(12,9))
plt.scatter(X_0[:, 0], X_0[:, 1], marker=&#39;x&#39;, s=150, color=&#39;red&#39;, label=&#39;0&#39;)
plt.scatter(X_1[:, 0], X_1[:, 1], marker=&#39;o&#39;, s=150, color=&#39;blue&#39;, label=&#39;1&#39;)
plt.scatter(X_2[:, 0], X_2[:, 1], marker=&#39;s&#39;, s=150, color=&#39;red&#39;, label=&#39;2&#39;)
plt.xlabel(&#39;$x1$&#39;, fontsize=20)
plt.ylabel(&#39;$x2$&#39;, fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;../1_4/output_7_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;softmax-regression&#34;&gt;Softmax Regression&lt;/h1&gt;

&lt;h2 id=&#34;how-to-get-the-probabilities-softmax&#34;&gt;How to get the probabilities? : SoftMax&lt;/h2&gt;

&lt;p&gt;Now the target is going to be $y \in {0, 1, 2}$, so sigmoid cannot be used here, as sigmoid will convert any number to range 0 to 1 , so it can only be used for binary classification.&lt;/p&gt;

&lt;p&gt;We need a function which converts the scores/logits of linear mapping into probabilities for all n classes.&lt;/p&gt;

&lt;p&gt;That function should have some properties:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;all probabilities should be &amp;gt;0&lt;/li&gt;
&lt;li&gt;probabilities should be in range $[0,1]$&lt;/li&gt;
&lt;li&gt;some of all class probabilities = 1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;
One possible function can be class_logit/sum of all class logits. Lets try it&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;p&gt;$logits = [-100, 40, -10]$, right now don&amp;rsquo;t bother how do we get 3 logits, we will discuss it below.&lt;/p&gt;

&lt;p&gt;$probabilities = [\dfrac{-100}{(-100+40-10)}, \dfrac{40}{(-100+40-10)}, \dfrac{-10}{(-100+40-10)}]$&lt;/p&gt;

&lt;p&gt;$probabilities = [\dfrac{100}{70}, \dfrac{-40}{70}, \dfrac{10}{70}]$&lt;/p&gt;

&lt;p&gt;you can see this example satisfies only the third property(sum=1). So we need a function which gives positive numbers. Exponential function can help us.&lt;/p&gt;

&lt;p&gt;$Logits = [l_0, l_1, l_2, &amp;hellip;, l_{n-1}]$&lt;/p&gt;

&lt;p&gt;$Probabilities = [\dfrac{e^{l_0}}{e^{l_1} + e^{l_2}+ &amp;hellip; + e^{l_{n-1}}}, \dfrac{e^{l_1}}{e^{l_1} + e^{l_2}+ &amp;hellip; + e^{l_{n-1}}}, &amp;hellip;, \dfrac{e^{l_{n-1}}}{e^{l_1} + e^{l_2}+ &amp;hellip; + e^{l_{n-1}}}]$&lt;/p&gt;

&lt;p&gt;This function is called Softmax, and this gives the probability that a data belongs to class j, given the logits.&lt;/p&gt;

&lt;p&gt;$P(y=j|z) = \dfrac{e^{z_j}}{\sum_{i=0}^{n-1}e^{z_i}}$&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s code softmax function in Numpy.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

def softmax(x):
  exp = np.exp(x)
  exp_sum = exp.sum(axis=1).reshape(-1,1)
  return exp/exp_sum
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.array([[22, 40, 10]])

softmax(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;array([[1.52299795e-08, 9.99999985e-01, 9.35762283e-14]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we know, replacing sigmoid with with softmax will help in the case of multi class classification. This softmax model is also called &lt;strong&gt;Softmax Regression&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;loss-function&#34;&gt;Loss Function&lt;/h2&gt;

&lt;p&gt;As we have already seen, for classification task we will use Cross Entropy loss.
The prediction of softmax regression $\hat{y} = [0.129, 0.8, 0.071]$, whereas the true label will be one of $y \in {0, 1, 2}$. We cannot directly use Cross Entropy loss with $\hat{y}$ and $y$.&lt;/p&gt;

&lt;p&gt;So we convert the true label into One-Hot Encoding form. One hot encoding is a vector representation of the label which has &amp;lsquo;1&amp;rsquo; at the index corresponding to the label and &amp;lsquo;0&amp;rsquo; elsewhere.&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;p&gt;Let $y \in {0, 1, 2, 3, 4}$, then&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;lsquo;4&amp;rsquo; is represented as $[0, 0, 0, 0, 1]$&lt;/li&gt;
&lt;li&gt;&amp;lsquo;3&amp;rsquo; is represented as $[0, 0, 0, 1, 0]$&lt;/li&gt;
&lt;li&gt;&amp;lsquo;2&amp;rsquo; is represented as $[0, 0, 1, 0, 0]$&lt;/li&gt;
&lt;li&gt;&amp;lsquo;1&amp;rsquo; is represented as $[0, 1, 0, 0, 0]$&lt;/li&gt;
&lt;li&gt;&amp;lsquo;0&amp;rsquo; is represented as $[1, 0, 0, 0, 0]$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;rsquo;s code the label to one hot conversion&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

def to_one_hot(labels, num_classes):
    return np.eye(num_classes)[labels]

num_classes = 5
labels = np.array([0, 1, 2, 3, 4])

to_one_hot(labels, num_classes)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;array([[1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0.],
       [0., 0., 1., 0., 0.],
       [0., 0., 0., 1., 0.],
       [0., 0., 0., 0., 1.]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also use sklearn.preprocessing.OneHotEncoder to convert labels to one hot vectors.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.preprocessing import OneHotEncoder

labels = np.array([[0], 
                   [1], 
                   [2], 
                   [3], 
                   [4]])

OneHotEncoder(categories=&#39;auto&#39;).fit_transform(labels).toarray()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;array([[1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0.],
       [0., 0., 1., 0., 0.],
       [0., 0., 0., 1., 0.],
       [0., 0., 0., 0., 1.]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Keras also have some utils functions which can help in one-hot encoding&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras.utils.np_utils import to_categorical   

labels = np.array([0, 1, 2, 3, 4])
to_categorical(labels, num_classes=5)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Using TensorFlow backend.

array([[1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0.],
       [0., 0., 1., 0., 0.],
       [0., 0., 0., 1., 0.],
       [0., 0., 0., 0., 1.]], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;softmax-regression-model&#34;&gt;Softmax Regression Model&lt;/h2&gt;

&lt;p&gt;From what we discussed so far, if the number of classes = 3, then we expect model to give a prediction $\hat{y} = Softmax(z)$ and $z$ will be like $z = [-10, 20, 5]$(Example).&lt;/p&gt;

&lt;p&gt;$z = X.W + b$ will only give one number like $z=[4]$ in logistic regression. But now we are using softmax regression which expect a model which gives 3 output for a 3 class classifier.&lt;/p&gt;

&lt;p&gt;If the no of input features = 2 and no of out[ut classes = 3
So we will use 3 linear classifier.&lt;/p&gt;

&lt;p&gt;$z_1 = X.W_1 + b_1$, $z_2 = X.W_2 + b_2$, $z_3 = X.W_3 + b_3$ which can be combined together with&lt;/p&gt;

&lt;p&gt;z = $\begin{bmatrix}z_1&amp;amp;z_2&amp;amp;z_3\ \end{bmatrix}$&lt;/p&gt;

&lt;p&gt;$ z = X . W + b $&lt;/p&gt;

&lt;p&gt;$W = \begin{bmatrix}W_1&amp;amp;W_2&amp;amp;W_3\ \end{bmatrix} $&lt;/p&gt;

&lt;p&gt;$b = \begin{bmatrix}b_1&amp;amp;b_2&amp;amp;b_3\ \end{bmatrix}$&lt;/p&gt;

&lt;p&gt;each $W_i = \begin{bmatrix}W_{i1}\ W_{i2}\ W_{i3}\ \end{bmatrix} $&lt;/p&gt;

&lt;p&gt;so the final $W = \begin{bmatrix}W_{11}&amp;amp;W_{12}&amp;amp;W_{13} \\ W_{21}&amp;amp;W_{22}&amp;amp;W_{23} \ \end{bmatrix}$&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;hr&gt;&lt;br&gt;
Let $X = \begin{bmatrix}x_1&amp;amp;x_2\ \end{bmatrix}$&lt;/p&gt;

&lt;p&gt;$z =\begin{bmatrix}z_1&amp;amp;z_2&amp;amp;z_3\ \end{bmatrix} = \begin{bmatrix}x_1&amp;amp;x_2 \ \end{bmatrix} . \begin{bmatrix}W_{11}&amp;amp;W_{12}&amp;amp;W_{13} \\ W_{21}&amp;amp;W_{22}&amp;amp;W_{23} \\ \end{bmatrix} + \begin{bmatrix}b_1&amp;amp;b_2&amp;amp;b_3\ \end{bmatrix}$&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For 1 data, $(1,2).(2,3) + (1,3) = (1,3)$&lt;/li&gt;
&lt;li&gt;For n data, $(n,2).(2,3) + (1,3) = (n,3)$, b will be added to all n data, this is called broadcasting.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;
Frameworks like Tensorflow, PyTorch will take care of this matrix form of $W$ and $b$ for you.&lt;/p&gt;

&lt;h1 id=&#34;task-2&#34;&gt;Task-2&lt;/h1&gt;

&lt;p&gt;Train a Softmax Regression with Tensorflow&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Data

from sklearn.datasets import make_blobs

X, y = make_blobs(n_samples=1000, n_features=2, centers=3, random_state=42)
X.shape, y.shape, set(y)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;((1000, 2), (1000,), {0, 1, 2})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We need to convert the labels into one hot vectors to train the model. Let&amp;rsquo;s use keras to_categorical function&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(y.shape)

y = to_categorical(y, num_classes=3)
print(y.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(1000,)
(1000, 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;train-validation-test-split&#34;&gt;Train-Validation-Test Split&lt;/h2&gt;

&lt;p&gt;So far we had a dataset and we used it for training and checked the accuracy/loss to see the performance. But its not the right way to check the preformance of a model. Usually any dataset is split into 3 parts namely Train-Validation-Test.&lt;/p&gt;

&lt;h3 id=&#34;train-set&#34;&gt;train set&lt;/h3&gt;

&lt;p&gt;This dataset is used to train the model. If the training is good, metrics of this dataset will be always good. Almost 60-70% of dataset is given to training set.&lt;/p&gt;

&lt;h3 id=&#34;validation-set&#34;&gt;validation set&lt;/h3&gt;

&lt;p&gt;After every epoch(generally) of training, metrics of this dataset is checked to ensure, the model is also performing good on unseen data as much as it performs on the training dataset. If the model performs well on training dataset but not good on validation set, it means the model has a problem called &amp;lsquo;Overfitting&amp;rsquo; whcih we will look in more detail later. Some hyper parameters are adjusted to make the model perform well in validation set as well during training. 10-20% of data is given to validation set.&lt;/p&gt;

&lt;h3 id=&#34;testing-set&#34;&gt;testing set&lt;/h3&gt;

&lt;p&gt;After the training is over for n epochs, when the model performs good in both training and validation sets, a final check is done to see the performance of the model on new unseen dataset. 20-30% of data is given to Test set.&lt;/p&gt;

&lt;p&gt;The percentage numbers depends on the total number of data we have access to, which you will understand as you work on more projects.&lt;/p&gt;

&lt;p&gt;We can split the dataset into train-test using &lt;code&gt;sklearn.model_selection.train_test_split&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.model_selection import train_test_split

print(X.shape, y.shape)
# test_size is the percent of split 0.2 means 20% of data is for testset.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(1000, 2) (1000, 3)
(800, 2) (200, 2) (800, 3) (200, 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;tensorflow-model&#34;&gt;Tensorflow Model&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Make a Dataset with 2 input features, 3 output classes&lt;/li&gt;
&lt;li&gt;one hot encode y&lt;/li&gt;
&lt;li&gt;Split Dataset into Train-Validation-Test&lt;/li&gt;
&lt;li&gt;Train Model with Validation Dataset, check the &lt;a href=&#34;https://keras.io/models/model/&#34; target=&#34;_blank&#34;&gt;docs&lt;/a&gt; on how to use validation data.&lt;/li&gt;
&lt;li&gt;predict X_test with the trained model, refer the docs(model.predict function)&lt;/li&gt;
&lt;li&gt;convert the prediction of X_test and y_test from one-hot to labels using np.argmax(pred, axis=1)&lt;/li&gt;
&lt;li&gt;use sklearn.metrics.accuracy_score on prediction of X_test and y_test to find the accuracy on Test set.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import make_blobs

# Make the Dataset
num_classes = 3
num_input_features = 2
X, y = make_blobs(n_samples=2000, n_features=num_input_features, centers=num_classes, random_state=42)

# to categorical
y = to_categorical(y, num_classes=num_classes)

# train-test split
# 20% of dataset to testset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)

# train-validation split
# 20% of trainset to valset
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True)

# Model
model = tf.keras.Sequential([keras.layers.Dense(units=num_classes, input_shape=[num_input_features]), keras.layers.Activation(&#39;softmax&#39;)])
model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
tf_history = model.fit(X_train, y_train, epochs=30, verbose=True, validation_data=(X_val, y_val))

# Prediction for Test set with trained Model
from sklearn.metrics import accuracy_score
y_test_pred = model.predict(X_test)
test_accuracy = accuracy_score(np.argmax(y_test_pred, axis=1), np.argmax(y_test, axis=1))

print(&#39;\nTest Accuracy = &#39;, test_accuracy)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Train on 1280 samples, validate on 320 samples
Epoch 1/30
1280/1280 [==============================] - 0s 300us/sample - loss: 5.9285 - acc: 0.3305 - val_loss: 5.8832 - val_acc: 0.3281
Epoch 2/30
1280/1280 [==============================] - 0s 40us/sample - loss: 5.3884 - acc: 0.3305 - val_loss: 5.3271 - val_acc: 0.3281
.
.
Epoch 29/30
1280/1280 [==============================] - 0s 41us/sample - loss: 0.0755 - acc: 0.9969 - val_loss: 0.0691 - val_acc: 0.9969
Epoch 30/30
1280/1280 [==============================] - 0s 41us/sample - loss: 0.0699 - acc: 0.9969 - val_loss: 0.0637 - val_acc: 0.9969

Test Accuracy =  0.995
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Train the model with different
- no of input features
- no of output classes
- no of data
- split of train-validation-test
- epochs&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Facial Emotion Recognition PyTorch ONNX</title>
      <link>/project/facial-emotion-recognition-pytorch/</link>
      <pubDate>Mon, 08 Jul 2019 01:07:01 +0530</pubDate>
      
      <guid>/project/facial-emotion-recognition-pytorch/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Overview and unifying conceptualization of Automated Machine Learning</title>
      <link>/publication/ads/</link>
      <pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/ads/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pneumonia Diagnosis with Deep Learning</title>
      <link>/project/pneumonia-diagnosis-with-deep-learning/</link>
      <pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/pneumonia-diagnosis-with-deep-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>GAN 5</title>
      <link>/post/gan-5/</link>
      <pubDate>Sat, 25 May 2019 04:43:00 +0000</pubDate>
      
      <guid>/post/gan-5/</guid>
      <description>

&lt;p&gt;As we have seen in &lt;a href=&#34;../gan-1&#34;&gt;GAN 1&lt;/a&gt;, &lt;a href=&#34;../gan-2&#34;&gt;GAN 2&lt;/a&gt;, &lt;a href=&#34;../gan-3&#34;&gt;GAN 3&lt;/a&gt;, &lt;a href=&#34;../gan-4&#34;&gt;GAN 4&lt;/a&gt; that GANs have 2 network, the Generator G and the Discriminator D. Given a latent vector z, the G generates a new sample from the distribution of the training data. D classifies a data sample as real(from the training data) or fake(generated by G).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://skymind.ai/images/wiki/GANs.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the starting, the G generates a random data sample(as it didnt learn the data distribution) and the D is not a good classifier now. As the training process goes, the G starts learning the data distribution and D becomes a good classifier. D tries to classify all sampels generated by D as fake, G tries to generate samples such that D classifies that as real. In the process, both the networks become better and Generator learns the distribution of the data and can now generate realistic samples. D becomes good at classifying real/fake data samples.
&lt;img src=&#34;https://cs.stanford.edu/people/karpathy/gan/gan.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;conditional-gan&#34;&gt;Conditional GAN&lt;/h1&gt;

&lt;p&gt;Let us consider MNIST GAN, after we trained a MNIST dataset on a GAN model, the generator(G) can now generate some images which look alike of the MNIST numbers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://gandlf.bolte.cc/resources/mnist_gan_lite.png&#34; alt=&#34;&#34; /&gt;
But what if we want the G to generate images of a specific digit?. The G which we trained generated images samples depending on the latent vector z. But we used a random z. So we cannot choose a map from random z - &amp;gt; Specific image.&lt;/p&gt;

&lt;p&gt;So we introduce a conditional label y, such that for a condition label y the generator have to generate sample.
&lt;img src=&#34;https://cdn-images-1.medium.com/max/1600/1*-buiUh_PpX_XkNOzwt83Cw.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now the Generator learns the distribution of the dataset and generates samples based on the condition y or c(condition).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/1600/1*4MxI-OrQqVMCCLbZXBV3rg.jpeg&#34; alt=&#34;&#34; /&gt;
The representation may vary, but the concept is the same.&lt;/p&gt;

&lt;p&gt;We can also generate a output for a specific input, G : x -&amp;gt; y. Here we generate an image y given an inpu image x. This is a Pix2Pix GAN.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://phillipi.github.io/pix2pix/images/edges2cats.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://phillipi.github.io/pix2pix/&#34; target=&#34;_blank&#34;&gt;Check this amazing demo of Pix2Pix GAN&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This kind of image to image (pix2pix) can be done with the help of Encoder-Decoder architecture, where the input image is encoded to a feature representation vector anf this vector is decoded to the target image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/10/4-Figure1-1-528x192.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So the generator will learn the mapping from G: x-&amp;gt;y with autoencoder architecture, and generate new samples for the given x.&lt;/p&gt;

&lt;p&gt;The generator G will get a pair of images:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;training x and training y&lt;br /&gt;
G will classify as real&lt;/li&gt;
&lt;li&gt;training x and generated y(for x)&lt;br /&gt;
G will classify as fake&lt;/li&gt;
&lt;li&gt;training x and generated y (different x)&lt;br /&gt;
G will classify as fake&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;data:image/jpeg;base64,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&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This way a conditional GAN(CGAN) or pix2pix GAN is trained, which has massive applications. In the next post we will see how to train a GAN to do a image to image translation(pix2pix) without labelled pair.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GAN 4</title>
      <link>/post/gan-4/</link>
      <pubDate>Fri, 24 May 2019 03:43:00 +0000</pubDate>
      
      <guid>/post/gan-4/</guid>
      <description>

&lt;p&gt;DGGAN is exactly the same as Linear GANs, excpet they use COnvolutional neural networks. As we all know CNNs are the best feature extractor for many kind of data like image, videos, audio, etc.&lt;/p&gt;

&lt;p&gt;We will use Street View House (SVHN) Dataset and generate new home numbers using DCGAN.
&lt;img src=&#34;https://agi.io/wp-content/uploads/2018/01/svhn-large.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The model architecture will be the same, except the CNNs will be used in the Discriminator to classify images as real or fake. Generator will use a transpose convolutional layers to upsample/generate new image samples from a given latent vector z.&lt;/p&gt;

&lt;h1 id=&#34;discriminator&#34;&gt;Discriminator&lt;/h1&gt;

&lt;p&gt;In the &lt;a href=&#34;https://arxiv.org/pdf/1511.06434.pdf&#34; target=&#34;_blank&#34;&gt;original paper&lt;/a&gt;, no max-pooling layers are used with the CNN layers, rather a stride of 2 is used.
&lt;img src=&#34;https://image.slidesharecdn.com/cnnmodels-170714134232/95/convolutional-neural-network-models-deep-learning-11-638.jpg?cb=1500039867&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1502.03167&#34; target=&#34;_blank&#34;&gt;Batch Normalization&lt;/a&gt; and Leaku ReLU are also used.
&lt;img src=&#34;https://cdn-images-1.medium.com/max/800/1*Hiq-rLFGDpESpr8QNsJ1jg.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.ytimg.com/vi/9h1ILdk0Cik/maxresdefault.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Linear layers are connected at the end of flattened cnn layers and sigmoid activation is used to make the output in the range 0 to 1.&lt;/p&gt;

&lt;h1 id=&#34;generator&#34;&gt;Generator&lt;/h1&gt;

&lt;p&gt;In generator as we need to upsample a latent vector to an image of size [3, 32, 32], we use &lt;a href=&#34;https://arxiv.org/pdf/1603.07285.pdf&#34; target=&#34;_blank&#34;&gt;transposed convolutional layer&lt;/a&gt; with ReLU activation and Batch Normalization. Tanh activation in the output layer.
&lt;img src=&#34;https://cdn-images-1.medium.com/max/1600/1*Tv7wjpBTB0Pg6rWfLm4YSA.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;let-s-code-the-dcgan&#34;&gt;Let&amp;rsquo;s code the DCGAN&lt;/h1&gt;

&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;

&lt;p&gt;Pytorch have SVHN dataset built-in to the datset library, we will use that for the dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
from torchvision import datasets
from torchvision import transforms

transform = transforms.ToTensor()
svhn = datasets.SVHN(root=&#39;data/&#39;, split=&#39;train&#39;, download=True, transform=transform)

train_loader = torch.utils.data.DataLoader(dataset=svhn_train,
                                          batch_size=256,
                                          shuffle=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We want to scale the images to have the value in the range -1 to 1.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def scale_img(x, feature_range=(-1, 1)):
    min, max = feature_range
    x = x * (max - min) + min
    return x
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;discriminator-1&#34;&gt;Discriminator&lt;/h2&gt;

&lt;p&gt;We will build a model with CNN and batch norm layers, it is a normal CNN classifier.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch.nn as nn
import torch.nn.functional as F

# function to return conv and batchnorm together
def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):
    layers = []
    conv_layer = nn.Conv2d(in_channels, out_channels, 
                           kernel_size, stride, padding, bias=False)
    layers.append(conv_layer)

    if batch_norm:
        layers.append(nn.BatchNorm2d(out_channels))
    return nn.Sequential(*layers)


class Discriminator(nn.Module):
    def __init__(self, conv_dim=32):
        super(Discriminator, self).__init__()
        self.conv_dim = conv_dim
        self.conv1 = conv(3, conv_dim, 4, batch_norm=False) #
        self.conv2 = conv(conv_dim, conv_dim*2, 4)
        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)
        self.fc = nn.Linear(conv_dim*4*4*4, 1)

    def forward(self, x):
        out = F.leaky_relu(self.conv1(x), 0.2)
        out = F.leaky_relu(self.conv2(out), 0.2)
        out = F.leaky_relu(self.conv3(out), 0.2)
        out = out.view(-1, self.conv_dim*4*4*4)
        out = self.fc(out)        
        return out
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;generator-1&#34;&gt;Generator&lt;/h2&gt;

&lt;p&gt;Generator need transposed Convolutioanl layer with Batch Norm and relu to upsample the latent vector to a image sample.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):
    layers = []
    transpose_conv_layer = nn.ConvTranspose2d(in_channels, out_channels, 
                                              kernel_size, stride, padding, bias=False)r
    layers.append(transpose_conv_layer)
    if batch_norm:
        layers.append(nn.BatchNorm2d(out_channels))
    return nn.Sequential(*layers)

class Generator(nn.Module):
    
    def __init__(self, z_size, conv_dim=32):
        super(Generator, self).__init__()
        self.conv_dim = conv_dim
        self.fc = nn.Linear(z_size, conv_dim*4*4*4)
        self.t_conv1 = deconv(conv_dim*4, conv_dim*2, 4)
        self.t_conv2 = deconv(conv_dim*2, conv_dim, 4)
        self.t_conv3 = deconv(conv_dim, 3, 4, batch_norm=False)
        
    def forward(self, x):
        out = self.fc(x)
        out = out.view(-1, self.conv_dim*4, 4, 4) # (
        out = F.relu(self.t_conv1(out))
        out = F.relu(self.t_conv2(out))
        out = self.t_conv3(out)
        out = F.tanh(out)
        return out 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;building-the-models&#34;&gt;Building the models&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;conv_dim = 32
z_size = 100

D = Discriminator(conv_dim)
G = Generator(z_size=z_size, conv_dim=conv_dim)

print(D)
print()
print(G)

train_on_gpu = torch.cuda.is_available()

if train_on_gpu:
    G.cuda()
    D.cuda()
    print(&#39;GPU available for training. Models moved to GPU&#39;)
else:
    print(&#39;Training on CPU.&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
Discriminator(
  (conv1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  )
  (conv2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (fc): Linear(in_features=2048, out_features=1, bias=True)
)



Generator(
  (fc): Linear(in_features=100, out_features=2048, bias=True)
  (t_conv1): Sequential(
    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (t_conv2): Sequential(
    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (t_conv3): Sequential(
    (0): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  )
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;loss&#34;&gt;Loss&lt;/h2&gt;

&lt;p&gt;Loss and training loop is exactly same as Linear GANs. we scale the images in range -1 to 1 inside training loop.
Check &lt;a href=&#34;https://shangeth.github.io/post/gan-2/&#34; target=&#34;_blank&#34;&gt;GAN2&lt;/a&gt; and &lt;a href=&#34;https://shangeth.github.io/post/gan-3/&#34; target=&#34;_blank&#34;&gt;GAN3&lt;/a&gt; is you dont understand about the loss and training loop.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def real_loss(D_out, smooth=False):
    batch_size = D_out.size(0)
    if smooth:
        labels = torch.ones(batch_size)*0.9
    else:
        labels = torch.ones(batch_size)   
    if train_on_gpu:
        labels = labels.cuda()
    criterion = nn.BCEWithLogitsLoss()
    loss = criterion(D_out.squeeze(), labels)
    return loss

def fake_loss(D_out):
    batch_size = D_out.size(0)
    labels = torch.zeros(batch_size)
    if train_on_gpu:
        labels = labels.cuda()
    criterion = nn.BCEWithLogitsLoss()

    loss = criterion(D_out.squeeze(), labels)
    return loss
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;optimizers&#34;&gt;Optimizers&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://ruder.io/optimizing-gradient-descent/&#34; target=&#34;_blank&#34;&gt;check this optimizer post by ruder&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import torch.optim as optim

lr = 1e-4
beta1=0.5
beta2=0.999 
d_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2])
g_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;training-loop&#34;&gt;Training loop&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;num_epochs = 50
samples = []
losses = []

print_every = 300
sample_size=16
fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))
fixed_z = torch.from_numpy(fixed_z).float()


for epoch in range(num_epochs):    
    for batch_i, (real_images, _) in enumerate(train_loader):
        batch_size = real_images.size(0)
        real_images = scale(real_images) # rescale image

        d_optimizer.zero_grad()
        if train_on_gpu:
            real_images = real_images.cuda()
        
        D_real = D(real_images)
        d_real_loss = real_loss(D_real)

        z = np.random.uniform(-1, 1, size=(batch_size, z_size))
        z = torch.from_numpy(z).float()

        if train_on_gpu:
            z = z.cuda()
        fake_images = G(z)         
        D_fake = D(fake_images)
        d_fake_loss = fake_loss(D_fake)
        
        d_loss = d_real_loss + d_fake_loss
        d_loss.backward()
        d_optimizer.step()
        
        g_optimizer.zero_grad()
        
        z = np.random.uniform(-1, 1, size=(batch_size, z_size))
        z = torch.from_numpy(z).float()
        if train_on_gpu:
            z = z.cuda()
        fake_images = G(z)
        D_fake = D(fake_images)
        g_loss = real_loss(D_fake) 
        
        g_loss.backward()
        g_optimizer.step()

        print(&#39;Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}&#39;.format(
                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Epoch [    1/   50] | d_loss: 1.3871 | g_loss: 0.7894
Epoch [    1/   50] | d_loss: 0.8700 | g_loss: 2.5015
Epoch [    2/   50] | d_loss: 1.0024 | g_loss: 1.4002
Epoch [    2/   50] | d_loss: 1.2057 | g_loss: 1.1445
Epoch [    3/   50] | d_loss: 0.9766 | g_loss: 1.0346
Epoch [    3/   50] | d_loss: 0.9508 | g_loss: 0.9849
Epoch [    4/   50] | d_loss: 1.0338 | g_loss: 1.2916
Epoch [    4/   50] | d_loss: 0.7476 | g_loss: 1.7354
Epoch [    5/   50] | d_loss: 0.8847 | g_loss: 1.9047
Epoch [    5/   50] | d_loss: 0.9131 | g_loss: 2.6848
Epoch [    6/   50] | d_loss: 0.3747 | g_loss: 2.0961
Epoch [    6/   50] | d_loss: 0.5761 | g_loss: 1.4796
Epoch [    7/   50] | d_loss: 1.0538 | g_loss: 2.5600
Epoch [    7/   50] | d_loss: 0.5655 | g_loss: 1.1675
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;view-generated-samples&#34;&gt;View generated Samples&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def view_samples(epoch, samples):
    fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True)
    for ax, img in zip(axes.flatten(), samples[epoch]):
        img = img.detach().cpu().numpy()
        img = np.transpose(img, (1, 2, 0))
        img = ((img +1)*255 / (2)).astype(np.uint8) 
        ax.xaxis.set_visible(False)
        ax.yaxis.set_visible(False)
        im = ax.imshow(img.reshape((32,32,3)))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;data:image/png;base64,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%0A&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Training a DCGAN is same as the Linear/Vanilla GAN, DCGANs can extract more features in an image with the CNN and can help in generating the distributions well.&lt;/p&gt;

&lt;p&gt;In the next post, we will look at Pix2Pix GAN and its applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GAN 3</title>
      <link>/post/gan-3/</link>
      <pubDate>Tue, 21 May 2019 03:43:00 +0000</pubDate>
      
      <guid>/post/gan-3/</guid>
      <description>

&lt;p&gt;We saw an &lt;a href=&#34;https://shangeth.github.io/post/gan-1/&#34; target=&#34;_blank&#34;&gt;Intro to GANs&lt;/a&gt; and the &lt;a href=&#34;https://shangeth.github.io/post/gna-2/&#34; target=&#34;_blank&#34;&gt;Theory of Game between Generator and Discriminator&lt;/a&gt; in the previous posts. In this post we are going to implement and learn about how to train GANs in PyTorch. We will start with MNIST dataset and in the future posts we will implement different applications of GANs and also my research paper on one of the application of GANs.&lt;/p&gt;

&lt;p&gt;So the task is to use the MNIST dataset to generate new MNIST alike data samples with GANs.
&lt;img src=&#34;https://cdn-images-1.medium.com/max/1200/1*M2Er7hbryb2y0RP1UOz5Rw.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;let-s-code-gan&#34;&gt;Let&amp;rsquo;s Code GAN&lt;/h1&gt;

&lt;h2 id=&#34;get-the-data&#34;&gt;Get the Data&lt;/h2&gt;

&lt;p&gt;Import all the necessary libraries like Numpy, Matplotlib, torch, torchvision.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import torch
import matplotlib.pyplot as plt

from torchvision import datasets
import torchvision.transforms as transforms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now lets get the MNIST data from the torchvision datasets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;transform = transforms.ToTensor()
data = datasets.MNIST(root=&#39;data&#39;, train=True,
                                   download=True, transform=transform)
data_loader = torch.utils.data.DataLoader(data, batch_size=1024)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMgAAADFCAYAAAARxr1AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-model&#34;&gt;The Model&lt;/h2&gt;

&lt;p&gt;As we have already seen in &lt;a href=&#34;https://shangeth.github.io/post/gna-2/&#34; target=&#34;_blank&#34;&gt;Theory of Game between Generator and Discriminator&lt;/a&gt;, the GAN models generally have 2 networks Discriminator D and Generator G.
We will code both of these network as seperate classes in PyTorch.
&lt;img src=&#34;https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/gan-mnist/assets/gan_network.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;discriminator&#34;&gt;Discriminator&lt;/h3&gt;

&lt;p&gt;The discriminator is a just a classifier , which takes input images and classifies the images as real or fake generated images. So lets make a classifier network in PyTorch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch.nn as nn
import torch.nn.functional as F

class D(nn.Module):

    def __init__(self, input_size, hidden_dim, output_size):
        super(D, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_dim*4)
        self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)
        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, output_size)
        self.dropout = nn.Dropout(0.3)      
        
    def forward(self, x):
        # flatten image
        x = x.view(-1, 28*28)
        x = F.leaky_relu(self.fc1(x), 0.2)
        x = self.dropout(x)
        x = F.leaky_relu(self.fc2(x), 0.2)
        x = self.dropout(x)
        x = F.leaky_relu(self.fc3(x), 0.2)
        x = self.dropout(x)
        out = F.log_softmax(self.fc4(x))
        return out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The D network has 4 linear layers with leaky relu and dropout layers in between.&lt;/p&gt;

&lt;p&gt;Here the input size will be 28*28*1 (size of MNIST image)&lt;br /&gt;
hidden dim can be anything of your choice.&lt;br /&gt;
output_size = 2 (real or fake)&lt;/p&gt;

&lt;p&gt;I am also adding a log softmax in the end for computation purpose.&lt;/p&gt;

&lt;p&gt;Lets make a Discriminator object&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;D_network = D(28*28*1, 50, 2)
print(D_network)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;output :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;D(
  (fc1): Linear(in_features=784, out_features=200, bias=True)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (fc3): Linear(in_features=100, out_features=50, bias=True)
  (fc4): Linear(in_features=50, out_features=2, bias=True)
  (dropout): Dropout(p=0.3)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;generator&#34;&gt;Generator&lt;/h3&gt;

&lt;p&gt;The Generator takes a random vector(z)(also called latent vector) and generates a sample image with a distribution close to the training data distribution. We want to upsample z to an image of size 1*28*28. Tanh was used as activation in the output layer(as used in the original paper) , but feel free to try other activations and check which gives good result.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class G(nn.Module):

    def __init__(self, input_size, hidden_dim, output_size):
        super(G, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim*2)
        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4)
        self.fc4 = nn.Linear(hidden_dim*4, output_size) 
        self.dropout = nn.Dropout(0.3)

    def forward(self, x):
        x = F.leaky_relu(self.fc1(x), 0.2)
        x = self.dropout(x)
        x = F.leaky_relu(self.fc2(x), 0.2)
        x = self.dropout(x)
        x = F.leaky_relu(self.fc3(x), 0.2)
        x = self.dropout(x)
        out = F.tanh(self.fc4(x))
        return out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The G network architecture is same as D&amp;rsquo;s architecture except now we upsample the z to 28*28*1 size image.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;G_network = G(100, 50, 1*28*28)
print(G_network)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;G(
  (fc1): Linear(in_features=100, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=100, bias=True)
  (fc3): Linear(in_features=100, out_features=200, bias=True)
  (fc4): Linear(in_features=200, out_features=784, bias=True)
  (dropout): Dropout(p=0.3)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;loss&#34;&gt;Loss&lt;/h2&gt;

&lt;p&gt;The discriminator wants the probability of fake images close to 0 and the generator wants the probability of the fake images generated by it to be close to 1.&lt;/p&gt;

&lt;p&gt;So we define 2 losses&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Real Loss (loss btw p and 1)&lt;/li&gt;
&lt;li&gt;Fake loss (loss btw p and 0)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;p is the probability of image to be real.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For Generator :
minimize real_loss(p) or p to be closer to 1. ie: fool generator by making realistic images.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For Discriminator :
minimize real_loss + fake loss. ie: p of real image close to 1 and p of fake image close to 0.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def real_loss(D_out, smooth=False):
    batch_size = D_out.size(0)
    # label smoothing
    if smooth:
        # smooth, real labels = 0.9
        labels = torch.ones(batch_size)*0.9
    else:
        labels = torch.ones(batch_size) # real labels = 1
    criterion = nn.NLLLoss()
    loss = criterion(D_out.squeeze(), labels.long().cuda())
    return loss

def fake_loss(D_out):
    batch_size = D_out.size(0)
    labels = torch.zeros(batch_size) # fake labels = 0
    criterion = nn.NLLLoss()
    loss = criterion(D_out.squeeze(), labels.long().cuda())
    return loss
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/gan-ways-to-improve-gan-performance-acf37f9f59b&#34; target=&#34;_blank&#34;&gt;label smoothing&lt;/a&gt; is also done for better convergence.&lt;/p&gt;

&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;

&lt;p&gt;We will use 2 optimizers&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;One for Generator, which optimizes the real_loss of fake images. ie: it tries to make the classification prediction of fake images equal to 1.&lt;/li&gt;
&lt;li&gt;Next is discriminator, which tries to optimize real+fake loss. ie: it tries to make the prediciton of fake images to 0 and real images to 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Adjust the no of epochs, latent vector size, optimizer parameters, dimensions etc.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;num_epochs = 100
print_every = 400

# train the network
D.train()
G.train()
for epoch in range(num_epochs):
    for batch_i, (images, _) in enumerate(train_loader):         
        batch_size = images.size(0)
        
        ## Important rescaling step ## 
        real_images = images*2 - 1  
        # rescale input images from [0,1) to [-1, 1)

        d_optimizer.zero_grad()
        D_real = D(real_images)
        d_real_loss = real_loss(D_real, smooth=True)
        
        
        z = np.random.uniform(-1, 1, size=(batch_size, z_size))
        z = torch.from_numpy(z).float()
        fake_images = G(z)
      
        D_fake = D(fake_images)
        d_fake_loss = fake_loss(D_fake)

        d_loss = d_real_loss + d_fake_loss
        d_loss.backward()
        d_optimizer.step()
        

        g_optimizer.zero_grad()
        z = np.random.uniform(-1, 1, size=(batch_size, z_size))
        z = torch.from_numpy(z).float()
        fake_images = G(z)
        
        D_fake = D(fake_images)
        g_loss = real_loss(D_fake) 
        g_optimizer.step()

        if batch_i % print_every == 0:
            print(&#39;Epoch {:5d}/{:5d}\td_loss: {:6.4f}\tg_loss: {:6.4f}&#39;.format(
                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch     1/  100 d_loss: 1.3925  g_loss: 0.6747
Epoch     2/  100 d_loss: 1.2275  g_loss: 0.6837
Epoch     3/  100 d_loss: 1.0829  g_loss: 0.6959
Epoch     4/  100 d_loss: 1.0295  g_loss: 0.7128
Epoch     5/  100 d_loss: 1.0443  g_loss: 0.7358
Epoch     6/  100 d_loss: 1.0362  g_loss: 0.7625
Epoch     7/  100 d_loss: 0.9942  g_loss: 0.8000
Epoch     8/  100 d_loss: 0.9445  g_loss: 0.8455
Epoch     9/  100 d_loss: 0.9005  g_loss: 0.9073
Epoch    10/  100 d_loss: 0.8604  g_loss: 0.9908
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;generate-new-mnist-samples&#34;&gt;Generate new MNIST Samples&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def view_samples(epoch, samples):
    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)
    for ax, img in zip(axes.flatten(), samples[epoch]):
        img = img.detach()
        ax.xaxis.set_visible(False)
        ax.yaxis.set_visible(False)
        im = ax.imshow(img.reshape((28,28)), cmap=&#39;Greys_r&#39;)

sample_size=16
rand_z = np.random.uniform(-1, 1, size=(sample_size, z_size))
rand_z = torch.from_numpy(rand_z).float()

G.eval()
rand_images = G(rand_z)
view_samples(0, [rand_images])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAZkAAAGRCAYAAAC39s6jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0AS0YIIUQ09JIRQggRjf8DCGGm5rjhC7AAAAAASUVORK5CYII=%0A&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Linear GAN Model does a decent job in generating MNIST images. In next post we will look into DCGAN(Deep Convolutional GAN), to use CNNs for generating new samples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN&#34; target=&#34;_blank&#34;&gt;Check this Awesome Repo&lt;/a&gt; on comparing Linear GAN and DCGAN for MNIST.
Also &lt;a href=&#34;https://github.com/Yangyangii/GAN-Tutorial/blob/master/MNIST/VanillaGAN.ipynb&#34; target=&#34;_blank&#34;&gt;this notebook&lt;/a&gt; for pytorch implementation of vanilla GAN(Linear).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GAN 2</title>
      <link>/post/gan-2/</link>
      <pubDate>Tue, 21 May 2019 02:43:00 +0000</pubDate>
      
      <guid>/post/gan-2/</guid>
      <description>

&lt;p&gt;As we saw in &lt;a href=&#34;https://shangeth.github.io/post/gan-1/&#34; target=&#34;_blank&#34;&gt;Intro and application of Generative Adversarial Network&lt;/a&gt;, GANs generate new data from a given dataset by learning the distribution of the dataset through adversarial process.&lt;/p&gt;

&lt;h2 id=&#34;what-is-an-adversarial-process-learning&#34;&gt;What is an adversarial process/learning?&lt;/h2&gt;

&lt;p&gt;A google search can tell you that adversarial machine learning is a technique used in Machine Learning which tries to fool the model by giving in false/malicious input.&lt;/p&gt;

&lt;h2 id=&#34;components-of-gans&#34;&gt;Components of GANs:&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://i.stack.imgur.com/UnKny.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;h3 id=&#34;generator&#34;&gt;Generator&lt;/h3&gt;

&lt;p&gt;The Generator network takes random noise as input and convert it to a data sample(image/music) . The output of generator is a fake but realistic data sample. The choice of the random noise determines the distribution into which the data sample generated falls.&lt;br /&gt;
But the generator network have to be trained to produce samples for the given random noise. ie: the generator have to learn the distribution of the dataset, so it generates new data samples from the distribution.&lt;/p&gt;

&lt;p&gt;As this is not a supervised learning, we cannot use labels to learn the parameters of generator. So we use adversarial learning technique to learn the distribution of the dataset.&lt;/p&gt;

&lt;p&gt;The idea is to maximize the probability that the data sample generated by the generator is from the training dataset. But it is not easy as its un labelled , so we use the help of another network called Discriminator.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h3 id=&#34;discriminator&#34;&gt;Discriminator&lt;/h3&gt;

&lt;p&gt;Discriminator is a normal Neural Network classifier. The discriminator finds if a data sample is from the training dataset or not.&lt;br /&gt;
During the training process, the discriminator is given data from the training dataset 50% of the time and data samples generated by generator other 50% of the time. The discriminator classifies the generated data samples as fake and data from the training dataset as real data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;the-game-theory&#34;&gt;The Game Theory&lt;/h2&gt;

&lt;p&gt;As the disciminator classifies the data sample from the generator as fake, the generator tries to fool the discriminator by generating more realistic data sample(learns the training data distribution well). The generator starts generating samples more close to the distribution of the training dataset.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cs.stanford.edu/people/karpathy/gan/gan.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As the generator tries to fool the discriminator, the discriminator learns to classify the more realistic(fake) data generated by the generator as fake. &lt;br /&gt;
By this process both the networks learn the parameters which gives best results. This creates a competition between Generator(G) and Discriminator(D), this makes this an adversarial learning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://cs.stanford.edu/people/karpathy/gan/&#34; target=&#34;_blank&#34;&gt;Check this&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In &lt;a href=&#34;https://en.wikipedia.org/wiki/Game_theory&#34; target=&#34;_blank&#34;&gt;game theory&lt;/a&gt;, an &lt;a href=&#34;https://www.youtube.com/watch?v=0i7p9DNvtjk&#34; target=&#34;_blank&#34;&gt;equlibrium&lt;/a&gt; is reached in a 2 player game when both the players recieve 0 payoff. When a player(P) wins, P gets a positive payoff of 1 and gets a negative payoff of -1 when loses. When a player loses, the player changes the stratergy to win the next round. As this continues the player becomes better but as the other player also gets better , an equilibrium is reached when both players uses random uniform stratergies.
At equilibrium, neither of the players can improve further.&lt;br /&gt;
&lt;br /&gt;
Most of the machine learning models we used so far depends on optimization algorithms. We finds a set of parameters for which the cost function is minimum. But GANs have 2 players G &amp;amp; D. The G is trying to fool D and D is trying to classify G&amp;rsquo;s sample as fake data. As we can see D is trying to minimize the probability of G&amp;rsquo;s output as true data, whereas G is trying to increase the probability.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-math&#34;&gt;Cost of D = minimize(P(generated sample data is real))
Cost of G = maximize(P(generated sample data is real))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Theoritically equlibrium occurs when both probabilities are equal.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-math&#34;&gt;P(generated sample data is real) = 0.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This occurs for a set of parameters for which the G got the maximum probability and D got minimum probability. ie: a &lt;a href=&#34;https://en.wikipedia.org/wiki/Saddle_point&#34; target=&#34;_blank&#34;&gt;saddle point&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;saddle point - both local maxima and local minima&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://math.etsu.edu/multicalc/prealpha/Chap2/Chap2-8/10-6-53.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Generator gets a local maxima when the distribution learned by generator is equal to the distribution of the training dataset.&lt;/p&gt;

&lt;p&gt;We will use 2 seperate optimization algorithms for D and G, so it is not possible for us to find the equilibrium. But if we can use a single optimization algorithm which reduces both D &amp;amp; G costs together, then we may encounter perfect equilibirum.&lt;/p&gt;

&lt;p&gt;In the next post, we will look into the practical implementation of GANs by coding and training it in PyTorch.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GAN 1</title>
      <link>/post/gan-1/</link>
      <pubDate>Mon, 20 May 2019 02:00:00 +0000</pubDate>
      
      <guid>/post/gan-1/</guid>
      <description>

&lt;h1 id=&#34;gans-https-arxiv-org-pdf-1406-2661-pdf-are-ml-models-that-can-imagine-new-things&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/1406.2661.pdf&#34; target=&#34;_blank&#34;&gt;GANs&lt;/a&gt; are ML models that can imagine new things.&lt;/h1&gt;

&lt;p&gt;GANs can generate new data with a given dataset by learning its distribution. GANs have been mostly used with image data but can also be used on any kind of data. GANs draw a sample from the learned probability distribution of the dataset which is a completely new sample.&lt;br /&gt;
&lt;img src=&#34;https://paulvanderlaken.files.wordpress.com/2017/10/1-az5-3wdndyyc2u0aq7rhig.png?w=816&#34; alt=&#34;&#34; /&gt;
GANs are &lt;a href=&#34;https://shangeth.github.io/post/unsupervised-learning/&#34; target=&#34;_blank&#34;&gt;unsupervised machine learning models&lt;/a&gt; , which learns the distribution of the data through adverserial process and generate new sample from the learned distribution.&lt;/p&gt;

&lt;h1 id=&#34;some-recent-research-in-gan&#34;&gt;Some Recent Research in GAN&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;h2 id=&#34;stack-gan-https-arxiv-org-abs-1612-03242&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1612.03242&#34; target=&#34;_blank&#34;&gt;Stack GAN&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Stack GAN can take a description of an image and can generate new images matching that description. GAN picks a sample from a distribution of images which matches the description.
&lt;img src=&#34;https://media.arxiv-vanity.com/render-output/707623/x1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cs.virginia.edu/~vicente/vislang/slides/stackgan.pdf&#34; target=&#34;_blank&#34;&gt;slide&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;igan-https-arxiv-org-pdf-1609-03552-pdf&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/1609.03552.pdf&#34; target=&#34;_blank&#34;&gt;iGAN&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://bzdww.com/cms_static/v2-9b2c42aaf68de307eb0cd97ae18409a7_b.jpg&#34; alt=&#34;&#34; /&gt;
iGANs can search for realistic possible image as the user draws the rough sketch.&lt;br /&gt;
&lt;a href=&#34;https://github.com/junyanz/iGAN&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;pix2pix-https-arxiv-org-abs-1611-07004&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.07004&#34; target=&#34;_blank&#34;&gt;Pix2Pix&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Images in one domain can be changed to image in another domain with GANS. Rough sketches can be made into a realistic image which are generated by GANs. Blue Prints of a building can be changed to an image of finished building with GANs.
&lt;img src=&#34;https://cdn-images-1.medium.com/max/1200/1*irKK9JBM-O23jrh1vfiGTw.png&#34; alt=&#34;&#34; /&gt;
&lt;a href=&#34;https://github.com/phillipi/pix2pix&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Many other applications like photos to cartoons, daylight image to night scene image, &lt;a href=&#34;https://arxiv.org/abs/1703.10593&#34; target=&#34;_blank&#34;&gt;Cycle GAN&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/wiseodd/generative-models&#34; target=&#34;_blank&#34;&gt;Check out&lt;/a&gt; all of these Generative models.&lt;/p&gt;

&lt;p&gt;In the next few posts, we will look deep into how GANs work and code GANs with PyTorch for different applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unsupervised Learning 101</title>
      <link>/post/unsupervised-learning/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/unsupervised-learning/</guid>
      <description>

&lt;h1 id=&#34;machine-learning-is-broadly-divided-into-3-types&#34;&gt;Machine Learning is broadly divided into 3 types:&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;h2 id=&#34;supervised-learning&#34;&gt;Supervised learning&lt;/h2&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;unsupervised-learning&#34;&gt;Unsupervised learning&lt;/h2&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;reinforcement-learning&#34;&gt;Reinforcement learning&lt;/h2&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;/p&gt;

&lt;h1 id=&#34;supervised-learning-https-en-wikipedia-org-wiki-supervised-learning&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Supervised_learning&#34; target=&#34;_blank&#34;&gt;Supervised Learning&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;The task in supervised learning is to learn a function to map a data X to a label y. All the &lt;a href=&#34;https://towardsdatascience.com/supervised-machine-learning-classification-5e685fe18a6d&#34; target=&#34;_blank&#34;&gt;classification&lt;/a&gt;, regression, object detection/recognition/segmentation generally comes under supervised learning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://corochann.com/wp-content/uploads/2017/02/mnist_plot-800x600.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://appliedmachinelearning.files.wordpress.com/2018/03/cifar2.jpg&#34; alt=&#34;&#34; /&gt;
  In supervised learning we have a dataset which contains data X and label y and we need to learn how to find y given X.&lt;/p&gt;

&lt;h1 id=&#34;unsupervised-learning-https-towardsdatascience-com-unsupervised-learning-with-python-173c51dc7f03&#34;&gt;&lt;a href=&#34;https://towardsdatascience.com/unsupervised-learning-with-python-173c51dc7f03&#34; target=&#34;_blank&#34;&gt;Unsupervised Learning&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;In unsupervised learning, we only have X and not the respective y. The goal is to learn the underlying structure/features of the dataset without any label.&lt;/p&gt;

&lt;p&gt;Some examples of Unsupervised Learning are&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;h2 id=&#34;clustering&#34;&gt;Clustering&lt;/h2&gt;

&lt;p&gt;Clustering is dividing the data into groups through some distance metric, like kmeans clustering.
&lt;img src=&#34;https://www.imperva.com/blog/wp-content/uploads/sites/9/2017/07/k-means-clustering-on-spherical-data-1v2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;feature-learning&#34;&gt;Feature Learning&lt;/h2&gt;

&lt;p&gt;As the name suggest, learning the features of each of the given data, without its label. This is generally done with a help of a model called &lt;a href=&#34;https://www.youtube.com/watch?v=H1AllrJ-_30&#34; target=&#34;_blank&#34;&gt;Autoencoders&lt;/a&gt;.&lt;br /&gt;
Autoencoders take the data X as the label y , it try to recreate the data X given data X and learns some underlying features in that process. We generally take the one of the middle layers of the autoencoder as the encoded feature.
&lt;img src=&#34;https://cdn-images-1.medium.com/max/1200/1*j_y0bNZLP1yzqtyF48Z3Ug.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;h2 id=&#34;dimensionality-reduction&#34;&gt;Dimensionality Reduction&lt;/h2&gt;

&lt;p&gt;As we know data can be multi dimensional which can extent even to millions. Computation and visualization of such multi dimensioanl data is difficult and thus we want to reduce the dimension of the data (to pick the dimensions which can represent the data more).&lt;br /&gt;
Dimensionality reduction is done by choosing the axis in the data space along which variance of the data is high.
&lt;img src=&#34;https://static1.squarespace.com/static/5a316dfecf81e0076f50dae2/t/5ac35d702b6a284b3fde6131/1522753187751/PCA.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;and many other examples like data compression(using auto encoders), Generative models, density estimation, etc.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;why-unsupervised-learning&#34;&gt;Why Unsupervised Learning?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Unsupervised learning doesn&amp;rsquo;t need labels.&lt;/li&gt;
&lt;li&gt;Making the training data for supervised learning is not easy. Its expensive, time consuming, labour consuming.&lt;/li&gt;
&lt;li&gt;The world has a lot of unlabelled data, which can be used directly or with a little pre processing for unsupervised learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unsupervised Learning is still an ameature area of research, which has a lot of potential. Unsupervised learning is less expensive and can accelerate the AI field so much.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction</title>
      <link>/publication/icacds/</link>
      <pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/icacds/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Cite as :&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Rajaa S., Sahoo J.K. (2019) Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction. In: Singh M., Gupta P., Tyagi V., Flusser J., ren T., Kashyap R. (eds) Advances in Computing and Data Sciences. ICACDS 2019. Communications in Computer and Information Science, vol 1045. Springer, Singapore&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Convolutional Feature Extraction and Neural Arithmetic Logic Units for Stock Prediction</title>
      <link>/talk/icacds/</link>
      <pubDate>Sat, 13 Apr 2019 13:00:00 +0000</pubDate>
      
      <guid>/talk/icacds/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/slides/example/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
   One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi Tasking Learning</title>
      <link>/talk/multi-tasking-learning/</link>
      <pubDate>Fri, 11 Jan 2019 20:00:00 +0000</pubDate>
      
      <guid>/talk/multi-tasking-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Character Generating RNN</title>
      <link>/project/character-generating-rnn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/character-generating-rnn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Computer Vision Security System</title>
      <link>/project/computer-vision-security-system/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/computer-vision-security-system/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emojification</title>
      <link>/project/emojification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/emojification/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hand Gesture Recognition</title>
      <link>/project/hand-gesture-recognition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/hand-gesture-recognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lane Detection with Deep Learning</title>
      <link>/project/lane-detection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/lane-detection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi Tasking Learning for face characterization</title>
      <link>/project/multitasking-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/multitasking-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NYTimes Topic Modelling</title>
      <link>/project/nytimes-topic-modelling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/nytimes-topic-modelling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neural Style Transfer</title>
      <link>/project/neural-style-transfer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/neural-style-transfer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PyRevShell</title>
      <link>/project/pyrevshell/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/pyrevshell/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self Driving Cars Steering Angle Prediction</title>
      <link>/project/self-driving-cars-steering-angle-prediction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/self-driving-cars-steering-angle-prediction/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self Driving Cars Vehicle Detection</title>
      <link>/project/self-driving-cars-vehicle-detection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/self-driving-cars-vehicle-detection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Seq2Seq Machine Translation</title>
      <link>/project/seq2seq-machine-translation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/seq2seq-machine-translation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Signature Verification with Deep Learning</title>
      <link>/project/signature-verification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/signature-verification/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SockChat</title>
      <link>/project/sockchat/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/sockchat/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tic Tac Toe</title>
      <link>/project/tictactoe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/tictactoe/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Twitter Sentiment analysis</title>
      <link>/project/nltk-sentiment-analysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/nltk-sentiment-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Vehicle Speed Estimation</title>
      <link>/project/vehicle-speed-estimation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/vehicle-speed-estimation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Web Builder</title>
      <link>/project/web-builder/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/web-builder/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Weekly Scheduler</title>
      <link>/project/weekly-scheduler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/weekly-scheduler/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
