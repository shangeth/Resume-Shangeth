<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Shangeth Rajaa">

  
  
  
    
  
  <meta name="description" content="Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Try Completing this Notebook before going through this page.   Open in GitHub Its a very simple assignment, you can finish it in less than 10 minutes. If you are stuck somewhere refer this page for solutions.
Task-1 : Linear Regression on Non-Linear Data  Get X and y from dataset() function Train a Linear Regression model for this dataset.">

  
  <link rel="alternate" hreflang="en-us" href="/google-ml-academy/deeplearning/1.2/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="hsl(339, 90%, 68%)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.26264af3549d61c0ce873bd043df951e.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-134441268-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/google-ml-academy/deeplearning/1.2/">

  
  
  
  
    
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@shangethr">
  <meta property="twitter:creator" content="@shangethr">
  
  <meta property="og:site_name" content="Shangeth">
  <meta property="og:url" content="/google-ml-academy/deeplearning/1.2/">
  <meta property="og:title" content=" | Shangeth">
  <meta property="og:description" content="Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Try Completing this Notebook before going through this page.   Open in GitHub Its a very simple assignment, you can finish it in less than 10 minutes. If you are stuck somewhere refer this page for solutions.
Task-1 : Linear Regression on Non-Linear Data  Get X and y from dataset() function Train a Linear Regression model for this dataset."><meta property="og:image" content="/img/instructor.jpeg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-08-29T00:00:00&#43;01:00">
  
  <meta property="article:modified_time" content="2019-08-29T00:00:00&#43;01:00">
  

  

  

  <title> | Shangeth</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" class="dark">
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Shangeth</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/google-ml-academy/">
            
            <span>Google ML Academy</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>



<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      




<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
  <input name="q" type="search" class="form-control" id="search-query" placeholder="Search..." autocomplete="off">
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/google-ml-academy/deeplearning/">Course Overview</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/google-ml-academy/deeplearning/1.1/">1.Intro to Deep Learning</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.1/">1.1.Linear Regression</a>
      </li>
      
      <li class="active">
        <a href="/google-ml-academy/deeplearning/1.2/">1.2.Assignment - Polynomial Regression</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.3/">1.3.Logistic Regression</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      
      <p class="docs-toc-title">On this page</p>
      

      <nav id="TableOfContents">
<ul>
<li><a href="#task-1-linear-regression-on-non-linear-data">Task-1 : Linear Regression on Non-Linear Data</a>
<ul>
<li><a href="#dataset">Dataset</a></li>
<li><a href="#scaling-dataset">Scaling Dataset</a></li>
<li><a href="#linear-regression-in-tensorflow">Linear Regression in TensorFlow</a></li>
</ul></li>
<li><a href="#polynomial-regression">Polynomial Regression</a>
<ul>
<li><a href="#polynomial-features">Polynomial Features</a></li>
</ul></li>
<li><a href="#task-2">Task - 2</a>
<ul>
<li><a href="#tensorflow-model-with-2nd-degree">Tensorflow Model with 2nd Degree</a></li>
<li><a href="#tensorflow-model-with-3rd-degree">Tensorflow Model with 3rd Degree</a></li>
</ul></li>
<li><a href="#tensorflow-model-with-4th-degree">Tensorflow Model with 4th Degree</a></li>
</ul>
</nav>

      <ul class="nav toc-top">
        <li><a href="#">Back to top</a></li>
      </ul>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article" itemscope itemtype="http://schema.org/Article">

        <div class="docs-article-container">
          <h1 itemprop="name"></h1>

          <div class="article-style" itemprop="articleBody">
            

<p><a href="https://colab.research.google.com/github/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_2_2_Assignment_Polynomial_Regression_Solution.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>

<p><center><a href="https://github.com/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_2_2_Assignment_Polynomial_Regression_Solution.ipynb" target="_parent"><svg class="octicon octicon-mark-github v-align-middle" height="30" viewBox="0 0 16 16" version="1.1" width="30" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg> Open in GitHub</a></center></p>

<p><h1><a href='https://shangeth.com/google-ml-academy/'>Google ML Academy 2019</a></h1>
<h3>Instructor: <a href='https://shangeth.com/'>Shangeth Rajaa</a></h3>
<hr></p>

<p>Try Completing this Notebook before going through this page.
<a href="https://colab.research.google.com/github/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_2_1_Assignment_Polynomial_Regression.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
<center><a href="https://github.com/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_2_1_Assignment_Polynomial_Regression.ipynb" target="_parent"><svg class="octicon octicon-mark-github v-align-middle" height="30" viewBox="0 0 16 16" version="1.1" width="30" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg> Open in GitHub</a></center>
 Its a very simple assignment, you can finish it in less than 10 minutes. If you are stuck somewhere refer this page for solutions.</p>

<h1 id="task-1-linear-regression-on-non-linear-data">Task-1 : Linear Regression on Non-Linear Data</h1>

<ul>
<li>Get X and y from dataset() function</li>
<li>Train a Linear Regression model for this dataset.</li>
<li>Visualize the model prediction</li>
</ul>

<h2 id="dataset">Dataset</h2>

<p>Call <code>dataset()</code> function to get X, y</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def dataset(show=True):
    X = np.arange(-25, 25, 0.1)
    # Try changing y to a different function
    y = X**3 + 20 + np.random.randn(500)*1000
    if show:
        plt.scatter(X, y)
        plt.show()
    return X, y

X, y = dataset()
</code></pre>

<p><img src="../1_2/output_4_0.png" alt="png" /></p>

<h2 id="scaling-dataset">Scaling Dataset</h2>

<p>The maximum value of y in the dataset goes upto 15000 and the minimum values is less than -15000. The range of y is very large which makes the convergence/loss reduction slower. So will we scale the data, scaling the data will help the model converge faster. If all the features and target are in same range, there will be symmetry in the curve of Loss vs weights/bias, which makes the convergence faster.</p>

<p>We will do a very simple type of scaling, we will divide all the values of the data with the maximum values for X and y respectively.</p>

<pre><code class="language-python">X, y = dataset()

print(max(X), max(y), min(X), min(y))

X = X/max(X)
y = y/max(y)

print(max(X), max(y), min(X), min(y))
</code></pre>

<p><img src="../1_2/output_6_0.png" alt="png" /></p>

<pre><code>24.90000000000071 16694.307606867886 -25.0 -16126.103960535462
1.0 1.0 -1.0040160642569995 -0.9659642280642613
</code></pre>

<p>This is not a great scaling method, but good to start. We will see many more scaling/normalizing methods later.</p>

<p><strong>Try training the model with and without scaling and see the difference yourself.</strong></p>

<h2 id="linear-regression-in-tensorflow">Linear Regression in TensorFlow</h2>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np

X, y = dataset(show=False)
X_scaled = X/max(X)
y_scaled = y/max(y)

model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])

# you can also define optimizers in this way, so you can change parameters like lr.
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(optimizer=optimizer, loss='mean_squared_error')
tf_history = model.fit(X_scaled, y_scaled, epochs=500, verbose=True)

plt.plot(tf_history.history['loss'])
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.show()

mse = tf_history.history['loss'][-1]
y_hat = model.predict(X_scaled)

plt.figure(figsize=(12,7))
plt.title('TensorFlow Model')
plt.scatter(X_scaled, y_scaled, label='Data $(X, y)$')
plt.plot(X_scaled, y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0)
plt.xlabel('$X$', fontsize=20)
plt.ylabel('$y$', fontsize=20)
plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
</code></pre>

<pre><code>WARNING: Logging before flag parsing goes to stderr.
W0829 03:45:55.755687 139671975163776 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor


Epoch 1/500
500/500 [==============================] - 0s 951us/sample - loss: 0.2951
Epoch 2/500
500/500 [==============================] - 0s 41us/sample - loss: 0.2856
.
.
Epoch 499/500
500/500 [==============================] - 0s 41us/sample - loss: 0.0264
Epoch 500/500
500/500 [==============================] - 0s 39us/sample - loss: 0.0263
</code></pre>

<p><img src="../1_2/output_9_2.png" alt="png" /></p>

<p><img src="../1_2/output_9_3.png" alt="png" /></p>

<p>Looks the model Prediction for this dataset is not very great, but that is expected as the model is a straight line, it cannot predict non linear regression data.  Is there a way to train a regression model for this task?</p>

<h1 id="polynomial-regression">Polynomial Regression</h1>

<p>So when the dataset is not linear, linear regression cannot learn the dataset and make good predictions.</p>

<p>So we need a polynomial model which consideres the polynomial terms as well. So we need terms like $x^2$, $x^3$, &hellip;, $x^n$ for the model to learn a polynomial of $n^{th}$ degree.</p>

<p>$\hat{y} = w_0 + w_1x + w_2x^2 + &hellip; + w_nx^n$</p>

<p>One down side of this model is that, We will have to decide the value of n. But this is better than a linear regression model. We can get an idea of the value of n by visualizing a dataset, but for multi variable dataset, we will have to try different values of n and check which is better.</p>

<h2 id="polynomial-features">Polynomial Features</h2>

<p>you can calculate the polynomial features for each feature by programming it or you can try <code>sklearn.preprocessing.PolynomialFeatures</code> which allows us to make polynomial terms of our data.</p>

<p>We will try degree 2, 3 and 4</p>

<pre><code class="language-python">X, y = dataset(show=False)
X_scaled = X/max(X)
y_scaled = y/max(y)
</code></pre>

<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2) 
                                        
X_2 = poly.fit_transform(X_scaled.reshape(-1,1))
print(X_2.shape)
print(X_2[0])
</code></pre>

<pre><code>(500, 3)
[ 1.         -1.00401606  1.00804826]
</code></pre>

<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=3) 
                                        
X_3 = poly.fit_transform(X_scaled.reshape(-1,1))
print(X_3.shape)
print(X_3[0])
</code></pre>

<pre><code>(500, 4)
[ 1.         -1.00401606  1.00804826 -1.01209664]
</code></pre>

<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=4) 
                                        
X_4 = poly.fit_transform(X_scaled.reshape(-1,1))
print(X_4.shape)
print(X_4[0])
</code></pre>

<pre><code>(500, 5)
[ 1.         -1.00401606  1.00804826 -1.01209664  1.01616129]
</code></pre>

<p>The PolynomialFeatures returns $[1, x, x^2, x^3,&hellip;]$.</p>

<h1 id="task-2">Task - 2</h1>

<ul>
<li>Train a model with polynomial terms in the dataset.</li>
<li>Visualize the prediction of the model</li>
</ul>

<p>The code remains the same except, the no of input features will be 3, 4, 5 respectively.</p>

<h2 id="tensorflow-model-with-2nd-degree">Tensorflow Model with 2nd Degree</h2>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[3])])

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

model.compile(optimizer=optimizer, loss='mean_squared_error')
tf_history = model.fit(X_2, y_scaled, epochs=500, verbose=True)

plt.plot(tf_history.history['loss'])
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.show()

mse = tf_history.history['loss'][-1]
y_hat = model.predict(X_2)

plt.figure(figsize=(12,7))
plt.title('TensorFlow Model')
plt.scatter(X_2[:, 1], y_scaled, label='Data $(X, y)$')
plt.plot(X_2[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0)
plt.xlabel('$X$', fontsize=20)
plt.ylabel('$y$', fontsize=20)
plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
</code></pre>

<pre><code>Epoch 1/500
500/500 [==============================] - 0s 332us/sample - loss: 0.5278
Epoch 2/500
500/500 [==============================] - 0s 38us/sample - loss: 0.4773
.
.
Epoch 499/500
500/500 [==============================] - 0s 44us/sample - loss: 0.0242
Epoch 500/500
500/500 [==============================] - 0s 43us/sample - loss: 0.0242
</code></pre>

<p><img src="../1_2/output_20_1.png" alt="png" /></p>

<p><img src="../1_2/output_20_2.png" alt="png" /></p>

<p>Why is the polynomial regression with 2-d features look like a straight line?</p>

<p>Well because the model thinks that a straight line(look like, we can&rsquo;t be sure if its a straight like, it can a parabola as well) better fits the dataset than a parabola. If you train the model for less epochs you can notice the model tries to fit the data with a parabola(2-d) but it improves as it moves to a line.</p>

<p>Train the same model for may be 50 epochs.</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[3])])

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

model.compile(optimizer=optimizer, loss='mean_squared_error')
tf_history = model.fit(X_2, y_scaled, epochs=50, verbose=True)

plt.plot(tf_history.history['loss'])
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.show()

mse = tf_history.history['loss'][-1]
y_hat = model.predict(X_2)

plt.figure(figsize=(12,7))
plt.title('TensorFlow Model')
plt.scatter(X_2[:, 1], y_scaled, label='Data $(X, y)$')
plt.plot(X_2[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0)
plt.xlabel('$X$', fontsize=20)
plt.ylabel('$y$', fontsize=20)
plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
</code></pre>

<pre><code>Epoch 1/50
500/500 [==============================] - 0s 370us/sample - loss: 0.8566
Epoch 2/50
500/500 [==============================] - 0s 38us/sample - loss: 0.7970
.
.
Epoch 49/50
500/500 [==============================] - 0s 38us/sample - loss: 0.1027
Epoch 50/50
500/500 [==============================] - 0s 35us/sample - loss: 0.1000
</code></pre>

<p><img src="../1_2/output_22_1.png" alt="png" /></p>

<p><img src="../1_2/output_22_2.png" alt="png" /></p>

<p>You can clearly see that the model tries to fit the data with a parabola which doen&rsquo;t seem to fit well, so it changes the parameters to fit a line.</p>

<h2 id="tensorflow-model-with-3rd-degree">Tensorflow Model with 3rd Degree</h2>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[4])])

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

model.compile(optimizer=optimizer, loss='mean_squared_error')
tf_history = model.fit(X_3, y_scaled, epochs=500, verbose=True)

plt.plot(tf_history.history['loss'])
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.show()

mse = tf_history.history['loss'][-1]
y_hat = model.predict(X_3)

plt.figure(figsize=(12,7))
plt.title('TensorFlow Model')
plt.scatter(X_3[:, 1], y_scaled, label='Data $(X, y)$')
plt.plot(X_3[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0)
plt.xlabel('$X$', fontsize=20)
plt.ylabel('$y$', fontsize=20)
plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
</code></pre>

<pre><code>Epoch 1/500
500/500 [==============================] - 0s 456us/sample - loss: 0.4445
Epoch 2/500
500/500 [==============================] - 0s 40us/sample - loss: 0.3993
.
.
Epoch 499/500
500/500 [==============================] - 0s 38us/sample - loss: 0.0040
Epoch 500/500
500/500 [==============================] - 0s 37us/sample - loss: 0.0039
</code></pre>

<p><img src="../1_2/output_25_1.png" alt="png" /></p>

<p><img src="../1_2/output_25_2.png" alt="png" /></p>

<p>3-D features perfectly fit the data with a 3rd degree polynomial as expected.</p>

<h1 id="tensorflow-model-with-4th-degree">Tensorflow Model with 4th Degree</h1>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[5])])

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

model.compile(optimizer=optimizer, loss='mean_squared_error')
tf_history = model.fit(X_4, y_scaled, epochs=500, verbose=True)

plt.plot(tf_history.history['loss'])
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.show()

mse = tf_history.history['loss'][-1]
y_hat = model.predict(X_4)

plt.figure(figsize=(12,7))
plt.title('TensorFlow Model')
plt.scatter(X_4[:, 1], y_scaled, label='Data $(X, y)$')
plt.plot(X_4[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0)
plt.xlabel('$X$', fontsize=20)
plt.ylabel('$y$', fontsize=20)
plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
</code></pre>

<pre><code>Epoch 1/500
500/500 [==============================] - 0s 240us/sample - loss: 0.5839
Epoch 2/500
500/500 [==============================] - 0s 37us/sample - loss: 0.5453
.
.
Epoch 499/500
500/500 [==============================] - 0s 37us/sample - loss: 0.0040
Epoch 500/500
500/500 [==============================] - 0s 39us/sample - loss: 0.0040
</code></pre>

<p><img src="../1_2/output_28_1.png" alt="png" /></p>

<p><img src="../1_2/output_28_2.png" alt="png" /></p>

<p>4th degree poly-regression also did a good job in fitting the data as it also have the 3rd degree terms.</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[5])])

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

model.compile(optimizer=optimizer, loss='mean_squared_error')
tf_history = model.fit(X_4, y_scaled, epochs=50, verbose=True)

plt.plot(tf_history.history['loss'])
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.show()

mse = tf_history.history['loss'][-1]
y_hat = model.predict(X_4)

plt.figure(figsize=(12,7))
plt.title('TensorFlow Model')
plt.scatter(X_4[:, 1], y_scaled, label='Data $(X, y)$')
plt.plot(X_4[:, 1], y_hat, color='red', label='Predicted Line $y = f(X)$',linewidth=4.0)
plt.xlabel('$X$', fontsize=20)
plt.ylabel('$y$', fontsize=20)
plt.text(0,0.70,'MSE = {:.3f}'.format(mse), fontsize=20)
plt.grid(True)
plt.legend(fontsize=20)
plt.show()
</code></pre>

<pre><code>Epoch 1/50
500/500 [==============================] - 0s 181us/sample - loss: 0.6724
Epoch 2/50
500/500 [==============================] - 0s 35us/sample - loss: 0.6104
.
.
Epoch 48/50
500/500 [==============================] - 0s 34us/sample - loss: 0.0661
Epoch 49/50
500/500 [==============================] - 0s 35us/sample - loss: 0.0655
Epoch 50/50
500/500 [==============================] - 0s 38us/sample - loss: 0.0648
</code></pre>

<p><img src="../1_2/output_30_1.png" alt="png" /></p>

<p><img src="../1_2/output_30_2.png" alt="png" /></p>

<p>If you run the 4th degree poly-regression for fewer epochs, you can notice, the model tries to fit a 4th(or higher than 3rd) degree polynomial but as the loss is high, the model changes it parameters to set the 4th degree terms to almost 0 and thus giving a 3rd degree polynomial as you train for more epochs.</p>

<p>This is polynomial regression. Yes, its easy. But one issue, as this was a toy dataset we know its a 3rd degree data, so we tried 2,3,4. But when the data is multi dimensional we cannot visualize the dataset, so its difficult to decide the degree. This is why you will see Neural Networks are awesome. They are End-End, they do not need several feature extraction from our side, they can extract necessary features of their own.</p>

<p><strong>Make a Higher degree (4th/5th degree) data and try polynomial regression on it. Also try different functions like exponents, trignometric..etc.</strong></p>

          </div>

          



          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/google-ml-academy/deeplearning/1.1/" rel="next"></a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/google-ml-academy/deeplearning/1.3/" rel="prev"></a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">
          Last updated on Aug 29, 2019
        </div>

      </article>

      <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2019 Shangeth Rajaa &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
  </p>
</footer>


    </main>
  </div>
</div>

    

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.3258b3a711acd6208568ec000de4beec.js"></script>

  </body>
</html>


