<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Shangeth Rajaa">

  
  
  
    
  
  <meta name="description" content="Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Gradient Descent We have seen what are the steps followed in Gradient Descent to minimize the loss function. Now let&rsquo;s get into more detail.
The objective of gradient descent is to find $W, b$ for which $\mathcal{L}(W, b) $ is minimum for given pairs of data $(X, y)$.
Example Let&rsquo;s forget about Machine learning for sometime and use gradient descent algorithm for optimization of simple functions.">

  
  <link rel="alternate" hreflang="en-us" href="/google-ml-academy/deeplearning/2.3/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="hsl(339, 90%, 68%)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.26264af3549d61c0ce873bd043df951e.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-134441268-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/google-ml-academy/deeplearning/2.3/">

  
  
  
  
    
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@shangethr">
  <meta property="twitter:creator" content="@shangethr">
  
  <meta property="og:site_name" content="Shangeth">
  <meta property="og:url" content="/google-ml-academy/deeplearning/2.3/">
  <meta property="og:title" content="Optimizers | Shangeth">
  <meta property="og:description" content="Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Gradient Descent We have seen what are the steps followed in Gradient Descent to minimize the loss function. Now let&rsquo;s get into more detail.
The objective of gradient descent is to find $W, b$ for which $\mathcal{L}(W, b) $ is minimum for given pairs of data $(X, y)$.
Example Let&rsquo;s forget about Machine learning for sometime and use gradient descent algorithm for optimization of simple functions."><meta property="og:image" content="/img/instructor.jpeg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-09-03T00:00:00&#43;01:00">
  
  <meta property="article:modified_time" content="2019-09-03T00:00:00&#43;01:00">
  

  

  

  <title>Optimizers | Shangeth</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" class="dark">
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Shangeth</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/google-ml-academy/">
            
            <span>Google ML Academy</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>



<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      




<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
  <input name="q" type="search" class="form-control" id="search-query" placeholder="Search..." autocomplete="off">
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/google-ml-academy/deeplearning/">Course Overview</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/google-ml-academy/deeplearning/1.1/">1.Intro to Deep Learning</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.1/">1.1.Linear Regression</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.2/">1.2.Assignment - Polynomial Regression</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.3/">1.3.Logistic Regression</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.4/">1.4.Assignment - Multiclass Classification</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.5/">1.5.Multi Layer Perceptron - Motivation</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/google-ml-academy/deeplearning/2.1/">2.Deep Neural Networks</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.1/">2.1.Neural Network Architectures</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.2/">2.2.Batch Training</a>
      </li>
      
      <li class="active">
        <a href="/google-ml-academy/deeplearning/2.3/">2.3.Optimizers</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.4/">2.4.Learning Rate</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.5/">2.5.Bias &amp; Variance</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.6/">2.6.Overfitting &amp; Regularization</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.7/">2.7.ANN - Medical Diagnosis</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.8/">2.8.ANN - Computer Vision</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.9/">2.9.ANN - Natural Language Processing</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      
      <p class="docs-toc-title">On this page</p>
      

      <nav id="TableOfContents">
<ul>
<li><a href="#gradient-descent">Gradient Descent</a>
<ul>
<li><a href="#example">Example</a>
<ul>
<li><a href="#step-1">Step - 1</a></li>
<li><a href="#step-2">Step - 2</a></li>
<li><a href="#step-3-learning-rate">Step - 3 Learning rate</a></li>
</ul></li>
</ul></li>
<li><a href="#gd-with-momentum">GD with Momentum</a>
<ul>
<li><a href="#momentum-in-numpy">Momentum in Numpy</a></li>
</ul></li>
<li><a href="#optimization-algorithms-with-tensorflow">Optimization algorithms with Tensorflow</a>
<ul>
<li><a href="#sgd-optimizer">SGD optimizer</a>
<ul>
<li><a href="#stochastic-gradient-descent-without-momentum">Stochastic gradient descent without momentum</a></li>
<li><a href="#stochastic-gradient-descent-with-momentum">Stochastic gradient descent with momentum</a></li>
</ul></li>
<li><a href="#adam-optimizer">Adam optimizer</a></li>
</ul></li>
</ul>
</nav>

      <ul class="nav toc-top">
        <li><a href="#">Back to top</a></li>
      </ul>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article" itemscope itemtype="http://schema.org/Article">

        <div class="docs-article-container">
          <h1 itemprop="name">Optimizers</h1>

          <div class="article-style" itemprop="articleBody">
            

<p><a href="https://colab.research.google.com/github/shangeth/Google-ML-Academy/blob/master/2-Deep-Neural-Networks/2_3_Optimizers.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>

<p><center><a href="https://github.com/shangeth/Google-ML-Academy/blob/master/2-Deep-Neural-Networks/2_3_Optimizers.ipynb" target="_parent"><svg class="octicon octicon-mark-github v-align-middle" height="30" viewBox="0 0 16 16" version="1.1" width="30" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg> Open in GitHub</a></center></p>

<p><center><h1><a href='https://shangeth.com/google-ml-academy/'>Google ML Academy 2019</a></h1></center>
<center><h3>Instructor: <a href='https://shangeth.com/'>Shangeth Rajaa</a></h3></center>
<hr></p>

<h1 id="gradient-descent">Gradient Descent</h1>

<p>We have seen what are the steps followed in Gradient Descent to minimize the loss function. Now let&rsquo;s get into more detail.</p>

<p>The objective of gradient descent is to find $W, b$ for which $\mathcal{L}(W, b) $ is minimum for given pairs of data $(X, y)$.</p>

<h2 id="example">Example</h2>

<p>Let&rsquo;s forget about Machine learning for sometime and use gradient descent algorithm for optimization of simple functions.</p>

<p>Given a function $y = f(x) = 0.08x^2+sin(x)$, the objective is to find $x$ at which $y = f(x)$ is minimum.</p>

<p>Let&rsquo;s use Numpy to solve this.</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def f(x):
    return 0.08* x**2 + np.sin(x)

x = np.arange(-10, 10, 0.01)
y = f(x)

plt.plot(x, y, label='$y = f(x)$')
plt.xlabel('$x$')
plt.ylabel('$y$')
plt.legend()
plt.show()
</code></pre>

<p><img src="../2_3/output_3_0.png" alt="png" /></p>

<p>We are going to use gradient descent to find $x$ for which this function is minimum.</p>

<h3 id="step-1">Step - 1</h3>

<p>Choose a random point</p>

<pre><code class="language-python">random_idx = np.random.choice(2000)
random_x = x[random_idx]

plt.plot(x, y, label='$y = f(x)$')
plt.scatter(random_x, f(random_x))
plt.xlabel('$x$')
plt.ylabel('$y$')
plt.legend()
plt.show()
</code></pre>

<p><img src="../2_3/output_6_0.png" alt="png" /></p>

<h3 id="step-2">Step - 2</h3>

<p>calculate $\dfrac{\partial \mathcal{y}}{\partial x}$.</p>

<p>Why? Let&rsquo;s visualize it</p>

<pre><code class="language-python">np.random.seed(60)

random_idx = np.random.choice(2000)
random_x = x[random_idx]

a = random_x
h = 0.01
x_tan = np.arange(a-1, a+1, 0.01)
fprime = (f(a+h)-f(a))/h
tan = f(a)+fprime*(x_tan-a)  

plt.figure(figsize=(12,8))
plt.plot(x, y, label='$y = f(x)$')
plt.plot(x_tan, tan, '--', label='gradient $\dfrac{\partial \mathcal{y}}{\partial x}$', linewidth=4.0)
plt.text(a+0.2,f(a+0.2),'$\dfrac{\partial \mathcal{y}}{\partial x}$='+'{:.3f}'.format(fprime), fontsize=20)
plt.scatter(random_x, f(random_x), s=200, color='black')
plt.xlabel('$x$', fontsize=20)
plt.ylabel('$y$', fontsize=20)
plt.legend(fontsize = 'xx-large')
plt.show()
</code></pre>

<p><img src="../2_3/output_8_0.png" alt="png" /></p>

<p>The gradient of $y = f(x) = 0.08x^2+sin(x)$ is $\dfrac{\partial \mathcal{y}}{\partial x} = 0.16x+cos(x)$</p>

<p>The gradient or slope $\dfrac{\partial \mathcal{y}}{\partial x}$ of the curve at the randomly selected point is negative. Which means the function will decrease as x increases and the function will increase as x decreases.</p>

<p>Our objective is to minimize the function, so we calculate $\dfrac{\partial \mathcal{y}}{\partial x}$, to decide in which direction to move, so we reach the minimum.</p>

<ul>
<li>if $\dfrac{\partial \mathcal{y}}{\partial x} &gt; 0$, we move towards left of decrease x</li>
<li>if $\dfrac{\partial \mathcal{y}}{\partial x} &lt; 0$, we move towards right or increase x</li>
<li>if $\dfrac{\partial \mathcal{y}}{\partial x} = 0$, which means we are already at a point of minimum</li>
</ul>

<p>But how much to increase x?</p>

<h3 id="step-3-learning-rate">Step - 3 Learning rate</h3>

<p>We now know, given a point should we increase or decrease x to reach minimum.</p>

<p>How much to increase or decrease x?</p>

<p>We change x with a factor called learning rate $\alpha$.
So we update x with $x := x - \alpha \frac{\partial \mathcal{y}}{\partial x}$</p>

<ul>
<li>when $\alpha$ is small, x is increased in small steps, so more iterations are required to reach minimum, but this will lead to more accurate steps.</li>
<li>when $\alpha$ is larger, x will increase in larger steps, so x will reach minimum y in few steps, but there is a risk of x skipping the minimum point, as x is increased in larger value.</li>
</ul>

<p>When do we stop?
We can iterate of any number of iterations, but when x reaches minimum y , then $\dfrac{\partial \mathcal{y}}{\partial x} = 0$</p>

<p>$\therefore x := x - \alpha . 0$, so x will remain same, even if you iterate more.</p>

<p>Let&rsquo;s code this.</p>

<pre><code class="language-python">import numpy as np

def gradient(x):
    return 0.16*x+np.cos(x)
</code></pre>

<pre><code class="language-python">np.random.seed(24) # change the seed to start from different point

x = np.arange(-10, 10, 0.01)
y = f(x)

random_idx = np.random.choice(2000)
x_iter = x[random_idx]

epochs = 100
lr = 0.2

for i in range(epochs):
    dy_dx = gradient(x_iter)    
    x_iter = x_iter - lr * dy_dx


    if i%20 == 0:
        a = x_iter
        h = 0.01
        x_tan = np.arange(a-1, a+1, 0.01)
        fprime = (f(a+h)-f(a))/h
        tan = f(a)+fprime*(x_tan-a)  

        plt.figure(figsize=(12,8))
        plt.plot(x, y, label='$y = f(x)$')
        plt.plot(x_tan, tan, '--', label='gradient $\dfrac{\partial \mathcal{y}}{\partial x}$', linewidth=4.0)
        plt.text(a+0.2,f(a+0.2),'$\dfrac{\partial \mathcal{y}}{\partial x}$='+'{:.3f}'.format(fprime), fontsize=20)
        plt.scatter(a, f(a), s=200, color='black')
        plt.xlabel('$x$', fontsize=20)
        plt.ylabel('$y$', fontsize=20)
        plt.legend(fontsize = 'xx-large')
        plt.show()



</code></pre>

<p><img src="../2_3/output_11_0.png" alt="png" /></p>

<p><img src="../2_3/output_11_1.png" alt="png" /></p>

<p><img src="../2_3/output_11_2.png" alt="png" /></p>

<p><img src="../2_3/output_11_3.png" alt="png" /></p>

<p><img src="../2_3/output_11_4.png" alt="png" /></p>

<pre><code class="language-python">np.random.seed(24) # change the seed to start from different point

x = np.arange(-10, 10, 0.01)
y = f(x)

random_idx = np.random.choice(2000)
x_iter = x[random_idx]

epochs = 100
lr = 0.2

x_list = []
y_list = []

for i in range(epochs):
    dy_dx = gradient(x_iter)
    
    x_iter = x_iter - lr * dy_dx
    x_list.append(x_iter)
    y_list.append(f(x_iter))

plt.figure(figsize=(12,8))
plt.plot(x, y, label='$y = f(x)$')
for i in range(len(x_list)):
    plt.scatter(x_list[i], y_list[i], s=200, color='black')
plt.xlabel('$x$', fontsize=20)
plt.ylabel('$y$', fontsize=20)
plt.legend(fontsize = 'xx-large')
plt.show()



</code></pre>

<p><img src="../2_3/output_12_0.png" alt="png" /></p>

<p>This is the path followed to reach the minimum.</p>

<p><strong>Run the code</strong> with
- large lr
- small lr
- more epochs
- less epochs</p>

<h1 id="gd-with-momentum">GD with Momentum</h1>

<p>The updation step is changed with</p>

<p>$v = \beta v + (1-\beta) \dfrac{\partial \mathcal{y}}{\partial x}$</p>

<p>$x = x - \alpha v$</p>

<p>This algorithms smoothens the updation process, by averaging the movement in other direction and forces x to move in the required direction which fastens the process.</p>

<h2 id="momentum-in-numpy">Momentum in Numpy</h2>

<pre><code class="language-python">np.random.seed(23) # change the seed to start from different point

x = np.arange(-10, 10, 0.01)
y = f(x)

random_idx = np.random.choice(2000)
x_iter = x[random_idx]

epochs = 100
lr = 0.2
v = 0
b = 0.9

for i in range(epochs):
    dy_dx = gradient(x_iter)
    v = b * v + (1-b) *  dy_dx   
    x_iter = x_iter - lr * v


    if i%20 == 0:
        a = x_iter
        h = 0.01
        x_tan = np.arange(a-1, a+1, 0.01)
        fprime = (f(a+h)-f(a))/h
        tan = f(a)+fprime*(x_tan-a)  

        plt.figure(figsize=(12,8))
        plt.plot(x, y, label='$y = f(x)$')
        plt.plot(x_tan, tan, '--', label='gradient $\dfrac{\partial \mathcal{y}}{\partial x}$', linewidth=4.0)
        plt.text(a+0.2,f(a+0.2),'$\dfrac{\partial \mathcal{y}}{\partial x}$='+'{:.3f}'.format(fprime), fontsize=20)
        plt.scatter(a, f(a), s=200, color='black')
        plt.xlabel('$x$', fontsize=20)
        plt.ylabel('$y$', fontsize=20)
        plt.legend(fontsize = 'xx-large')
        plt.show()



</code></pre>

<p><img src="../2_3/output_15_0.png" alt="png" /></p>

<p><img src="../2_3/output_15_1.png" alt="png" /></p>

<p><img src="../2_3/output_15_2.png" alt="png" /></p>

<p><img src="../2_3/output_15_3.png" alt="png" /></p>

<p><img src="../2_3/output_15_4.png" alt="png" /></p>

<pre><code class="language-python">np.random.seed(24) # change the seed to start from different point

x = np.arange(-10, 10, 0.01)
y = f(x)

random_idx = np.random.choice(2000)
x_iter = x[random_idx]

epochs = 200
lr = 0.1
v = 0
b = 0.9

x_list = []
y_list = []

for i in range(epochs):
    dy_dx = gradient(x_iter)
    
    v = b * v + (1-b) *  dy_dx   
    x_iter = x_iter - lr * v

    x_list.append(x_iter)
    y_list.append(f(x_iter))

plt.figure(figsize=(12,8))
plt.plot(x, y, label='$y = f(x)$')
for i in range(len(x_list)):
    plt.scatter(x_list[i], y_list[i], s=200, color='black')
plt.xlabel('$x$', fontsize=20)
plt.ylabel('$y$', fontsize=20)
plt.legend(fontsize = 'xx-large')
plt.show()



</code></pre>

<p><img src="../2_3/output_16_0.png" alt="png" /></p>

<h1 id="optimization-algorithms-with-tensorflow">Optimization algorithms with Tensorflow</h1>

<p>Check the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers" target="_blank">Tensorflow docs</a>.</p>

<p>So far we have been using &lsquo;Adam&rsquo; algorithm for optimization with default parameter values. We will now train models with different optimizers and hyper parameters.</p>

<pre><code class="language-python">from sklearn.datasets import make_gaussian_quantiles
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

X, y = make_gaussian_quantiles(n_samples=2000, n_features=2, n_classes=2, random_state=3, cov=0.1)

plt.figure(figsize=(10,10))
plt.scatter(X[:,0], X[:,1],c=y)
plt.grid(True)
plt.show()


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True)

print('Train = {}\nTest = {}\nVal = {}'.format(len(X_train), len(X_test), len(X_val)))
</code></pre>

<p><img src="../2_3/output_18_0.png" alt="png" /></p>

<pre><code>Train = 1120
Test = 600
Val = 280
</code></pre>

<h2 id="sgd-optimizer">SGD optimizer</h2>

<h3 id="stochastic-gradient-descent-without-momentum">Stochastic gradient descent without momentum</h3>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
tf.keras.backend.clear_session()

# random number initialized will to same, for reproducing same results.
np.random.seed(0)
tf.set_random_seed(0)

model = tf.keras.Sequential([
                             keras.layers.Dense(units=10, input_shape=[2]), 
                             keras.layers.Activation('tanh'),
                             keras.layers.Dense(units=10),
                             keras.layers.Activation('tanh'),
                             keras.layers.Dense(units=1), 
                             keras.layers.Activation('sigmoid')
                             ])

# we used the default optimizer parameters by using optimizer='adam'
# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# we can also define optimizer with
optimizer = tf.keras.optimizers.SGD(lr=0.01, nesterov=False)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
tf_history = model.fit(X_train, y_train, batch_size=100, epochs=50, verbose=True, validation_data=(X_val, y_val))


# contour plot
xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01]
grid = np.c_[xx.ravel(), yy.ravel()]
probs = model.predict(grid)[:,0].reshape(xx.shape)

f, ax = plt.subplots(figsize=(8, 6))
contour = ax.contourf(xx, yy, probs, 25, cmap=&quot;RdBu&quot;,
                      vmin=0, vmax=1)
ax_c = f.colorbar(contour)
ax_c.set_label(&quot;$P(y = 1)$&quot;)
ax_c.set_ticks([0, .25, .5, .75, 1])

ax.scatter(X[:,0], X[:, 1], c=y, s=50,
           cmap=&quot;RdBu&quot;, vmin=-.2, vmax=1.2,
           edgecolor=&quot;white&quot;, linewidth=1)

plt.show()

# test accuracy
from sklearn.metrics import accuracy_score
y_test_pred = model.predict(X_test)
test_accuracy = accuracy_score((y_test_pred &gt; 0.5), y_test)

print('\nTest Accuracy = ', test_accuracy)

</code></pre>

<pre><code>Train on 1120 samples, validate on 280 samples
Epoch 1/50
1120/1120 [==============================] - 0s 96us/sample - loss: 0.7056 - acc: 0.4991 - val_loss: 0.7119 - val_acc: 0.4536
Epoch 2/50
1120/1120 [==============================] - 0s 20us/sample - loss: 0.7048 - acc: 0.5000 - val_loss: 0.7114 - val_acc: 0.4643
.
.
Epoch 49/50
1120/1120 [==============================] - 0s 19us/sample - loss: 0.6910 - acc: 0.6366 - val_loss: 0.6997 - val_acc: 0.5929
Epoch 50/50
1120/1120 [==============================] - 0s 22us/sample - loss: 0.6910 - acc: 0.6348 - val_loss: 0.6991 - val_acc: 0.5714
</code></pre>

<p><img src="../2_3/output_20_1.png" alt="png" /></p>

<pre><code>Test Accuracy =  0.6633333333333333
</code></pre>

<h3 id="stochastic-gradient-descent-with-momentum">Stochastic gradient descent with momentum</h3>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
tf.keras.backend.clear_session()

# random number initialized will to same, for reproducing same results.
np.random.seed(0)
tf.set_random_seed(0)

model = tf.keras.Sequential([
                             keras.layers.Dense(units=10, input_shape=[2]), 
                             keras.layers.Activation('tanh'),
                             keras.layers.Dense(units=10),
                             keras.layers.Activation('tanh'),
                             keras.layers.Dense(units=1), 
                             keras.layers.Activation('sigmoid')
                             ])

# we used the default optimizer parameters by using optimizer='adam'
# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# we can also define optimizer with
optimizer = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
tf_history = model.fit(X_train, y_train, batch_size=100, epochs=50, verbose=True, validation_data=(X_val, y_val))


# contour plot
xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01]
grid = np.c_[xx.ravel(), yy.ravel()]
probs = model.predict(grid)[:,0].reshape(xx.shape)

f, ax = plt.subplots(figsize=(8, 6))
contour = ax.contourf(xx, yy, probs, 25, cmap=&quot;RdBu&quot;,
                      vmin=0, vmax=1)
ax_c = f.colorbar(contour)
ax_c.set_label(&quot;$P(y = 1)$&quot;)
ax_c.set_ticks([0, .25, .5, .75, 1])

ax.scatter(X[:,0], X[:, 1], c=y, s=50,
           cmap=&quot;RdBu&quot;, vmin=-.2, vmax=1.2,
           edgecolor=&quot;white&quot;, linewidth=1)

plt.show()

# test accuracy
from sklearn.metrics import accuracy_score
y_test_pred = model.predict(X_test)
test_accuracy = accuracy_score((y_test_pred &gt; 0.5), y_test)

print('\nTest Accuracy = ', test_accuracy)

</code></pre>

<pre><code>Train on 1120 samples, validate on 280 samples
Epoch 1/50
1120/1120 [==============================] - 0s 104us/sample - loss: 0.7047 - acc: 0.5080 - val_loss: 0.7094 - val_acc: 0.4821
Epoch 2/50
1120/1120 [==============================] - 0s 20us/sample - loss: 0.7002 - acc: 0.5000 - val_loss: 0.7033 - val_acc: 0.4714
.
.
Epoch 49/50
1120/1120 [==============================] - 0s 20us/sample - loss: 0.6351 - acc: 0.7714 - val_loss: 0.6531 - val_acc: 0.7214
Epoch 50/50
1120/1120 [==============================] - 0s 21us/sample - loss: 0.6307 - acc: 0.7750 - val_loss: 0.6463 - val_acc: 0.7393
</code></pre>

<p><img src="../2_3/output_22_1.png" alt="png" /></p>

<pre><code>Test Accuracy =  0.7866666666666666
</code></pre>

<p>You can note the difference, optimizer with momentum got accuracy of 0.78 in 50 epochs, whereas optimizer without momentum got 0.663 in 50 epochs</p>

<h2 id="adam-optimizer">Adam optimizer</h2>

<p><img src="https://www.math.purdue.edu/~nwinovic/figures/adam.png" alt="" />
Check this <a href="https://arxiv.org/abs/1412.6980" target="_blank">paper</a> to know about adam.</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
tf.keras.backend.clear_session()

# random number initialized will to same, for reproducing same results.
np.random.seed(0)
tf.set_random_seed(0)

model = tf.keras.Sequential([
                             keras.layers.Dense(units=10, input_shape=[2]), 
                             keras.layers.Activation('tanh'),
                             keras.layers.Dense(units=10),
                             keras.layers.Activation('tanh'),
                             keras.layers.Dense(units=1), 
                             keras.layers.Activation('sigmoid')
                             ])

# we used the default optimizer parameters by using optimizer='adam'
# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# we can also define optimizer with
optimizer = tf.keras.optimizers.Adam(lr=0.01)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
tf_history = model.fit(X_train, y_train, batch_size=100, epochs=50, verbose=True, validation_data=(X_val, y_val))


# contour plot
xx, yy = np.mgrid[-1.5:1.5:.01, -1.5:1.5:.01]
grid = np.c_[xx.ravel(), yy.ravel()]
probs = model.predict(grid)[:,0].reshape(xx.shape)

f, ax = plt.subplots(figsize=(8, 6))
contour = ax.contourf(xx, yy, probs, 25, cmap=&quot;RdBu&quot;,
                      vmin=0, vmax=1)
ax_c = f.colorbar(contour)
ax_c.set_label(&quot;$P(y = 1)$&quot;)
ax_c.set_ticks([0, .25, .5, .75, 1])

ax.scatter(X[:,0], X[:, 1], c=y, s=50,
           cmap=&quot;RdBu&quot;, vmin=-.2, vmax=1.2,
           edgecolor=&quot;white&quot;, linewidth=1)

plt.show()

# test accuracy
from sklearn.metrics import accuracy_score
y_test_pred = model.predict(X_test)
test_accuracy = accuracy_score((y_test_pred &gt; 0.5), y_test)

print('\nTest Accuracy = ', test_accuracy)

</code></pre>

<pre><code>Train on 1120 samples, validate on 280 samples
Epoch 1/50
1120/1120 [==============================] - 0s 120us/sample - loss: 0.6971 - acc: 0.5080 - val_loss: 0.6996 - val_acc: 0.6036
Epoch 2/50
1120/1120 [==============================] - 0s 24us/sample - loss: 0.6908 - acc: 0.5545 - val_loss: 0.6979 - val_acc: 0.5357
.
.
Epoch 49/50
1120/1120 [==============================] - 0s 33us/sample - loss: 0.0785 - acc: 0.9804 - val_loss: 0.0694 - val_acc: 0.9821
Epoch 50/50
1120/1120 [==============================] - 0s 25us/sample - loss: 0.0748 - acc: 0.9804 - val_loss: 0.0729 - val_acc: 0.9786
</code></pre>

<p><img src="../2_3/output_25_1.png" alt="png" /></p>

<pre><code>Test Accuracy =  0.9833333333333333
</code></pre>

<p>Adam was able to give an accuracy of 0.98 in the same 50 epochs.</p>

<p>So Optimization algorithms affect the learning performance a lot. A good(right) optimization algorithm can improve your performance a lot.</p>

<p>Check out the other optimizers in Tensorflow in the docs.</p>

          </div>

          



          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/google-ml-academy/deeplearning/2.2/" rel="next">Batch Training</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/google-ml-academy/deeplearning/2.4/" rel="prev">Learning Rate</a>
  </div>
  
</div>

          </div>
          
        </div>

        
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "shangeth-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


        
        <div class="body-footer">
          Last updated on Sep 3, 2019
        </div>

      </article>

      <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2019 Shangeth Rajaa &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
  </p>
</footer>


    </main>
  </div>
</div>

    

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//shangeth-com.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.3258b3a711acd6208568ec000de4beec.js"></script>

  </body>
</html>


