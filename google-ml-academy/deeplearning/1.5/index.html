<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Shangeth Rajaa">

  
  
  
    
  
  <meta name="description" content="Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Non-Linearity Note: We will look mostly at classification examples, but the same concepts apply to regression problems as well with a little change in using activation function(Sigmoid, Softmax which we learned in previous sections).
So far the datasets we have used are linearly seperable, which means they can be seperated by line(2-d), plane(3-d) and linear multi dimensional classifiers.">

  
  <link rel="alternate" hreflang="en-us" href="/google-ml-academy/deeplearning/1.5/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="hsl(339, 90%, 68%)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.26264af3549d61c0ce873bd043df951e.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-134441268-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/google-ml-academy/deeplearning/1.5/">

  
  
  
  
    
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@shangethr">
  <meta property="twitter:creator" content="@shangethr">
  
  <meta property="og:site_name" content="Shangeth">
  <meta property="og:url" content="/google-ml-academy/deeplearning/1.5/">
  <meta property="og:title" content="Motivation for Multi Layer Perceptron | Shangeth">
  <meta property="og:description" content="Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Non-Linearity Note: We will look mostly at classification examples, but the same concepts apply to regression problems as well with a little change in using activation function(Sigmoid, Softmax which we learned in previous sections).
So far the datasets we have used are linearly seperable, which means they can be seperated by line(2-d), plane(3-d) and linear multi dimensional classifiers."><meta property="og:image" content="/img/instructor.jpeg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-09-01T00:00:00&#43;01:00">
  
  <meta property="article:modified_time" content="2019-09-01T00:00:00&#43;01:00">
  

  

  

  <title>Motivation for Multi Layer Perceptron | Shangeth</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" class="dark">
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Shangeth</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/google-ml-academy/">
            
            <span>Google ML Academy</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>



<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      




<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
  <input name="q" type="search" class="form-control" id="search-query" placeholder="Search..." autocomplete="off">
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/google-ml-academy/deeplearning/">Course Overview</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/google-ml-academy/deeplearning/1.1/">1.Intro to Deep Learning</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.1/">1.1.Linear Regression</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.2/">1.2.Assignment - Polynomial Regression</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.3/">1.3.Logistic Regression</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.4/">1.4.Assignment - Multiclass Classification</a>
      </li>
      
      <li class="active">
        <a href="/google-ml-academy/deeplearning/1.5/">1.5.Multi Layer Perceptron - Motivation</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/google-ml-academy/deeplearning/2.1/">2.Deep Neural Networks</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.1/">2.1.Neural Network Architectures</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.2/">2.2.Batch Training</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.3/">2.3.Optimizers</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.4/">2.4.Learning Rate</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.5/">2.5.Bias &amp; Variance</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.6/">2.6.Overfitting &amp; Regularization</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.7/">2.7.ANN - Medical Diagnosis</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      
      <p class="docs-toc-title">On this page</p>
      

      <nav id="TableOfContents">
<ul>
<li><a href="#non-linearity">Non-Linearity</a>
<ul>
<li><a href="#visualize-linear-dataset-and-linear-model">Visualize Linear Dataset and Linear Model</a></li>
<li><a href="#train-a-logistic-regression-model-with-tensorflow-for-linearly-seperable-dataset">Train a Logistic Regression Model with TensorFlow for Linearly Seperable Dataset</a></li>
<li><a href="#visualize-non-linear-dataset-and-linear-model">Visualize Non-Linear Dataset and Linear Model</a></li>
<li><a href="#train-a-logistic-regression-model-with-tensorflow-for-non-linear-dataset">Train a Logistic Regression Model with TensorFlow for Non-Linear Dataset</a></li>
<li><a href="#another-example-more-difficult">Another Example (More difficult)</a></li>
</ul></li>
<li><a href="#non-linear-dataset-with-linear-classifier">Non-linear Dataset with Linear Classifier</a>
<ul>
<li><a href="#polynomial-terms">Polynomial Terms</a></li>
<li><a href="#train-logistic-regression-model-with-2nd-degree-polynomial-terms">Train Logistic Regression Model with 2nd degree polynomial terms</a></li>
</ul></li>
<li><a href="#combination-of-linear-classifiers">Combination of Linear Classifiers</a>
<ul>
<li><a href="#non-linear-data-and-linear-classifier">Non-linear data and Linear Classifier</a>
<ul>
<li><a href="#classifier-1">classifier 1</a></li>
<li><a href="#classifier-2">classifier 2</a></li>
</ul></li>
</ul></li>
<li><a href="#how-do-we-combine-the-linear-classifiers">How do we combine the Linear Classifiers?</a>
<ul>
<li><a href="#tensorflow-playground">Tensorflow Playground</a></li>
<li><a href="#mlp-in-tensorflow">MLP in Tensorflow</a></li>
</ul></li>
</ul>
</nav>

      <ul class="nav toc-top">
        <li><a href="#">Back to top</a></li>
      </ul>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article" itemscope itemtype="http://schema.org/Article">

        <div class="docs-article-container">
          <h1 itemprop="name">Motivation for Multi Layer Perceptron</h1>

          <div class="article-style" itemprop="articleBody">
            

<p><a href="https://colab.research.google.com/github/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_5_Motivation_for_Multi_Layer_Perceptron.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>

<p><center><a href="https://github.com/shangeth/Google-ML-Academy/blob/master/1-Intro-to-Deep-Learning/1_5_Motivation_for_Multi_Layer_Perceptron.ipynb" target="_parent"><svg class="octicon octicon-mark-github v-align-middle" height="30" viewBox="0 0 16 16" version="1.1" width="30" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg> Open in GitHub</a></center></p>

<p><center><h1><a href='https://shangeth.com/google-ml-academy/'>Google ML Academy 2019</a></h1></center>
<center><h3>Instructor: <a href='https://shangeth.com/'>Shangeth Rajaa</a></h3></center>
<hr></p>

<h1 id="non-linearity">Non-Linearity</h1>

<p>Note: We will look mostly at classification examples, but the same concepts apply to regression problems as well with a little change in using activation function(Sigmoid, Softmax which we learned in previous sections).</p>

<p>So far the datasets we have used are linearly seperable, which means they can be seperated by line(2-d), plane(3-d) and linear multi dimensional classifiers.</p>

<p>But in real world, not all datasets are 2-d(visualizable) and linearly seperable.</p>

<p><img src="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/more_images/linear_vs_nonlinear_problems.png" alt="" /></p>

<p><a href="https://leonardoaraujosantos.gitbooks.io" target="_blank">Image Source</a></p>

<p>Logistic Regression(Binary Classification) and Softmax Regression(Multi Class Classification) are linear models, they can only predict lines/planes/linear n-dim models to classify the data.
They are note good at classifying a non-linear data.</p>

<p>Let&rsquo;s visualize it on linear and non-linear data.</p>

<h2 id="visualize-linear-dataset-and-linear-model">Visualize Linear Dataset and Linear Model</h2>

<ul>
<li>Make a Linear Dataset</li>
<li>Train a Logisitc Regression</li>
<li>Predict the probabilities using the model for whole plot grid, and colour it based on if p&gt;0.5 or not.</li>
</ul>

<p>It&rsquo;s fine if you don&rsquo;t understand how we are going to visualize it, its just a matplotlib plot.</p>

<pre><code class="language-python">from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

X, y = make_blobs(n_samples=1000, n_features=2, centers=2, random_state=3)

# remember the code we wrote to get the index of 0, 1 classes and scatter it seperately
# that was to understand how to plot each class
# that plotting can also be done easily in a single line :D :p

plt.figure(figsize=(10,10))
plt.scatter(X[:,0], X[:,1],c=y)
plt.grid(True)
plt.show()
</code></pre>

<p><img src="../1_5/output_4_0.png" alt="png" /></p>

<h2 id="train-a-logistic-regression-model-with-tensorflow-for-linearly-seperable-dataset">Train a Logistic Regression Model with TensorFlow for Linearly Seperable Dataset</h2>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
import numpy as np

# get the dataset
X, y = make_blobs(n_samples=2000, n_features=2, centers=2, random_state=3)

# make train-validation split, let's ignore test set for now.
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=3)


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[2]), keras.layers.Activation('sigmoid')])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
tf_history = model.fit(X_train, y_train, epochs=20, verbose=True, validation_data=(X_val, y_val))
</code></pre>

<pre><code>Train on 2000 samples, validate on 600 samples
Epoch 1/20
2000/2000 [==============================] - 0s 204us/sample - loss: 0.4580 - acc: 0.7000 - val_loss: 0.3898 - val_acc: 0.7650
Epoch 2/20
2000/2000 [==============================] - 0s 49us/sample - loss: 0.3796 - acc: 0.7930 - val_loss: 0.3236 - val_acc: 0.8450
.
.
Epoch 19/20
2000/2000 [==============================] - 0s 43us/sample - loss: 0.0602 - acc: 0.9970 - val_loss: 0.0521 - val_acc: 1.0000
Epoch 20/20
2000/2000 [==============================] - 0s 41us/sample - loss: 0.0566 - acc: 0.9970 - val_loss: 0.0490 - val_acc: 1.0000
</code></pre>

<pre><code class="language-python">import numpy as np

xx, yy = np.mgrid[-10:10:.1, -10:10:.1]
grid = np.c_[xx.ravel(), yy.ravel()]
probs = model.predict(grid)[:,0].reshape(xx.shape)

f, ax = plt.subplots(figsize=(8, 6))
contour = ax.contourf(xx, yy, probs, 25, cmap=&quot;RdBu&quot;,
                      vmin=0, vmax=1)
ax_c = f.colorbar(contour)
ax_c.set_label(&quot;$P(y = 1)$&quot;)
ax_c.set_ticks([0, .25, .5, .75, 1])

ax.scatter(X[:,0], X[:, 1], c=y, s=50,
           cmap=&quot;RdBu&quot;, vmin=-.2, vmax=1.2,
           edgecolor=&quot;white&quot;, linewidth=1)

plt.show()
</code></pre>

<p><img src="../1_5/output_7_0.png" alt="png" /></p>

<p>This plot shows the probability of each point in the grid belongs to class &lsquo;1&rsquo;. We can see the white line is able to seperate almost all the points and accuracy is also good.</p>

<h2 id="visualize-non-linear-dataset-and-linear-model">Visualize Non-Linear Dataset and Linear Model</h2>

<ul>
<li>Make a Non-Linear Dataset with <code>sklearn.datasets.make_gaussian_quantiles</code>.</li>
<li>Train a Logisitc Regression</li>
<li>Predict the probabilities using the model for whole plot grid, and visualize the model classifier</li>
</ul>

<pre><code class="language-python">from sklearn.datasets import make_moons
import matplotlib.pyplot as plt

X, y = make_moons(n_samples=1000, random_state=3, noise=0.1)

idx = np.where(X[:,0] &lt; 1.1)
X = X[idx]
y = y[idx]

plt.figure(figsize=(10,10))
plt.scatter(X[:,0], X[:,1],c=y)
plt.grid(True)
plt.show()
</code></pre>

<p><img src="../1_5/output_10_0.png" alt="png" /></p>

<h2 id="train-a-logistic-regression-model-with-tensorflow-for-non-linear-dataset">Train a Logistic Regression Model with TensorFlow for Non-Linear Dataset</h2>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from sklearn.datasets import make_gaussian_quantiles
from sklearn.model_selection import train_test_split
import numpy as np

# get the dataset
# X, y = make_gaussian_quantiles(n_samples=1000, n_features=2, n_classes=2, random_state=3, cov=0.1)
X, y = make_moons(n_samples=1000, random_state=3, noise=0.1)

idx = np.where(X[:,0] &lt; 1.1)
X = X[idx]
y = y[idx]

# make train-validation split, let's ignore test set for now.
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=3)


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[2]), keras.layers.Activation('sigmoid')])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
tf_history = model.fit(X_train, y_train, epochs=20, verbose=True, validation_data=(X_val, y_val))
</code></pre>

<pre><code>Train on 764 samples, validate on 230 samples
Epoch 1/20
764/764 [==============================] - 0s 469us/sample - loss: 0.4796 - acc: 0.8338 - val_loss: 0.4810 - val_acc: 0.8130
Epoch 2/20
764/764 [==============================] - 0s 46us/sample - loss: 0.4747 - acc: 0.8351 - val_loss: 0.4766 - val_acc: 0.8087
.
.
Epoch 19/20
764/764 [==============================] - 0s 44us/sample - loss: 0.4223 - acc: 0.8364 - val_loss: 0.4281 - val_acc: 0.8174
Epoch 20/20
764/764 [==============================] - 0s 44us/sample - loss: 0.4202 - acc: 0.8364 - val_loss: 0.4261 - val_acc: 0.8174
</code></pre>

<pre><code class="language-python">import numpy as np

xx, yy = np.mgrid[-2:2:.01, -2:2:.01]
grid = np.c_[xx.ravel(), yy.ravel()]
probs = model.predict(grid)[:,0].reshape(xx.shape)

f, ax = plt.subplots(figsize=(8, 6))
contour = ax.contourf(xx, yy, probs, 25, cmap=&quot;RdBu&quot;,
                      vmin=0, vmax=1)
ax_c = f.colorbar(contour)
ax_c.set_label(&quot;$P(y = 1)$&quot;)
ax_c.set_ticks([0, .25, .5, .75, 1])

ax.scatter(X[:,0], X[:, 1], c=y, s=50,
           cmap=&quot;RdBu&quot;, vmin=-.2, vmax=1.2,
           edgecolor=&quot;white&quot;, linewidth=1)

plt.show()
</code></pre>

<p><img src="../1_5/output_13_0.png" alt="png" /></p>

<p>You can see the logistic regression model, cannot seperate a non-linear data. The model(white area) seperates the grid into 2 regions lienarly which is not a good classifier for this dataset.</p>

<p>Let&rsquo;s make another dataset</p>

<h2 id="another-example-more-difficult">Another Example (More difficult)</h2>

<pre><code class="language-python">from sklearn.datasets import make_gaussian_quantiles

X, y = make_gaussian_quantiles(n_samples=1000, n_features=2, n_classes=2, random_state=3, cov=0.1)

plt.figure(figsize=(10,10))
plt.scatter(X[:,0], X[:,1],c=y)
plt.grid(True)
plt.show()



# make train-validation split, let's ignore test set for now.
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=3)


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[2]), keras.layers.Activation('sigmoid')])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
tf_history = model.fit(X_train, y_train, epochs=20, verbose=True, validation_data=(X_val, y_val))


# contour plot
xx, yy = np.mgrid[-2:2:.01, -2:2:.01]
grid = np.c_[xx.ravel(), yy.ravel()]
probs = model.predict(grid)[:,0].reshape(xx.shape)

f, ax = plt.subplots(figsize=(8, 6))
contour = ax.contourf(xx, yy, probs, 25, cmap=&quot;RdBu&quot;,
                      vmin=0, vmax=1)
ax_c = f.colorbar(contour)
ax_c.set_label(&quot;$P(y = 1)$&quot;)
ax_c.set_ticks([0, .25, .5, .75, 1])

ax.scatter(X[:,0], X[:, 1], c=y, s=50,
           cmap=&quot;RdBu&quot;, vmin=-.2, vmax=1.2,
           edgecolor=&quot;white&quot;, linewidth=1)

plt.show()

</code></pre>

<p><img src="../1_5/output_16_0.png" alt="png" /></p>

<pre><code>Train on 1000 samples, validate on 300 samples
Epoch 1/20
1000/1000 [==============================] - 0s 375us/sample - loss: 0.7122 - acc: 0.5000 - val_loss: 0.6955 - val_acc: 0.5300
Epoch 2/20
1000/1000 [==============================] - 0s 44us/sample - loss: 0.7114 - acc: 0.4980 - val_loss: 0.6952 - val_acc: 0.5300
.
.
Epoch 19/20
1000/1000 [==============================] - 0s 48us/sample - loss: 0.7018 - acc: 0.4940 - val_loss: 0.6912 - val_acc: 0.5433
Epoch 20/20
1000/1000 [==============================] - 0s 48us/sample - loss: 0.7013 - acc: 0.4930 - val_loss: 0.6910 - val_acc: 0.5467
</code></pre>

<p><img src="../1_5/output_16_2.png" alt="png" /></p>

<p>Logistic Regression Classifier for this dataset is worse. It uses a straight line to seperate concentric circles. We cannot depend much on Linear classifiers on real world dataset(non-linear).</p>

<h1 id="non-linear-dataset-with-linear-classifier">Non-linear Dataset with Linear Classifier</h1>

<p>Hope you remember Polynomial Regression, when the dataset is non-linear, we gave the model the non linear terms, so the model can use it in linear fashion to make a non-linear regressor.</p>

<p>We are going to try the same for classifier.</p>

<h2 id="polynomial-terms">Polynomial Terms</h2>

<p>As the data is circular, 2nd degree polynomial terms will do good. Also try higher degree to see if accuracy increases.</p>

<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2) 

X, y = make_gaussian_quantiles(n_samples=2000, n_features=2, n_classes=2, random_state=3, cov=0.1)

X_2 = poly.fit_transform(X)
print(X_2.shape)
print(X_2[0])
</code></pre>

<pre><code>(2000, 6)
[1.         0.16654979 0.21185179 0.02773883 0.03528387 0.04488118]
</code></pre>

<h2 id="train-logistic-regression-model-with-2nd-degree-polynomial-terms">Train Logistic Regression Model with 2nd degree polynomial terms</h2>

<pre><code class="language-python">X, y = make_gaussian_quantiles(n_samples=2000, n_features=2, n_classes=2, random_state=3, cov=0.1)
X_2 = poly.fit_transform(X)

# make train-validation split, let's ignore test set for now.
X_train, X_val, y_train, y_val = train_test_split(X_2, y, test_size=0.3, random_state=3)


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[6]), keras.layers.Activation('sigmoid')])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
tf_history = model.fit(X_train, y_train, epochs=200, verbose=True, validation_data=(X_val, y_val))


# contour plot
xx, yy = np.mgrid[-2:2:.01, -2:2:.01]
grid = np.c_[xx.ravel(), yy.ravel()]
probs = model.predict(poly.fit_transform(grid))[:,0].reshape(xx.shape)

f, ax = plt.subplots(figsize=(8, 6))
contour = ax.contourf(xx, yy, probs, 25, cmap=&quot;RdBu&quot;,
                      vmin=0, vmax=1)
ax_c = f.colorbar(contour)
ax_c.set_label(&quot;$P(y = 1)$&quot;)
ax_c.set_ticks([0, .25, .5, .75, 1])

ax.scatter(X[:,0], X[:, 1], c=y, s=50,
           cmap=&quot;RdBu&quot;, vmin=-.2, vmax=1.2,
           edgecolor=&quot;white&quot;, linewidth=1)

plt.show()
</code></pre>

<pre><code>Train on 1400 samples, validate on 600 samples
Epoch 1/200
1400/1400 [==============================] - 1s 387us/sample - loss: 0.7530 - acc: 0.4914 - val_loss: 0.7292 - val_acc: 0.5200
Epoch 2/200
1400/1400 [==============================] - 0s 48us/sample - loss: 0.7375 - acc: 0.4914 - val_loss: 0.7169 - val_acc: 0.5200
.
.
Epoch 199/200
1400/1400 [==============================] - 0s 50us/sample - loss: 0.3552 - acc: 0.9164 - val_loss: 0.3886 - val_acc: 0.8933
Epoch 200/200
1400/1400 [==============================] - 0s 50us/sample - loss: 0.3543 - acc: 0.9136 - val_loss: 0.3876 - val_acc: 0.8967
</code></pre>

<p><img src="../1_5/output_22_1.png" alt="png" /></p>

<p>The model with 2nd degree data performed well, it got an accuracy of 0.91 in 200 epochs and seems to increase if you train longer.</p>

<p>The problem is, as we were able visualize this dataset, we concluded to include 2nd degree terms, but real world datasets are multi-dimensional, you cannot visualize those.</p>

<p>Multi-Layer Perceptron or usually called as Artificial neural Networks or simply Neural Networks can help us with this problem.</p>

<p>Note: Linear Regression, Logistic Regression are also Neurons(not Network). Softmax Regression is a network of several linear classifier so it is also a network. But we usually denote something as neural netowrk when it has a hidden layer, which we will discuss later.</p>

<h1 id="combination-of-linear-classifiers">Combination of Linear Classifiers</h1>

<h2 id="non-linear-data-and-linear-classifier">Non-linear data and Linear Classifier</h2>

<h3 id="classifier-1">classifier 1</h3>

<pre><code class="language-python">from sklearn.datasets import make_moons
import matplotlib.pyplot as plt

X, y = make_moons(n_samples=1000, random_state=3, noise=0.1)

idx = np.where(X[:,0] &lt; 1.1)
X = X[idx]
y = y[idx]

xline = np.arange(-1.3, 1.2, 0.01)
yline = xline + 0.5 # random classifier

plt.figure(figsize=(10,10))
plt.scatter(X[:,0], X[:,1],c=y)
plt.plot(xline, yline)
plt.grid(True)
plt.show()
</code></pre>

<p><img src="../1_5/output_25_0.png" alt="png" /></p>

<p>This line can be one of the classifier, but does not give a better accuracy, as yellow side also have few blue data points.</p>

<h3 id="classifier-2">classifier 2</h3>

<pre><code class="language-python">from sklearn.datasets import make_moons
import matplotlib.pyplot as plt

X, y = make_moons(n_samples=1000, random_state=3, noise=0.1)

idx = np.where(X[:,0] &lt; 1.1)
X = X[idx]
y = y[idx]

xline = np.arange(-1.3, 1.2, 0.01)
yline = - xline + 0.6 # random classifier

plt.figure(figsize=(10,10))
plt.scatter(X[:,0], X[:,1],c=y)
plt.plot(xline, yline)
plt.grid(True)
plt.show()
</code></pre>

<p><img src="../1_5/output_28_0.png" alt="png" /></p>

<p>This can also be a classifier but again it has few data mis classified.</p>

<p>Let&rsquo;s say there are 2 logistic regression classifiers
- classifier 1 - clf1
- classifier 2 - clf2</p>

<p>both are fine, but they also misclassifies some data points.</p>

<p>So both the classifiers individually cannot classify the data very well because the data is non-linear and the classifiers are linear.</p>

<p>But if we combine these 2 classifiers that can make a non-linear classifier.</p>

<pre><code class="language-python">from sklearn.datasets import make_moons
import matplotlib.pyplot as plt

X, y = make_moons(n_samples=1000, random_state=3, noise=0.1)

idx = np.where(X[:,0] &lt; 1.1)
X = X[idx]
y = y[idx]

xline1 = np.arange(-1.3, 0.01, 0.01)
yline1 = xline1 + 0.6

xline2 = np.arange(0, 1.2, 0.01)
yline2 = - xline2 + 0.6 # random classifier

plt.figure(figsize=(10,10))
plt.scatter(X[:,0], X[:,1],c=y)
plt.plot(xline1, yline1, color='b')
plt.plot(xline2, yline2, color='b')
plt.grid(True)
plt.show()
</code></pre>

<p><img src="../1_5/output_30_0.png" alt="png" /></p>

<p>This will be one of best classifier, by moving the 2 lines little more, we can get a accuracy of nearly 1.0 for this dataset. Also we can make this classifier more better by combining more number of linear classifiers.</p>

<p>So <strong>combination of linear classifiers can be a good non-linear classifier</strong>.</p>

<p>This is the primary motivation behind Artificial neural networks.</p>

<h1 id="how-do-we-combine-the-linear-classifiers">How do we combine the Linear Classifiers?</h1>

<p>We saw thet combining multiple linear classifiers can give a non-linear classifier. But how do we do that?</p>

<p>Lets assume we have 2 classifiers: $C_1, C_2$</p>

<p>$C_1 : \hat{y}_1 = \sigma(X.W_1+b_1)$</p>

<p>$C_2 : \hat{y}_2 = \sigma(X.W_2+b_2)$</p>

<p>So both $C_1, C_2$ gives the probability that a points belongs to class &lsquo;1&rsquo;. We need to combine $C_1, C_2$, so we can take the outputs of $C_1, C_2$ ie: $\hat{y}_1$ and $\hat{y}_2$ and send it to a new logistic regression model $C_3$.</p>

<p>But we no more need probabilities from $C_1, C_2$, so we may/maynot use sigmoid function. But we need to use some non-linearity as combination of linear function directly will be a linear function, which is of no use to us. So we can also use some other famous activations like tanh, ReLU, etc.
<br></p>

<p>$C_1 : \hat{y}_1 = g(X.W_1+b_1)$</p>

<p>$C_2 : \hat{y}_2 = g(X.W_2+b_2)$</p>

<p>g can be any activation, sigmoid, tanh, relu,..etc.</p>

<p>$X_{new} = [\hat{y}_1, \hat{y}_2]$</p>

<p>$C_3 : \hat{y} = \sigma(X_{new}.W_3 + b_3)$</p>

<p>This combination of Neurons is called Multi Layer Perceptron or ANN or Neural netowrks in general.</p>

<p><br></p>

<p>Let&rsquo;s implement this in Tensorflow.</p>

<p>Before that try this Tensorflow Playground</p>

<h2 id="tensorflow-playground">Tensorflow Playground</h2>

<ul>
<li><a href="https://playground.tensorflow.org/#activation=sigmoid&amp;batchSize=10&amp;dataset=xor&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=&amp;seed=0.90757&amp;showTestData=false&amp;discretize=true&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false" target="_blank">Linear Classifier</a></li>
<li><a href="https://playground.tensorflow.org/#activation=sigmoid&amp;batchSize=10&amp;dataset=xor&amp;regDataset=reg-plane&amp;learningRate=0.1&amp;regularizationRate=0&amp;noise=0&amp;networkShape=2&amp;seed=0.90757&amp;showTestData=false&amp;discretize=true&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false" target="_blank">Combination of 2 Linear Classifiers</a></li>
</ul>

<h2 id="mlp-in-tensorflow">MLP in Tensorflow</h2>

<pre><code class="language-python">X, y = make_moons(n_samples=2000, random_state=3, noise=0.1)

idx = np.where(X[:,0] &lt; 1.1)
X = X[idx]
y = y[idx]

# make train-validation split, let's ignore test set for now.
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=3)


model = tf.keras.Sequential([
                             keras.layers.Dense(units=2, input_shape=[2]),
                             keras.layers.Activation('tanh'),
                             keras.layers.Dense(units=1), 
                             keras.layers.Activation('sigmoid')])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
tf_history = model.fit(X_train, y_train, epochs=500, verbose=True, validation_data=(X_val, y_val))


# contour plot
xx, yy = np.mgrid[-1.5:1.5:.1, -1:1.5:.1]
grid = np.c_[xx.ravel(), yy.ravel()]
probs = model.predict(grid)[:,0].reshape(xx.shape)

f, ax = plt.subplots(figsize=(8, 6))
contour = ax.contourf(xx, yy, probs, 25, cmap=&quot;RdBu&quot;,
                      vmin=0, vmax=1)
ax_c = f.colorbar(contour)
ax_c.set_label(&quot;$P(y = 1)$&quot;)
ax_c.set_ticks([0, .25, .5, .75, 1])

ax.scatter(X[:,0], X[:, 1], c=y, s=50,
           cmap=&quot;RdBu&quot;, vmin=-.2, vmax=1.2,
           edgecolor=&quot;white&quot;, linewidth=1)

plt.show()
</code></pre>

<pre><code>Train on 1066 samples, validate on 457 samples
Epoch 1/500
1066/1066 [==============================] - 1s 776us/sample - loss: 0.5274 - acc: 0.7739 - val_loss: 0.5217 - val_acc: 0.7856
Epoch 2/500
1066/1066 [==============================] - 0s 61us/sample - loss: 0.5105 - acc: 0.7927 - val_loss: 0.5075 - val_acc: 0.7812
.
.
Epoch 499/500
1066/1066 [==============================] - 0s 63us/sample - loss: 0.0167 - acc: 0.9991 - val_loss: 0.0138 - val_acc: 1.0000
Epoch 500/500
1066/1066 [==============================] - 0s 62us/sample - loss: 0.0165 - acc: 0.9991 - val_loss: 0.0137 - val_acc: 1.0000
</code></pre>

<p><img src="../1_5/output_34_1.png" alt="png" /></p>

<p><strong>Yay! We just trained our first Neural Network</strong>(atleast for this course).</p>

<p>You can see the neural network model is as expected, it combined 2 linear models to give a better non-linear model.</p>

<p>Neural Networks are the state of the art(SOTA) for almost all the modern learning problems, there are many types of neural networks which we will learn later in the course.</p>

<p>Train the model with:
- different units in the first layer
- another linear layer and activation function
- different activation function
- different number of epochs
- different dataset(especially for the concentric circles dataset)</p>

          </div>

          



          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/google-ml-academy/deeplearning/1.4/" rel="next">Multi Class Classification</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/google-ml-academy/deeplearning/2.1/" rel="prev">Neural Network Architectures</a>
  </div>
  
</div>

          </div>
          
        </div>

        
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "shangeth-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


        
        <div class="body-footer">
          Last updated on Sep 1, 2019
        </div>

      </article>

      <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2019 Shangeth Rajaa &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
  </p>
</footer>


    </main>
  </div>
</div>

    

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//shangeth-com.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.3258b3a711acd6208568ec000de4beec.js"></script>

  </body>
</html>


