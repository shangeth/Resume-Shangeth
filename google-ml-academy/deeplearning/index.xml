<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Course Overview on Shangeth</title>
    <link>/google-ml-academy/deeplearning/</link>
    <description>Recent content in Course Overview on Shangeth</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year} Shangeth Rajaa</copyright>
    <lastBuildDate>Thu, 29 Aug 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/google-ml-academy/deeplearning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Linear Regression</title>
      <link>/google-ml-academy/deeplearning/1.1/</link>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/google-ml-academy/deeplearning/1.1/</guid>
      <description>Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Before starting with Neural Networks, we will look into 2 important machine learning models to understand regression and classification tasks - Linear Regression (Regression) - Logistic Regression (Classification)

Most of the Deep learning Courses do not start with linear regression(LinReg), but LinReg gave me a better understanding of machine learning, so i will start with that, hoping that will make the understanding of Neural networks easier.</description>
    </item>
    
    <item>
      <title>Polynomial Regression</title>
      <link>/google-ml-academy/deeplearning/1.2/</link>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/google-ml-academy/deeplearning/1.2/</guid>
      <description>Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Solutions Here:

 Open in GitHub
Its a very simple assignment, you can finish it in less than 10 minutes. If you are stuck somewhere refer this for solutions.
Task-1 : Linear Regression on Non-Linear Data  Get X and y from dataset() function Train a Linear Regression model for this dataset. Visualize the model prediction  Dataset Call dataset() function to get X, y</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>/google-ml-academy/deeplearning/1.3/</link>
      <pubDate>Fri, 30 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/google-ml-academy/deeplearning/1.3/</guid>
      <description>Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Logistic Regression is one of the most commonly used classification model. Unlike Linear Regression which predicts a real unbounded value $\hat{y} = f(X) = WX+b$, Logistic Regression predicts the probability of a data belonging to a particular class.

For example for a given data (X,y) where X is a recieved email and y is 0 if email is spam and 1 if email is not spam.</description>
    </item>
    
    <item>
      <title>Multi Class Classification</title>
      <link>/google-ml-academy/deeplearning/1.4/</link>
      <pubDate>Fri, 30 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>/google-ml-academy/deeplearning/1.4/</guid>
      <description>Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Solutions Here: 
 Open in GitHub
In the previous notebeook we used logistic regression for Binary Classification, now we will see how to train a classifier model for Multi-Class Classification.
What is Multi-Class Classification? If the target values have n discrete classification classes ie: y can take discrete value from 0 to n-1. If $y \in {0, 1, 2, 3, &amp;hellip;, n-1}$, then the classification task is n-Multi-Class.</description>
    </item>
    
    <item>
      <title>Motivation for Multi Layer Perceptron</title>
      <link>/google-ml-academy/deeplearning/1.5/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/google-ml-academy/deeplearning/1.5/</guid>
      <description>Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Non-Linearity Note: We will look mostly at classification examples, but the same concepts apply to regression problems as well with a little change in using activation function(Sigmoid, Softmax which we learned in previous sections).
So far the datasets we have used are linearly seperable, which means they can be seperated by line(2-d), plane(3-d) and linear multi dimensional classifiers.</description>
    </item>
    
    <item>
      <title>Neural Network Architectures</title>
      <link>/google-ml-academy/deeplearning/2.1/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/google-ml-academy/deeplearning/2.1/</guid>
      <description>Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
We trained our first neural network in the previous notebook which had 3 layers
 Input Layer Hidden Layer Output Layer  Multiple Nodes The network had 2 linear layers($C_1$,0 $C_2$) in the hidden layer each of which gave us a linear classifier, then we used another linear layer($C_3$) in the output layer to combine $C_1$ and $C_2$ to give us a non-linear classifier($C$).</description>
    </item>
    
    <item>
      <title>Batch Training</title>
      <link>/google-ml-academy/deeplearning/2.2/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/google-ml-academy/deeplearning/2.2/</guid>
      <description>Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Batch Training Batch Training is something very important but we didn&amp;rsquo;t use before as our dataset was smaller and we were just learning how to train models.
So far,
 we took each example $(X^i, y^i)$ made prediction with $\hat{y}^i = g(X^i.W+b)$ calculated the loss $\mathcal{L}(y^i , \hat{y}^i)$ used back propagation to update $W$ and $b$ (all parameters in the model) with $w := w - \alpha \frac{\partial \mathcal{L}}{\partial w}$ and $b := b - \alpha \frac{\partial \mathcal{L}}{\partial b}$ and repeated this process.</description>
    </item>
    
    <item>
      <title>Optimizers</title>
      <link>/google-ml-academy/deeplearning/2.3/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/google-ml-academy/deeplearning/2.3/</guid>
      <description>Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Gradient Descent We have seen what are the steps followed in Gradient Descent to minimize the loss function. Now let&amp;rsquo;s get into more detail.
The objective of gradient descent is to find $W, b$ for which $\mathcal{L}(W, b) $ is minimum for given pairs of data $(X, y)$.
Example Let&amp;rsquo;s forget about Machine learning for sometime and use gradient descent algorithm for optimization of simple functions.</description>
    </item>
    
    <item>
      <title>Learning Rate</title>
      <link>/google-ml-academy/deeplearning/2.4/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/google-ml-academy/deeplearning/2.4/</guid>
      <description>Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Learning Rate In Gradient Descent we update the parameters of the model with
$w := w - \alpha \frac{\partial \mathcal{L}}{\partial w}$ and $b := b - \alpha \frac{\partial \mathcal{L}}{\partial b}$.
The learning rate $\alpha$ actually affects the learning process a lot.
 small $\alpha$ is slow, but more accuracte, as it does not miss the minimum, but it also get stuck in the local minimum.</description>
    </item>
    
    <item>
      <title>Bias &amp; Variance</title>
      <link>/google-ml-academy/deeplearning/2.5/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0100</pubDate>
      
      <guid>/google-ml-academy/deeplearning/2.5/</guid>
      <description>Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
Bias &amp;amp; Variance Let us train a DNN model for a simple regression problem.
import numpy as np import matplotlib.pyplot as plt def dataset(show=True): X = np.arange(-5, 5, 0.01) y = 8 * np.sin(X) + np.random.randn(1000) if show: yy = 8 * np.sin(X) plt.figure(figsize=(15,9)) plt.scatter(X, y) plt.plot(X, yy, color=&#39;red&#39;, linewidth=7) plt.show() return X, y X, y = dataset(show=True)  Lets train 2 models for this dataset</description>
    </item>
    
  </channel>
</rss>