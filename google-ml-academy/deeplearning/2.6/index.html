<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Shangeth Rajaa">

  
  
  
    
  
  <meta name="description" content="Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
MNIST Dataset The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.
Let&rsquo;s get the dataset using tf.keras.datasets
Download MNIST import tensorflow as tf (x_train, y_train), (x_test, y_test) = tf.">

  
  <link rel="alternate" hreflang="en-us" href="/google-ml-academy/deeplearning/2.6/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="hsl(339, 90%, 68%)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.26264af3549d61c0ce873bd043df951e.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-134441268-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/google-ml-academy/deeplearning/2.6/">

  
  
  
  
    
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@shangethr">
  <meta property="twitter:creator" content="@shangethr">
  
  <meta property="og:site_name" content="Shangeth">
  <meta property="og:url" content="/google-ml-academy/deeplearning/2.6/">
  <meta property="og:title" content="Overfitting &amp; Regularization | Shangeth">
  <meta property="og:description" content="Open in GitHub
Google ML Academy 2019 Instructor: Shangeth Rajaa 
MNIST Dataset The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.
Let&rsquo;s get the dataset using tf.keras.datasets
Download MNIST import tensorflow as tf (x_train, y_train), (x_test, y_test) = tf."><meta property="og:image" content="/img/instructor.jpeg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-09-06T00:00:00&#43;01:00">
  
  <meta property="article:modified_time" content="2019-09-06T00:00:00&#43;01:00">
  

  

  

  <title>Overfitting &amp; Regularization | Shangeth</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" class="dark">
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Shangeth</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/google-ml-academy/">
            
            <span>Google ML Academy</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>



<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      




<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
  <input name="q" type="search" class="form-control" id="search-query" placeholder="Search..." autocomplete="off">
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/google-ml-academy/deeplearning/">Course Overview</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/google-ml-academy/deeplearning/1.1/">1.Intro to Deep Learning</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.1/">1.1.Linear Regression</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.2/">1.2.Assignment - Polynomial Regression</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.3/">1.3.Logistic Regression</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.4/">1.4.Assignment - Multiclass Classification</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/1.5/">1.5.Multi Layer Perceptron - Motivation</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/google-ml-academy/deeplearning/2.1/">2.Deep Neural Networks</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.1/">2.1.Neural Network Architectures</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.2/">2.2.Batch Training</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.3/">2.3.Optimizers</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.4/">2.4.Learning Rate</a>
      </li>
      
      <li >
        <a href="/google-ml-academy/deeplearning/2.5/">2.5.Bias &amp; Variance</a>
      </li>
      
      <li class="active">
        <a href="/google-ml-academy/deeplearning/2.6/">2.6.Overfitting &amp; Regularization</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      
      <p class="docs-toc-title">On this page</p>
      

      <nav id="TableOfContents">
<ul>
<li><a href="#mnist-dataset">MNIST Dataset</a>
<ul>
<li><a href="#download-mnist">Download MNIST</a></li>
<li><a href="#visualize-mnist">Visualize MNIST</a></li>
<li><a href="#scale-the-data">Scale the data</a></li>
<li><a href="#one-hot-encoding">One-Hot Encoding</a></li>
<li><a href="#flatten">Flatten</a></li>
<li><a href="#model">Model</a></li>
<li><a href="#training-the-model">Training the model</a></li>
</ul></li>
<li><a href="#weight-regularization">Weight Regularization</a>
<ul>
<li><a href="#regualrization-in-tensorflow">Regualrization in Tensorflow</a></li>
</ul></li>
<li><a href="#dropouts">Dropouts</a>
<ul>
<li><a href="#dropouts-in-tensorflow">Dropouts in Tensorflow</a></li>
</ul></li>
</ul>
</nav>

      <ul class="nav toc-top">
        <li><a href="#">Back to top</a></li>
      </ul>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article" itemscope itemtype="http://schema.org/Article">

        <div class="docs-article-container">
          <h1 itemprop="name">Overfitting &amp; Regularization</h1>

          <div class="article-style" itemprop="articleBody">
            

<p><a href="https://colab.research.google.com/github/shangeth/Google-ML-Academy/blob/master/2-Deep-Neural-Networks/2_6_Overfitting_Regularization.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>

<p><center><a href="https://github.com/shangeth/Google-ML-Academy/blob/master/2-Deep-Neural-Networks/2_6_Overfitting_Regularization.ipynb" target="_parent"><svg class="octicon octicon-mark-github v-align-middle" height="30" viewBox="0 0 16 16" version="1.1" width="30" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg> Open in GitHub</a></center></p>

<p><center><h1><a href='https://shangeth.com/google-ml-academy/'>Google ML Academy 2019</a></h1></center>
<center><h3>Instructor: <a href='https://shangeth.com/'>Shangeth Rajaa</a></h3></center>
<hr></p>

<h1 id="mnist-dataset">MNIST Dataset</h1>

<p>The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.</p>

<p><img src="https://www.researchgate.net/profile/Steven_Young11/publication/306056875/figure/fig1/AS:393921575309346@1470929630835/Example-images-from-the-MNIST-dataset.png" alt="" /></p>

<p>Let&rsquo;s get the dataset using tf.keras.datasets</p>

<h2 id="download-mnist">Download MNIST</h2>

<pre><code class="language-python">import tensorflow as tf

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')
</code></pre>

<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 0s 0us/step
</code></pre>

<h2 id="visualize-mnist">Visualize MNIST</h2>

<p>Let&rsquo;s visualize what is in the dataset</p>

<pre><code class="language-python">import matplotlib.pyplot as plt

num_imgs = 15
plt.figure(figsize=(num_imgs*2,3))

for i in range(1,num_imgs):
    plt.subplot(1,num_imgs,i).set_title('{}'.format(y_train[i]))
    plt.imshow(x_train[i], cmap='gray')
    plt.axis('off')
plt.show()
</code></pre>

<p><img src="../2_6/output_6_0.png" alt="png" /></p>

<h2 id="scale-the-data">Scale the data</h2>

<pre><code class="language-python">import numpy as np

print('Max = {}\nMin = {}'.format(np.max(x_train), np.min(x_train)))
</code></pre>

<pre><code>Max = 255
Min = 0
</code></pre>

<p>The maximum value in the image if 255 and minimum value is 0. So we need to scale it to a smaller range for faster convergence.</p>

<p>Let&rsquo;s divide by maximum value of 255, so set it to a range of [0, 1].</p>

<pre><code class="language-python">x_train = x_train/255
x_test = x_test/255

print('Max = {}\nMin = {}'.format(np.max(x_train), np.min(x_train)))
</code></pre>

<pre><code>Max = 1.0
Min = 0.0
</code></pre>

<h2 id="one-hot-encoding">One-Hot Encoding</h2>

<p>As this is a 10 class multiclass classification, we need to one hot encode the labels.</p>

<pre><code class="language-python">print(y_train[:5])
</code></pre>

<pre><code>[5 0 4 1 9]
</code></pre>

<pre><code class="language-python">num_classes = 10

y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

print(y_train[:5])
</code></pre>

<pre><code>[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
</code></pre>

<h2 id="flatten">Flatten</h2>

<p>As we are going to use ANNs which takes in vectors and classify/ regress it. But the MNIST images are 2-D matrix, so we cannot directly pass them to ANNs.
We need to flatten to the 2-D Matrix to 1-D vectors.</p>

<p><img src="=10x20" alt="" />
<img src="https://miro.medium.com/max/1790/1*Lzx2pNLpHjGTKcofsaSH1g.png" alt="Flattening" width="50%"/></p>

<p>This 3x3 2-D matrix was flattened into a 9 unit vector. So our 28x28 MNIST image will be flattened into a 784 units vector.</p>

<p><img src="https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/d5222c6e3d15770a.png" alt="" /></p>

<pre><code class="language-python">x = np.random.randn(3,3)
print(x)
print(x.shape)
print()

flattened_x = x.reshape(-1)
print(flattened_x)
print(flattened_x.shape)
</code></pre>

<pre><code>[[-0.09460654  0.70636938 -0.73136131]
 [ 0.9414648   0.89831745 -0.03268361]
 [ 1.27416493 -0.37996    -0.31976928]]
(3, 3)

[-0.09460654  0.70636938 -0.73136131  0.9414648   0.89831745 -0.03268361
  1.27416493 -0.37996    -0.31976928]
(9,)
</code></pre>

<p>This can also be done by the model easily with Flatten layer, which we will add to the model.</p>

<h2 id="model">Model</h2>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras

tf.keras.backend.clear_session()

input_shape = (28,28)
nclasses = 10

model = tf.keras.Sequential([
                             tf.keras.layers.Flatten(input_shape=input_shape),
                             tf.keras.layers.Dense(units=50), 
                             tf.keras.layers.Activation('tanh'),
                             tf.keras.layers.Dense(units=50),
                             tf.keras.layers.Activation('tanh'),
                             tf.keras.layers.Dense(units=nclasses), 
                             tf.keras.layers.Activation('sigmoid')
                             ])
model.summary()
</code></pre>

<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
dense (Dense)                (None, 50)                39250     
_________________________________________________________________
activation (Activation)      (None, 50)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 50)                2550      
_________________________________________________________________
activation_1 (Activation)    (None, 50)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                510       
_________________________________________________________________
activation_2 (Activation)    (None, 10)                0         
=================================================================
Total params: 42,310
Trainable params: 42,310
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<h2 id="training-the-model">Training the model</h2>

<pre><code class="language-python">optimizer = tf.keras.optimizers.Adam(lr=0.0005)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
tf_history = model.fit(x_train, y_train, batch_size=100, epochs=100, verbose=True, validation_data=(x_test, y_test))
</code></pre>

<pre><code>Train on 60000 samples, validate on 10000 samples
Epoch 1/100
60000/60000 [==============================] - 3s 53us/sample - loss: 0.7443 - acc: 0.8594 - val_loss: 0.3127 - val_acc: 0.9262
Epoch 2/100
60000/60000 [==============================] - 3s 52us/sample - loss: 0.2590 - acc: 0.9319 - val_loss: 0.2154 - val_acc: 0.9402
.
.
Epoch 99/100
60000/60000 [==============================] - 3s 51us/sample - loss: 9.9574e-05 - acc: 1.0000 - val_loss: 0.1855 - val_acc: 0.9722
Epoch 100/100
60000/60000 [==============================] - 3s 49us/sample - loss: 9.1121e-05 - acc: 1.0000 - val_loss: 0.1879 - val_acc: 0.9719
</code></pre>

<pre><code class="language-python">plt.figure(figsize=(20,7))

plt.subplot(1,2,1)
plt.plot(tf_history.history['loss'], label='Training Loss')
plt.plot(tf_history.history['val_loss'], label='Validation Loss')
plt.legend()

plt.subplot(1,2,2)
plt.plot(tf_history.history['acc'], label='Training Accuracy')
plt.plot(tf_history.history['val_acc'], label='Validation Accuracy')
plt.legend()
plt.show()
</code></pre>

<p><img src="../2_6/output_21_0.png" alt="png" /></p>

<p>You can observe from this learning curve, as we train more, the training performance improves, but the validation performance goes in reverse direction. This is because the model tries to overfit the training data to improve training data and does not generalize. We have few methods to avoid overfitting.
Adding More data always work, but its not always possible get more data, so few regularization techniques may help.</p>

<h1 id="weight-regularization">Weight Regularization</h1>

<p>$\mathcal{L} = \dfrac{1}{m} \sum_{i=1}^{m}\mathcal{L}(\hat{y}^i, y^i)$</p>

<p>This is the loss which we want to reduce suring gradient descent.</p>

<p>We will add some new terms to the loss function for regularization</p>

<p>$\mathcal{L} = \dfrac{1}{m} \sum_{i=1}^{m}\mathcal{L}(\hat{y}^i, y^i) + \dfrac{\lambda}{2m}\sum_{l=1}^{L}||w^l||^2_2$</p>

<p>where $\lambda$ is called regularization parameter. This is L2 regularization as L2 norm is used.</p>

<p>The objective of any optimization algorithm is to reduce the Loss, now the loss has 2 terms</p>

<ul>
<li>MSE or Cross Entropy(can be other loss too)</li>
<li>Regularization term</li>
</ul>

<p>To reduce the loss, the optimizer should also reduce $\dfrac{\lambda}{2m}\sum_{l=1}^{L}||w^l||^2_2$ term, which means the weights are also reduced or penalized. How much the weights are penalized depends on $\lambda$, if $\lambda$ is high then the weights are penalized more. Penalizing weights also penalize the activations, which makes a more complex activations to be simpler, so the model does not overfit.</p>

<p>You don&rsquo;t need to implement it, you can regularize the weights in keras easily with kernel_regularizer.</p>

<h2 id="regualrization-in-tensorflow">Regualrization in Tensorflow</h2>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras

tf.keras.backend.clear_session()

input_shape = (28,28)
nclasses = 10

model = tf.keras.Sequential([
                             tf.keras.layers.Flatten(input_shape=input_shape),
                             tf.keras.layers.Dense(units=50, kernel_regularizer=tf.keras.regularizers.l2(0.0001)), 
                             tf.keras.layers.Activation('tanh'),
                             tf.keras.layers.Dense(units=50, kernel_regularizer=tf.keras.regularizers.l2(0.0001)),
                             tf.keras.layers.Activation('tanh'),
                             tf.keras.layers.Dense(units=nclasses), 
                             tf.keras.layers.Activation('sigmoid')
                             ])
model.summary()
</code></pre>

<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
dense (Dense)                (None, 50)                39250     
_________________________________________________________________
activation (Activation)      (None, 50)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 50)                2550      
_________________________________________________________________
activation_1 (Activation)    (None, 50)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                510       
_________________________________________________________________
activation_2 (Activation)    (None, 10)                0         
=================================================================
Total params: 42,310
Trainable params: 42,310
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<pre><code class="language-python">optimizer = tf.keras.optimizers.Adam(lr=0.0005)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
tf_history_reg = model.fit(x_train, y_train, batch_size=100, epochs=100, verbose=True, validation_data=(x_test, y_test))
</code></pre>

<pre><code>Train on 60000 samples, validate on 10000 samples
Epoch 1/100
60000/60000 [==============================] - 3s 55us/sample - loss: 0.7775 - acc: 0.8574 - val_loss: 0.3297 - val_acc: 0.9241
Epoch 2/100
60000/60000 [==============================] - 3s 52us/sample - loss: 0.2782 - acc: 0.9320 - val_loss: 0.2294 - val_acc: 0.9450
.
.
Epoch 99/100
60000/60000 [==============================] - 3s 52us/sample - loss: 0.0249 - acc: 0.9988 - val_loss: 0.1390 - val_acc: 0.9724
Epoch 100/100
60000/60000 [==============================] - 3s 52us/sample - loss: 0.0279 - acc: 0.9977 - val_loss: 0.1359 - val_acc: 0.9741
</code></pre>

<pre><code class="language-python">plt.figure(figsize=(20,7))

plt.subplot(1,2,1)
plt.plot(tf_history.history['loss'], label='Training Loss')
plt.plot(tf_history.history['val_loss'], label='Validation Loss')
plt.plot(tf_history_reg.history['loss'], label='Training Loss with Reg')
plt.plot(tf_history_reg.history['val_loss'], label='Validation Loss with Reg', linestyle='--')
plt.legend()

plt.subplot(1,2,2)
plt.plot(tf_history.history['acc'], label='Training Accuracy')
plt.plot(tf_history.history['val_acc'], label='Validation Accuracy')
plt.plot(tf_history_reg.history['acc'], label='Training Accuracy with Reg')
plt.plot(tf_history_reg.history['val_acc'], label='Validation Accuracy with Reg', linestyle='--')
plt.legend()
plt.show()
</code></pre>

<p><img src="../2_6/output_27_0.png" alt="png" /></p>

<p>L2 Regularization slightly improved the performance on validation set.</p>

<p>Try changing the l2 regularization $\lambda$ and observer the performance.</p>

<h1 id="dropouts">Dropouts</h1>

<p>Dropout drops certain nodes in our network during each pass with a defined probability p. So if p=0.5, then each node have a probability of 0.5 to get dropped.</p>

<p>This will make the network simpler duirng each pass depending on p and now the network cannot rely on any node as it may be dropped, so the network will not give high weights to any node as it may be dropped any time and every node will be utilized equally as all of them have equal probability of being dropped.</p>

<p><img src="https://miro.medium.com/proxy/1*iWQzxhVlvadk6VAJjsgXgg.png" alt="" /></p>

<p>Again this can be easily used in keras with tf.keras.layers.Dropout(p).</p>

<h2 id="dropouts-in-tensorflow">Dropouts in Tensorflow</h2>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras

tf.keras.backend.clear_session()

input_shape = (28,28)
nclasses = 10

model = tf.keras.Sequential([
                             tf.keras.layers.Flatten(input_shape=input_shape),
                             tf.keras.layers.Dense(units=50), 
                             tf.keras.layers.Activation('tanh'),
                             tf.keras.layers.Dropout(0.2),
                             tf.keras.layers.Dense(units=50),
                             tf.keras.layers.Activation('tanh'),
                             tf.keras.layers.Dropout(0.2),
                             tf.keras.layers.Dense(units=nclasses), 
                             tf.keras.layers.Activation('sigmoid')
                             ])
model.summary()
</code></pre>

<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
dense (Dense)                (None, 50)                39250     
_________________________________________________________________
activation (Activation)      (None, 50)                0         
_________________________________________________________________
dropout (Dropout)            (None, 50)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 50)                2550      
_________________________________________________________________
activation_1 (Activation)    (None, 50)                0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 50)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                510       
_________________________________________________________________
activation_2 (Activation)    (None, 10)                0         
=================================================================
Total params: 42,310
Trainable params: 42,310
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<pre><code class="language-python">optimizer = tf.keras.optimizers.Adam(lr=0.0001)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
tf_history_dp = model.fit(x_train, y_train, batch_size=100, epochs=100, verbose=True, validation_data=(x_test, y_test))
</code></pre>

<pre><code>Train on 60000 samples, validate on 10000 samples
Epoch 1/100
60000/60000 [==============================] - 4s 60us/sample - loss: 1.6479 - acc: 0.6138 - val_loss: 1.1078 - val_acc: 0.8455
Epoch 2/100
60000/60000 [==============================] - 3s 53us/sample - loss: 0.9871 - acc: 0.8051 - val_loss: 0.6767 - val_acc: 0.8874
.
.
Epoch 99/100
60000/60000 [==============================] - 3s 54us/sample - loss: 0.1294 - acc: 0.9605 - val_loss: 0.1068 - val_acc: 0.9677
Epoch 100/100
60000/60000 [==============================] - 3s 55us/sample - loss: 0.1294 - acc: 0.9605 - val_loss: 0.1065 - val_acc: 0.9680
</code></pre>

<pre><code class="language-python">plt.figure(figsize=(20,7))

plt.subplot(1,2,1)
plt.plot(tf_history.history['loss'], label='Training Loss')
plt.plot(tf_history.history['val_loss'], label='Validation Loss')
plt.plot(tf_history_dp.history['loss'], label='Training Loss with Dp')
plt.plot(tf_history_dp.history['val_loss'], label='Validation Loss with Dp', linestyle='--')
plt.legend()

plt.subplot(1,2,2)
plt.plot(tf_history.history['acc'], label='Training Accuracy')
plt.plot(tf_history.history['val_acc'], label='Validation Accuracy')
plt.plot(tf_history_dp.history['acc'], label='Training Accuracy with Dp')
plt.plot(tf_history_dp.history['val_acc'], label='Validation Accuracy with Dp', linestyle='--')
plt.legend()
plt.show()
</code></pre>

<p><img src="../2_6/output_33_0.png" alt="png" /></p>

          </div>

          



          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/google-ml-academy/deeplearning/2.5/" rel="next">Bias &amp; Variance</a>
  </div>
  
  
</div>

          </div>
          
        </div>

        
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "shangeth-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


        
        <div class="body-footer">
          Last updated on Sep 6, 2019
        </div>

      </article>

      <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2019 Shangeth Rajaa &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
  </p>
</footer>


    </main>
  </div>
</div>

    

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//shangeth-com.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.3258b3a711acd6208568ec000de4beec.js"></script>

  </body>
</html>


