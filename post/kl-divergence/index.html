<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Shangeth Rajaa">

  
  
  
    
  
  <meta name="description" content="Colab Notebook
Before seeing KL Divergence, let&rsquo;s see a very simple concept called Entropy
Entropy Entropy is the expected information contained in a Distribution. It measures the uncertainty.
$H(x) = \sum{p(x)I(x)}$ where $I(x)$ is called the Information content of $x$.
&ldquo;If an event is very probable, it is no surprise (and generally uninteresting) when that event happens as expected. However, if an event is unlikely to occur, it has much more information to learn that the event happened or will happen.">

  
  <link rel="alternate" hreflang="en-us" href="/post/kl-divergence/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="hsl(339, 90%, 68%)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.04bf00581ae5351cdd9e3f0836914cc1.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-134441268-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/kl-divergence/">

  
  
  
  
    
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@shangethr">
  <meta property="twitter:creator" content="@shangethr">
  
  <meta property="og:site_name" content="Shangeth">
  <meta property="og:url" content="/post/kl-divergence/">
  <meta property="og:title" content="KL Divergence | Shangeth">
  <meta property="og:description" content="Colab Notebook
Before seeing KL Divergence, let&rsquo;s see a very simple concept called Entropy
Entropy Entropy is the expected information contained in a Distribution. It measures the uncertainty.
$H(x) = \sum{p(x)I(x)}$ where $I(x)$ is called the Information content of $x$.
&ldquo;If an event is very probable, it is no surprise (and generally uninteresting) when that event happens as expected. However, if an event is unlikely to occur, it has much more information to learn that the event happened or will happen."><meta property="og:image" content="/img/instructor2.jpeg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2020-01-26T02:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2020-01-26T02:00:00&#43;00:00">
  

  

  

  <title>KL Divergence | Shangeth</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" class="dark">
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Shangeth</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/">
            
            <span>Deep Learning Course</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">KL Divergence</h1>

  
  <p class="page-subtitle">Entropy, KL Divergence and Cross Entropy in PyTorch</p>
  

  
    



<meta content="2020-01-26 02:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2020-01-26 02:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/admin/">Shangeth Rajaa</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Jan 26, 2020</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    4 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="/post/kl-divergence/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    <a href="/categories/deep-learning/">Deep Learning</a>, <a href="/categories/python/">Python</a>, <a href="/categories/information-theory/">Information Theory</a></span>
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=&amp;url="
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u="
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=&amp;title="
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=&amp;title="
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=&amp;body=">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <p><a href="https://colab.research.google.com/drive/1EXb4dW-jVaYsYOpYvLRZ5WVhivh-aglU">Colab Notebook</a></p>
<p>Before seeing KL Divergence, let&rsquo;s see a very simple concept called Entropy</p>
<h1 id="entropy">Entropy</h1>
<p>Entropy is the expected information contained in a Distribution. It measures the uncertainty.</p>
<p>$H(x) = \sum{p(x)I(x)}$ where $I(x)$ is called the Information content of $x$.</p>
<p>&ldquo;If an event is very probable, it is no surprise (and generally uninteresting) when that event happens as expected. However, if an event is unlikely to occur, it has much more information to learn that the event happened or will happen. For instance, the knowledge that some particular number will not be the winning number of a lottery provides very little information, because any particular chosen number will almost certainly not win. However, the knowledge that a particular number will win a lottery has a high value because it communicates the outcome of a very low probability event. &quot;</p>
<p><br>
So the information content is an increasing function of inverse of its probability. More the probability, lesser the information content and vice versa.
${\bf I(x) = log_2(\dfrac{1}{p(x)}) = -log_2(p(x))}$</p>
<p>Therefore,
${\bf H(x) = \sum{p(x)I(x)} = -\sum{p(x)log_2(p(x))}}$
and for continuous case ${\bf H(x) = -\int{p(x)log_2(p(x))}}$.</p>
<h2 id="entropy-in-pytorch">Entropy in Pytorch</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">entropy</span>(p, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-8</span>):
    <span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">    epsilon to avoid log(0) error
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    logp <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>log2(p <span style="color:#f92672">+</span> epsilon)
    e <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sum(<span style="color:#f92672">-</span>p<span style="color:#f92672">*</span>logp)
    <span style="color:#66d9ef">return</span> e

<span style="color:#75715e"># probability of choosing [a, b, c, d]</span>
p1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor([<span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.4</span>, <span style="color:#ae81ff">0.3</span>])
p2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor([<span style="color:#ae81ff">0.9</span>, <span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.1</span>])

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Entropy of Distribution {} = {}&#39;</span><span style="color:#f92672">.</span>format(p1<span style="color:#f92672">.</span>numpy(), entropy(p1)))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Entropy of Distribution {} = {}&#39;</span><span style="color:#f92672">.</span>format(p2<span style="color:#f92672">.</span>numpy(), entropy(p2)))
</code></pre></div><pre><code>Entropy of Distribution [0.1 0.2 0.4 0.3] = 1.846439242362976
Entropy of Distribution [0.9 0.  0.  0.1] = 0.4689956307411194
</code></pre>
<p>The first distribution has more entropy(uncertainty) as when we sample a data, there is a fair chance of the sampled data is from any of a, b, c, d.</p>
<p>But the second distribution is almost deterministic and has less uncertainty.</p>
<h1 id="kullback-leibler-divergence">Kullback-Leibler Divergence</h1>
<p>KL Divergence measures the similarity of two probability distribution. Let $P$ and $Q$ be two probability distributions. For ex: Let $P$ be a Gaussian Distribution and $Q$ be Uniform.</p>
<p>The Kullback-Leibler Divergence is</p>
<p>${\bf D_{KL}(p(x)||q(x)) = \int_{-\infty}^{+\infty} p(x)ln{\dfrac{p(x)}{q(x)}}}$</p>
<p><br>
For discrete distribution,</p>
<p>${\bf D_{KL}(p(x)||q(x)) = \sum p(x)ln{\dfrac{p(x)}{q(x)}}}$</p>
<h2 id="properties-of-kl-divergence">Properties of KL Divergence</h2>
<ul>
<li>KLdivergence is not a distance measure or &ldquo;metric&rdquo; measure.</li>
<li>It is not symmetric ie: $D_{KL}(p(x)||q(x)) \neq D_{KL}(q(x)||p(x))$.</li>
<li>It need not satisfy the triangular inequality.</li>
<li>It is a non-negative measure ie:$D_{KL}(p(x)||q(x)) \geq 0$ and $D_{KL}(p(x)||q(x)) = 0$ if and only if $p(x)=q(x)$.</li>
</ul>
<h2 id="kl-divergence-in-pytorch">KL Divergence in PyTorch</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">kl_divergence</span>(p, q, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-8</span>):
    <span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">    epsilon to avoid log(0) or divided by 0 error
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    d_kl <span style="color:#f92672">=</span> (p <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>log((p <span style="color:#f92672">/</span> (q<span style="color:#f92672">+</span> epsilon)) <span style="color:#f92672">+</span> epsilon))<span style="color:#f92672">.</span>sum()
    <span style="color:#66d9ef">return</span> d_kl

<span style="color:#75715e"># comparison of kl of similar distribution p2-p3 </span>
<span style="color:#75715e"># and dissimilar distribution p1-p2</span>
p1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor([<span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.4</span>, <span style="color:#ae81ff">0.3</span>])
p2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor([<span style="color:#ae81ff">0.9</span>, <span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.1</span>])
p3 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor([<span style="color:#ae81ff">0.8</span>, <span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.05</span>, <span style="color:#ae81ff">0.15</span>])

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;KL Divergence of {} and {} = {}&#39;</span><span style="color:#f92672">.</span>format(p1<span style="color:#f92672">.</span>numpy(), p1<span style="color:#f92672">.</span>numpy(), kl_divergence(p1, p2)))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;KL Divergence of {} and {} = {}&#39;</span><span style="color:#f92672">.</span>format(p2<span style="color:#f92672">.</span>numpy(), p3<span style="color:#f92672">.</span>numpy(), kl_divergence(p2, p3)))
</code></pre></div><pre><code>KL Divergence of [0.1 0.2 0.4 0.3] and [0.1 0.2 0.4 0.3] = 10.47386646270752
KL Divergence of [0.9 0.  0.  0.1] and [0.8  0.   0.05 0.15] = 0.06545820087194443
</code></pre>
<h1 id="mutual-information-with-kl-divergence">Mutual Information with KL Divergence</h1>
<p>Let random variable $x$ and $y$ has distribution of $p(x)$ and $p(y)$.
If $x$ and $y$ are independent, then $p(x,y) = p(x)p(y)$.</p>
<p>If the distance between $p(x,y)$ and $p(x)p(y)$ becomes larger, $p(x)$ and $p(y)$ becomes dependent. We can use KL Divergence of $p(x,y)$ and $p(x)p(y)$ to measure the dependency of x and y.</p>
<p>${\bf I(x, y) = D_{KL}(p(x,y)||p(x)p(y)) = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} p(x, y)ln{\dfrac{p(x, y)}{p(x)p(y)}}}$</p>
<h1 id="cross-entropy">Cross Entropy</h1>
<p>The Kullback-Leibler Divergence of $p(x)$ and $q(x)$ is given by</p>
<p>$D_{KL}(p(x)||q(x)) = \int_{-\infty}^{+\infty} p(x)ln{\dfrac{p(x)}{q(x)}}$</p>
<p>$\qquad = \int_{-\infty}^{+\infty} p(x)ln(p(x))-\int_{-\infty}^{+\infty}p(x)ln(q(x))$</p>
<p>$D_{KL}(p(x)||q(x)) = - H(p(x)) -\int_{-\infty}^{+\infty}p(x)ln(q(x))$</p>
<p>$D_{KL}(p(x)||q(x)) + H(p(x)) = -\int_{-\infty}^{+\infty}p(x)ln(q(x)) = H(p(x), q(x))$</p>
<p><br>
$H(p(x), q(x))$ is called the Cross-Entropy. When $p(x)$ and $q(x)$ are same, then the Cross Entropy is equal to the entropy as $D_{KL}(p(x)||q(x))=0$.</p>
<p>The amount by which the cross entropy exceeds the entropy is called relative entropy or KL Divergence.</p>
<p>${\bf Cross\ Entropy = KL\ Divergence + Entropy }$</p>
<h2 id="cross-entropy-in-pytorch">Cross Entropy in PyTorch</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cross_entropy</span>(p, q, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-8</span>):
    <span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">    epsilon to avoid log(0) error
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    logq <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>log2(q <span style="color:#f92672">+</span> epsilon)
    ce <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sum(<span style="color:#f92672">-</span>p<span style="color:#f92672">*</span>logq)
    <span style="color:#66d9ef">return</span> ce

<span style="color:#75715e"># comparison of cross_entropy of similar distribution p2-p3 </span>
<span style="color:#75715e"># and dissimilar distribution p1-p2</span>
p1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor([<span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.4</span>, <span style="color:#ae81ff">0.3</span>])
p2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor([<span style="color:#ae81ff">0.9</span>, <span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.1</span>])
p3 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor([<span style="color:#ae81ff">0.8</span>, <span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.05</span>, <span style="color:#ae81ff">0.15</span>])

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Cross Entropy of {} and {} = {}&#39;</span><span style="color:#f92672">.</span>format(p1<span style="color:#f92672">.</span>numpy(), p2<span style="color:#f92672">.</span>numpy(), cross_entropy(p1, p2)))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Cross Entropy of {} and {} = {}&#39;</span><span style="color:#f92672">.</span>format(p2<span style="color:#f92672">.</span>numpy(), p3<span style="color:#f92672">.</span>numpy(), cross_entropy(p2, p3)))
</code></pre></div><pre><code>Cross Entropy of [0.1 0.2 0.4 0.3] and [0.9 0.  0.  0.1] = 16.957033157348633
Cross Entropy of [0.9 0.  0.  0.1] and [0.8  0.   0.05 0.15] = 0.5634317994117737
</code></pre>
<h4 id="reference">Reference</h4>
<ul>
<li>Elements of Information Theory
Thomas M. Cover, Joy A. Thomas</li>
<li><a href="https://web.stanford.edu/~montanar/RESEARCH/BOOK/partA.pdf">https://web.stanford.edu/~montanar/RESEARCH/BOOK/partA.pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cross_entropy">https://en.wikipedia.org/wiki/Cross_entropy</a></li>
</ul>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/information-theory/">Information Theory</a>
  
  <a class="badge badge-light" href="/tags/python/">Python</a>
  
</div>



    
      








  
  
    
  
  





  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu33d8f2710ea4928d295bd08cdc05f6eb_18243_250x250_fill_q90_lanczos_center.jpeg" itemprop="image" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="/authors/admin/">Shangeth Rajaa</a></h5>
      <h6 class="card-subtitle">Research Intern at IBM Research Labs | Incoming Research at NTU, Singapore.</h6>
      <p class="card-text" itemprop="description">Research Intern at IBM Research, ML Facilitator at Google AI, Research Collaborator at INRIA</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://twitter.com/shangethr" target="_blank" rel="noopener">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://www.linkedin.com/in/shangeth" target="_blank" rel="noopener">
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://scholar.google.co.in/citations?user=apmFPkAAAAAJ&amp;hl=en" target="_blank" rel="noopener">
              <i class="ai ai-google-scholar"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/shangeth" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://medium.com/@shangethrajaa" target="_blank" rel="noopener">
              <i class="fab fa-medium"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/files/resume-shangeth.pdf" >
              <i class="ai ai-cv"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/post/off-policy-monte-carlo/">Off Policy Monte Carlo Prediction with Importance sampling</a></li>
          
          <li><a href="/project/drl-navigator/">DRL Navigator</a></li>
          
          <li><a href="/project/policy-based-rl/">Policy Based Reinforcement Learning</a></li>
          
          <li><a href="/project/temporal-difference/">Temporal Difference</a></li>
          
          <li><a href="/project/monte-carlo/">Monte Carlo Prediction</a></li>
          
        </ul>
      </div>
      
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "shangeth-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    Â© 2020 Shangeth Rajaa &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//shangeth-com.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.ee8463f2a394889d45e169a983fe913d.js"></script>

  </body>
</html>

