<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Shangeth Rajaa">

  
  
  
    
  
  <meta name="description" content="Off-Policy Monte Carlo with Importance Sampling Off Policy Learning Link to the Notebook By exploration-exploitation trade-off, the agent should take sub-optimal exploratory action by which the agent may receive less reward. One way of exploration is by using an epsilon-greedy policy, where the agent takes a nongreedy action with a small probability.
In an on-policy, improvement and evaluation are done on the policy which is used to select actions.
In off-policy, improvement and evaluation are done on a different policy from the one used to select actions.">

  
  <link rel="alternate" hreflang="en-us" href="/post/off-policy-monte-carlo/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="hsl(339, 90%, 68%)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.04bf00581ae5351cdd9e3f0836914cc1.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-134441268-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/off-policy-monte-carlo/">

  
  
  
  
    
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@shangethr">
  <meta property="twitter:creator" content="@shangethr">
  
  <meta property="og:site_name" content="Shangeth">
  <meta property="og:url" content="/post/off-policy-monte-carlo/">
  <meta property="og:title" content="Off Policy Monte Carlo Prediction with Importance sampling | Shangeth">
  <meta property="og:description" content="Off-Policy Monte Carlo with Importance Sampling Off Policy Learning Link to the Notebook By exploration-exploitation trade-off, the agent should take sub-optimal exploratory action by which the agent may receive less reward. One way of exploration is by using an epsilon-greedy policy, where the agent takes a nongreedy action with a small probability.
In an on-policy, improvement and evaluation are done on the policy which is used to select actions.
In off-policy, improvement and evaluation are done on a different policy from the one used to select actions."><meta property="og:image" content="/post/off-policy-monte-carlo/featured.jpg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2020-01-19T02:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2020-01-19T02:00:00&#43;00:00">
  

  

  

  <title>Off Policy Monte Carlo Prediction with Importance sampling | Shangeth</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" class="dark">
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Shangeth</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/">
            
            <span>Deep Learning Course</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  













<div class="article-header d-xl-none">
  <div class="featured-image" style="background-image: url('/post/off-policy-monte-carlo/featured_hu6215271f898eea7ef987e7dcedadcfe4_98428_800x0_resize_q90_lanczos.jpg');"></div>
  <span class="article-header-caption">Image credit: <a href="https://i.redd.it/7805mzyjcf001.jpg"><strong>Gimages</strong></a></span>
</div>


<div class="container-fluid split-header d-none d-xl-block">
  <div class="row">
    <div class="col-6">
      <div class="split-header-content">
        <h1 itemprop="name">Off Policy Monte Carlo Prediction with Importance sampling</h1>

        
        <p class="page-subtitle">Importance sampling for off Policy methods with MC Prediction in Python</p>
        

        



<meta content="2020-01-19 02:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2020-01-19 02:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/admin/">Shangeth Rajaa</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Jan 19, 2020</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    6 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="/post/off-policy-monte-carlo/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    <a href="/categories/deep-learning/">Deep Learning</a>, <a href="/categories/python/">Python</a>, <a href="/categories/openai-gym/">OpenAI Gym</a></span>
  

  

</div>


        















        
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Off%20Policy%20Monte%20Carlo%20Prediction%20with%20Importance%20sampling&amp;url=%2fpost%2foff-policy-monte-carlo%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2foff-policy-monte-carlo%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2foff-policy-monte-carlo%2f&amp;title=Off%20Policy%20Monte%20Carlo%20Prediction%20with%20Importance%20sampling"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2foff-policy-monte-carlo%2f&amp;title=Off%20Policy%20Monte%20Carlo%20Prediction%20with%20Importance%20sampling"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Off%20Policy%20Monte%20Carlo%20Prediction%20with%20Importance%20sampling&amp;body=%2fpost%2foff-policy-monte-carlo%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


      </div>
    </div>
    <div class="col-6">
      <div class="split-header-image">
        <img src="/post/off-policy-monte-carlo/featured_hu6215271f898eea7ef987e7dcedadcfe4_98428_680x500_fill_q90_lanczos_smart1.jpg" itemprop="image" alt="">
        <span class="article-header-caption">Image credit: <a href="https://i.redd.it/7805mzyjcf001.jpg"><strong>Gimages</strong></a></span>
      </div>
    </div>
  </div>
</div>

<div class="article-container d-xl-none">
  <h1 itemprop="name">Off Policy Monte Carlo Prediction with Importance sampling</h1>

  
  <p class="page-subtitle">Importance sampling for off Policy methods with MC Prediction in Python</p>
  

  



<meta content="2020-01-19 02:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2020-01-19 02:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/admin/">Shangeth Rajaa</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>Jan 19, 2020</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    6 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="/post/off-policy-monte-carlo/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    <a href="/categories/deep-learning/">Deep Learning</a>, <a href="/categories/python/">Python</a>, <a href="/categories/openai-gym/">OpenAI Gym</a></span>
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=&amp;url="
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u="
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=&amp;title="
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=&amp;title="
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=&amp;body=">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

  














</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      <h1 id="off-policy-monte-carlo-with-importance-sampling">Off-Policy Monte Carlo with Importance Sampling</h1>
<h2 id="off-policy-learning">Off Policy Learning</h2>
<h3 id="link-to-the-notebookhttpsgithubcomshangethmonte-carlo-predictionblobmasteroff_policy_learning_with_importance_samplingipynb"><a href="https://github.com/shangeth/Monte-Carlo-Prediction/blob/master/Off_Policy_Learning_with_Importance_Sampling.ipynb">Link to the Notebook</a></h3>
<p>By exploration-exploitation trade-off, the agent should take sub-optimal exploratory action by which the agent may receive less reward. One way of exploration is by using an epsilon-greedy policy, where the agent takes a nongreedy action with a small probability.</p>
<p>In an on-policy, improvement and evaluation are done on the policy which is used to select actions.</p>
<p>In off-policy, improvement and evaluation are done on a different policy from the one used to select actions. the policy learned is off the policy used for action selection while gathering episodes.</p>
<ul>
<li>Target Policy $\pi(a/s)$ : The value function of learning is based on $\pi(a/s)$. We want the target policy to be the optimal policy $\pi^{*}(a/s)$. The target policy will be used for action selection after the learning process is complete(deployment).</li>
<li>Behavior Policy $b(a/s)$: Behavior policy is used for action selection while gathering episodes to train the agent. This generally follows an exploratory policy.</li>
</ul>
<h2 id="importance-sampling">Importance Sampling</h2>
<p>We have a random variable $x \sim b$ sampled from behavior policy distribution $b$. We want to estimate the expected value of $x$ wrt the target distribution $\pi$ ie: $E_{\pi}[X]$. The sample average will give the expected value of x under b $E_{b}[X]$.</p>
<p>$E_{\pi}[X] = \sum x \pi(x)$ where $x \in X$</p>
<p>$ \qquad = \sum x \pi(x) \dfrac{b(x)}{b(x)}$</p>
<p>$ \qquad = \sum x \dfrac{\pi(x)}{b(x)} b(x)$</p>
<p>$ \qquad = \sum x \rho(x) b(x)$</p>
<p>where $\rho(x) = \dfrac{\pi(x)}{b(x)}$ and is called the importance sampling ratio.</p>
<p><br>
Let $x\rho(x)$ be a new random variable $X_\rho(X)$.
Then, $E_{\pi}[X] = \sum x \rho(x) b(x) = E_{b}[X_{\rho}(X)]$. Now we have expectation under $b$ instead of $\pi$.</p>
<p>\</p>
<h3 id="how-do-we-estimating-expectation-from-the-data">How do we estimating expectation from the data?</h3>
<p>Compute the weighted sample average with importance sampling ratio as the weights.
$E_{\pi}[X] = \sum x \rho(x) b(x) = E_{b}[X_{\rho}(X)] \approx \dfrac{1}{n}\sum_{i=1}^{n}x_i \rho(x_i)$ and $x_i \sim b$.</p>
<h2 id="off-policy-monte-carlo-prediction-with-importance-sampling">Off Policy Monte Carlo Prediction with Importance Sampling</h2>
<p>In Monte Carlo prediction, we estimate the value of each state by computing a sample average over returns starting from that state. $V_{\pi}(s) = E_{\pi}[G_t|S_t=s]$</p>
<p><br>
In off-policy, we are trying to estimate value under the target policy $\pi(s)$ using returns following the behavior policy $b(s)$. As discussed above, we need to find the value of $\rho$ for each of the sampled returns. $\rho$ is the probability of trajectory under $\pi$ divided by the probability of trajectory under $b$.</p>
<p>$V_{\pi}(s) = E_{b}[\rho G_t|S_t=s]$ ; $\rho = \dfrac{P(trajectory\ under\ \pi)}{P(trajectory\ under\ b)}$</p>
<p><br>
The probaility of a trajectory under a policy can be given by
$P(trajectory\ under\ policy) = P(A_t, S_{t+1}, A_{t+1}, &hellip;, S_T| S_t, A_{t:T})$</p>
<p>where all actions are taken under the policy $b$.</p>
<p><br>
As this is a Markov process, we can split the probability terms into
$P(A_t, S_{t+1}, A_{t+1}, &hellip;, S_T|S_t, A_{t:T}) = b(A_t|S_t)p(S_{t+1}|S_t, A_t)\  b(A_{t+1}|S_{t+1}) p(S_{t+2}|S_{t+1}, A_{t+1}) &hellip; \ p(S_T|S_{T-1}, A_{T-1})$</p>
<p><br>
$b(A_t|S_t)p(S_{t+1}|S_t, A_t)$ give the probability of action $A_t$ at state $S_t$ times the probability of the state transition to state $S_{t+1}$</p>
<p>$P(A_t, S_{t+1}, A_{t+1}, &hellip;, S_T| S_t, A_{t:T}) = \prod_{k=1}^{T} b(A_k|S_k)p(S_{k+1}|S_k, A_k)$</p>
<p>$\rho = \dfrac{P(trajectory\ under\ \pi)}{P(trajectory\ under\ b)}$</p>
<p>$\quad = \prod_{k=t}^{T-1}\dfrac{\pi(A_k|S_k)p(S_{k+1}|S_k, A_k)}{b(A_k|S_k)p(S_{k+1}|S_k, A_k)}$</p>
<p>$\rho = \prod_{k=t}^{T-1}\dfrac{\pi(A_k|S_k)}{b(A_k|S_k)}$</p>
<h2 id="incremental-implementation-of-off-policy-mc">Incremental Implementation of Off-policy MC</h2>
<p>Suppose we have a sequence of returns $G_1, G_2, &hellip;G_{n-1}$ all starting in the same state and with corresponding weights $W_i$, then the weighted average of returns $V_n = \dfrac{\sum_{k=1}^{n-1} W_kG_k}{\sum_{k=1}^{n-1} W_k}$ , $n \geq 2$ and the cumulative sum of weights $C_{n+1} = C_n + W_{n+1}$ where $C_0 = 0$.</p>
<p>The update rule for $V_{n+1} = V_n + \dfrac{W_n}{C_n}[G_n-V_n]$, $n \geq 1$ and $V_1$ is arbitrary.</p>
<p>Here $W_n$ will be the importance sampling weight.</p>
<h2 id="greedy-pi-policy">Greedy $\pi$ policy</h2>
<p>When the target policy $\pi$ is $\epsilon$ greedy, it is determenistic and $\pi(A_t/S_t) = 1$ . So $\rho = \prod_{k=t}^{T-1}\dfrac{1}{b(A_k|S_k)}$ and $W$ can be updated with $W \leftarrow W \dfrac{1}{b(A_t|S_t)}$ for each time step in the trajectory.</p>
<p>So the Off policy Monte Carlo Control algorithm is
<img src="https://i.stack.imgur.com/Xi0vX.png" alt=""></p>
<h1 id="off-policy-mc-control-with-weighted-importance-in-python">Off-Policy MC Control with Weighted Importance in Python</h1>
<h2 id="blackjack-environment">BlackJack Environment</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> sys
<span style="color:#f92672">import</span> gym
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> defaultdict
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#39;Blackjack-v0&#39;</span>)

<span style="color:#66d9ef">print</span>(vars(env), end<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">&#39;</span>)
<span style="color:#66d9ef">print</span>(dir(env))
</code></pre></div><pre><code>{'action_space': Discrete(2), 'observation_space': Tuple(Discrete(32), Discrete(11), Discrete(2)), 'np_random': RandomState(MT19937) at 0x7FE1148D2888, 'natural': False, 'dealer': [10, 1], 'player': [10, 1], 'spec': EnvSpec(Blackjack-v0)}

['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_get_obs', 'action_space', 'close', 'dealer', 'metadata', 'natural', 'np_random', 'observation_space', 'player', 'render', 'reset', 'reward_range', 'seed', 'spec', 'step', 'unwrapped']
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">print</span>(env<span style="color:#f92672">.</span>observation_space)
<span style="color:#66d9ef">print</span>(env<span style="color:#f92672">.</span>action_space)
</code></pre></div><pre><code>Tuple(Discrete(32), Discrete(11), Discrete(2))
Discrete(2)
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">random_state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Random State&#39;</span>, random_state)

random_action <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>sample()
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Random Action&#39;</span>, random_action)
</code></pre></div><pre><code>Random State (18, 2, False)
Random Action 1
</code></pre>
<h2 id="generate-random-eposides">Generate Random Eposides</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">num_episodes <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_episodes):
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Episode : &#39;</span>, i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)
    state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
    step <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#66d9ef">while</span> True:
        step <span style="color:#f92672">+=</span><span style="color:#ae81ff">1</span>
        action <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>sample()
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Step = {}</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> State = {}</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> Action Taken = {}&#39;</span><span style="color:#f92672">.</span>format(step, state, action))
        state, reward, done, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
        <span style="color:#66d9ef">if</span> done:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Game Ended...&#39;</span>)
            <span style="color:#66d9ef">if</span> reward <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>: <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Agent Won!</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
            <span style="color:#66d9ef">else</span>: <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Agent Lost!</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)
            <span style="color:#66d9ef">break</span>
</code></pre></div><pre><code>Episode :  1
Step = 1	 State = (10, 7, False)	 Action Taken = 1
Step = 2	 State = (21, 7, True)	 Action Taken = 1
Step = 3	 State = (15, 7, False)	 Action Taken = 1
Step = 4	 State = (18, 7, False)	 Action Taken = 0
Game Ended...
Agent Lost!

Episode :  2
Step = 1	 State = (20, 8, False)	 Action Taken = 0
Game Ended...
Agent Won!

Episode :  3
Step = 1	 State = (12, 7, False)	 Action Taken = 1
Step = 2	 State = (15, 7, False)	 Action Taken = 0
Game Ended...
Agent Won!
</code></pre>
<h2 id="training">Training</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">random_policy</span>(nA):
    A <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(nA, dtype<span style="color:#f92672">=</span>float) <span style="color:#f92672">/</span> nA
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">policy_fn</span>(observation):
        <span style="color:#66d9ef">return</span> A
    <span style="color:#66d9ef">return</span> policy_fn
    
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">greedy_policy</span>(Q):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">policy_fn</span>(state):
        A <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros_like(Q[state], dtype<span style="color:#f92672">=</span>float)
        best_action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(Q[state])
        A[best_action] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
        <span style="color:#66d9ef">return</span> A
    <span style="color:#66d9ef">return</span> policy_fn
    
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mc_off_policy</span>(env, num_episodes, behavior_policy, max_time<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, discount_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>):
    Q <span style="color:#f92672">=</span> defaultdict(<span style="color:#66d9ef">lambda</span>:np<span style="color:#f92672">.</span>zeros(env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>n))
    C <span style="color:#f92672">=</span> defaultdict(<span style="color:#66d9ef">lambda</span>:np<span style="color:#f92672">.</span>zeros(env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>n))

    target_policy <span style="color:#f92672">=</span> greedy_policy(Q)

    <span style="color:#66d9ef">for</span> i_episode <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, num_episodes<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>):
        <span style="color:#66d9ef">if</span> i_episode <span style="color:#f92672">%</span> <span style="color:#ae81ff">1000</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\r</span><span style="color:#e6db74">Episode {}/{}.&#34;</span><span style="color:#f92672">.</span>format(i_episode, num_episodes), end<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>)
            sys<span style="color:#f92672">.</span>stdout<span style="color:#f92672">.</span>flush()

        episode <span style="color:#f92672">=</span> []
        state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
        <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(max_time):
            probs <span style="color:#f92672">=</span> behavior_policy(state)
            action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(np<span style="color:#f92672">.</span>arange(len(probs)), p<span style="color:#f92672">=</span>probs)
            next_state, reward, done, _ <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
            episode<span style="color:#f92672">.</span>append((state, action, reward))
            <span style="color:#66d9ef">if</span> done:
                <span style="color:#66d9ef">break</span>
            state <span style="color:#f92672">=</span> next_state
        
        G <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
        W <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
        <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(len(episode))[::<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]:
            state, action, reward <span style="color:#f92672">=</span> episode[t]
            G <span style="color:#f92672">=</span> discount_factor <span style="color:#f92672">*</span> G <span style="color:#f92672">+</span> reward
            C[state][action] <span style="color:#f92672">+=</span> W
            Q[state][action] <span style="color:#f92672">+=</span> (W <span style="color:#f92672">/</span> C[state][action]) <span style="color:#f92672">*</span> (G <span style="color:#f92672">-</span> Q[state][action])
            <span style="color:#66d9ef">if</span> action <span style="color:#f92672">!=</span>  np<span style="color:#f92672">.</span>argmax(target_policy(state)):
                <span style="color:#66d9ef">break</span>
            W <span style="color:#f92672">=</span> W <span style="color:#f92672">*</span> <span style="color:#ae81ff">1.</span><span style="color:#f92672">/</span>behavior_policy(state)[action]
    <span style="color:#66d9ef">return</span> Q, target_policy
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">random_policy <span style="color:#f92672">=</span> random_policy(env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>n)
Q, policy <span style="color:#f92672">=</span> mc_off_policy(env, num_episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">500000</span>, behavior_policy<span style="color:#f92672">=</span>random_policy)
</code></pre></div><pre><code>Episode 500000/500000.
</code></pre>
<h2 id="plot">Plot</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">from</span> mpl_toolkits.axes_grid1 <span style="color:#f92672">import</span> make_axes_locatable


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_policy</span>(policy):

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_Z</span>(x, y, usable_ace):
        <span style="color:#66d9ef">if</span> (x,y,usable_ace) <span style="color:#f92672">in</span> policy:
            <span style="color:#66d9ef">return</span> policy[x,y,usable_ace]
        <span style="color:#66d9ef">else</span>:
            <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_figure</span>(usable_ace, ax):
        x_range <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">11</span>, <span style="color:#ae81ff">22</span>)
        y_range <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        X, Y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>meshgrid(x_range, y_range)
        Z <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[get_Z(x,y,usable_ace) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> x_range] <span style="color:#66d9ef">for</span> y <span style="color:#f92672">in</span> y_range])
        surf <span style="color:#f92672">=</span> ax<span style="color:#f92672">.</span>imshow(Z, cmap<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>get_cmap(<span style="color:#e6db74">&#39;Pastel2&#39;</span>, <span style="color:#ae81ff">2</span>), vmin<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, vmax<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, extent<span style="color:#f92672">=</span>[<span style="color:#ae81ff">10.5</span>, <span style="color:#ae81ff">21.5</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">10.5</span>])
        plt<span style="color:#f92672">.</span>xticks(x_range)
        plt<span style="color:#f92672">.</span>yticks(y_range)
        plt<span style="color:#f92672">.</span>gca()<span style="color:#f92672">.</span>invert_yaxis()
        ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#39;Player</span><span style="color:#ae81ff">\&#39;</span><span style="color:#e6db74">s Current Sum&#39;</span>)
        ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#39;Dealer</span><span style="color:#ae81ff">\&#39;</span><span style="color:#e6db74">s Showing Card&#39;</span>)
        ax<span style="color:#f92672">.</span>grid(color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;w&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;-&#39;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        divider <span style="color:#f92672">=</span> make_axes_locatable(ax)
        cax <span style="color:#f92672">=</span> divider<span style="color:#f92672">.</span>append_axes(<span style="color:#e6db74">&#34;right&#34;</span>, size<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;5%&#34;</span>, pad<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)
        cbar <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>colorbar(surf, ticks<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>], cax<span style="color:#f92672">=</span>cax)
        cbar<span style="color:#f92672">.</span>ax<span style="color:#f92672">.</span>set_yticklabels([<span style="color:#e6db74">&#39;0 (STICK)&#39;</span>,<span style="color:#e6db74">&#39;1 (HIT)&#39;</span>])
            
    fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">15</span>, <span style="color:#ae81ff">15</span>))
    ax <span style="color:#f92672">=</span> fig<span style="color:#f92672">.</span>add_subplot(<span style="color:#ae81ff">121</span>)
    ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Usable Ace&#39;</span>)
    get_figure(True, ax)
    ax <span style="color:#f92672">=</span> fig<span style="color:#f92672">.</span>add_subplot(<span style="color:#ae81ff">122</span>)
    ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;No Usable Ace&#39;</span>)
    get_figure(False, ax)
    plt<span style="color:#f92672">.</span>show()
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">policy <span style="color:#f92672">=</span> dict((k,np<span style="color:#f92672">.</span>argmax(v)) <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> Q<span style="color:#f92672">.</span>items())
plot_policy(policy)
</code></pre></div><p><img src="output_18_0.png" alt="png"></p>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/reinforcement-learning/">Reinforcement Learning</a>
  
  <a class="badge badge-light" href="/tags/python/">Python</a>
  
</div>



    
      








  
  
    
  
  





  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu33d8f2710ea4928d295bd08cdc05f6eb_18243_250x250_fill_q90_lanczos_center.jpeg" itemprop="image" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="/authors/admin/">Shangeth Rajaa</a></h5>
      <h6 class="card-subtitle">Research Intern at IBM Research Labs | Incoming Research at NTU, Singapore.</h6>
      <p class="card-text" itemprop="description">Research Intern at IBM Research, ML Facilitator at Google AI, Research Collaborator at INRIA</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://twitter.com/shangethr" target="_blank" rel="noopener">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://www.linkedin.com/in/shangeth" target="_blank" rel="noopener">
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://scholar.google.co.in/citations?user=apmFPkAAAAAJ&amp;hl=en" target="_blank" rel="noopener">
              <i class="ai ai-google-scholar"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/shangeth" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://medium.com/@shangethrajaa" target="_blank" rel="noopener">
              <i class="fab fa-medium"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/files/resume-shangeth.pdf" >
              <i class="ai ai-cv"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/project/drl-navigator/">DRL Navigator</a></li>
          
          <li><a href="/project/policy-based-rl/">Policy Based Reinforcement Learning</a></li>
          
          <li><a href="/project/temporal-difference/">Temporal Difference</a></li>
          
          <li><a href="/project/monte-carlo/">Monte Carlo Prediction</a></li>
          
          <li><a href="/project/facial-emotion-recognition-pytorch/">Facial Emotion Recognition PyTorch ONNX</a></li>
          
        </ul>
      </div>
      
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "shangeth-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    © 2020 Shangeth Rajaa &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//shangeth-com.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.ee8463f2a394889d45e169a983fe913d.js"></script>

  </body>
</html>

